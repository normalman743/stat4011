import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import os
from datetime import datetime

# è®¾ç½®è®¾å¤‡
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

class FinancialMLP(nn.Module):
    """30â†’128â†’64â†’64â†’32â†’16â†’2 æ¶æ„çš„MLP"""
    
    def __init__(self, input_dim=30, dropout_rates=[0.3, 0.2, 0.2, 0.1, 0.1]):
        super(FinancialMLP, self).__init__()
        
        self.network = nn.Sequential(
            # 30 -> 128
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_rates[0]),
            
            # 128 -> 64
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rates[1]),
            
            # 64 -> 64
            nn.Linear(64, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rates[2]),
            
            # 64 -> 32
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(dropout_rates[3]),
            
            # 32 -> 16
            nn.Linear(32, 16),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(dropout_rates[4]),
            
            # 16 -> 2
            nn.Linear(16, 2)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)

class EarlyStopping:
    """æ—©åœæ³•ç±»"""
    def __init__(self, patience=10, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_score = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, score, model):
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                if self.restore_best_weights:
                    model.load_state_dict(self.best_weights)
                return True
        else:
            self.best_score = score
            self.counter = 0
            self.save_checkpoint(model)
        return False
    
    def save_checkpoint(self, model):
        self.best_weights = model.state_dict().copy()

def preprocess_features(df, feature_cols, show_details=True):
    """æ•°æ®é¢„å¤„ç† - å¢å¼ºç‰ˆå¸¦è¯¦ç»†æ‰“å°"""
    
    print("=" * 60)
    print("ğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    print(f"åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
    
    X = df[feature_cols].copy()
    
    # ===== 1. æ£€æŸ¥å’Œåˆ é™¤å¸¸æ•°ç‰¹å¾ =====
    print("\nğŸ“Š æ£€æŸ¥å¸¸æ•°ç‰¹å¾...")
    constant_features = []
    feature_stats = {}
    
    for col in X.columns:
        unique_count = X[col].nunique()
        unique_values = X[col].unique()
        
        feature_stats[col] = {
            'unique_count': unique_count,
            'unique_values': unique_values[:5] if len(unique_values) > 5 else unique_values  # åªæ˜¾ç¤ºå‰5ä¸ª
        }
        
        if unique_count <= 1:
            constant_features.append(col)
            print(f"  âŒ {col}: åªæœ‰ {unique_count} ä¸ªå”¯ä¸€å€¼ -> {unique_values}")
    
    if constant_features:
        print(f"\nğŸ—‘ï¸  åˆ é™¤ {len(constant_features)} ä¸ªå¸¸æ•°ç‰¹å¾: {constant_features}")
        X = X.drop(columns=constant_features)
    else:
        print("âœ… æ²¡æœ‰å‘ç°å¸¸æ•°ç‰¹å¾")
    
    # ===== 2. æ˜¾ç¤ºæ•°æ®èŒƒå›´æƒ…å†µ =====
    print(f"\nğŸ“ˆ åŸå§‹æ•°æ®èŒƒå›´åˆ†æ:")
    print("-" * 50)
    
    extreme_features = []
    for col in X.columns:
        min_val, max_val = X[col].min(), X[col].max()
        std_val = X[col].std()
        
        print(f"{col:25} | èŒƒå›´: [{min_val:>12.2e}, {max_val:>12.2e}] | æ ‡å‡†å·®: {std_val:>10.2e}")
        
        # æ£€æŸ¥æç«¯å€¼
        if max_val > 1e10 or min_val < -1e10:
            extreme_features.append(col)
    
    if extreme_features:
        print(f"\nâš ï¸  å‘ç°æç«¯æ•°å€¼ç‰¹å¾: {extreme_features}")
    
    # ===== 3. å¤„ç†æç«¯å¼‚å¸¸å€¼ =====
    print(f"\nğŸ› ï¸  å¤„ç†å¼‚å¸¸å€¼...")
    processing_log = {}
    
    for col in X.columns:
        original_min, original_max = X[col].min(), X[col].max()
        
        if 'profit' in col:
            print(f"\n  å¤„ç†é‡‘é¢ç‰¹å¾: {col}")
            
            # Step 1: logå˜æ¢
            print(f"    åŸå§‹èŒƒå›´: [{original_min:.2e}, {original_max:.2e}]")
            X[col] = np.sign(X[col]) * np.log1p(np.abs(X[col]))
            log_min, log_max = X[col].min(), X[col].max()
            print(f"    Logå˜æ¢å: [{log_min:.4f}, {log_max:.4f}]")
            
            # Step 2: clipå¼‚å¸¸å€¼
            Q01 = X[col].quantile(0.01)
            Q99 = X[col].quantile(0.99)
            print(f"    1%åˆ†ä½æ•°: {Q01:.4f}, 99%åˆ†ä½æ•°: {Q99:.4f}")
            
            # ç»Ÿè®¡ä¼šè¢«clipçš„æ•°æ®
            will_be_clipped = ((X[col] < Q01) | (X[col] > Q99)).sum()
            clip_percentage = will_be_clipped / len(X) * 100
            
            X[col] = np.clip(X[col], Q01, Q99)
            print(f"    ClipåèŒƒå›´: [{X[col].min():.4f}, {X[col].max():.4f}]")
            print(f"    è¢«è£å‰ªæ•°æ®: {will_be_clipped}/{len(X)} ({clip_percentage:.2f}%)")
            
            processing_log[col] = {
                'type': 'profit',
                'original_range': (original_min, original_max),
                'log_range': (log_min, log_max),
                'clip_range': (Q01, Q99),
                'clipped_count': will_be_clipped
            }
            
        elif 'ratio' in col:
            print(f"\n  å¤„ç†æ¯”ä¾‹ç‰¹å¾: {col}")
            print(f"    åŸå§‹èŒƒå›´: [{original_min:.4f}, {original_max:.4f}]")
            
            # ç»Ÿè®¡ä¼šè¢«clipçš„æ•°æ®
            will_be_clipped = ((X[col] < 0) | (X[col] > 50)).sum()
            clip_percentage = will_be_clipped / len(X) * 100
            
            X[col] = np.clip(X[col], 0, 50)
            print(f"    Clipåˆ°[0, 50]: [{X[col].min():.4f}, {X[col].max():.4f}]")
            print(f"    è¢«è£å‰ªæ•°æ®: {will_be_clipped}/{len(X)} ({clip_percentage:.2f}%)")
            
            processing_log[col] = {
                'type': 'ratio',
                'original_range': (original_min, original_max),
                'clip_range': (0, 50),
                'clipped_count': will_be_clipped
            }
        
        else:
            # å…¶ä»–ç‰¹å¾ä¸å¤„ç†ï¼Œåªè®°å½•
            processing_log[col] = {
                'type': 'other',
                'original_range': (original_min, original_max)
            }
    
    # ===== 4. æ ‡å‡†åŒ– =====
    print(f"\nğŸ“ åº”ç”¨RobustScaleræ ‡å‡†åŒ–...")
    scaler = RobustScaler()
    
    # æ˜¾ç¤ºæ ‡å‡†åŒ–å‰åçš„ç»Ÿè®¡
    print("æ ‡å‡†åŒ–å‰åå¯¹æ¯” (å‰5ä¸ªç‰¹å¾):")
    print("-" * 60)
    
    for i, col in enumerate(X.columns[:5]):
        before_mean, before_std = X[col].mean(), X[col].std()
        print(f"  {col:20} | å‡å€¼: {before_mean:>8.3f} | æ ‡å‡†å·®: {before_std:>8.3f}")
    
    X_scaled = pd.DataFrame(
        scaler.fit_transform(X), 
        columns=X.columns, 
        index=X.index
    )
    
    print("\næ ‡å‡†åŒ–å:")
    for i, col in enumerate(X_scaled.columns[:5]):
        after_mean, after_std = X_scaled[col].mean(), X_scaled[col].std()
        print(f"  {col:20} | å‡å€¼: {after_mean:>8.3f} | æ ‡å‡†å·®: {after_std:>8.3f}")
    
    # ===== 5. æœ€ç»ˆæ‘˜è¦ =====
    print("\n" + "=" * 60)
    print("ğŸ“‹ é¢„å¤„ç†å®Œæˆæ‘˜è¦:")
    print(f"  â€¢ åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
    print(f"  â€¢ åˆ é™¤å¸¸æ•°ç‰¹å¾: {len(constant_features)}")
    print(f"  â€¢ æœ€ç»ˆç‰¹å¾æ•°: {X_scaled.shape[1]}")
    print(f"  â€¢ æ ·æœ¬æ•°: {X_scaled.shape[0]}")
    
    # ç»Ÿè®¡å„ç±»å¤„ç†çš„ç‰¹å¾æ•°
    profit_features = [k for k, v in processing_log.items() if v.get('type') == 'profit']
    ratio_features = [k for k, v in processing_log.items() if v.get('type') == 'ratio']
    other_features = [k for k, v in processing_log.items() if v.get('type') == 'other']
    
    print(f"  â€¢ é‡‘é¢ç‰¹å¾(log+clip): {len(profit_features)}")
    print(f"  â€¢ æ¯”ä¾‹ç‰¹å¾(clip): {len(ratio_features)}")
    print(f"  â€¢ å…¶ä»–ç‰¹å¾: {len(other_features)}")
    
    print("=" * 60)
    
    return X_scaled, scaler

def train_model(model, train_loader, val_loader, epochs=100, learning_rate=0.001, 
                weight_decay=1e-4, class_weights=None):
    """è®­ç»ƒæ¨¡å‹"""
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    if class_weights is not None:
        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))
    else:
        criterion = nn.CrossEntropyLoss()
    
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=5
    )
    
    # æ—©åœæ³•
    early_stopping = EarlyStopping(patience=10, min_delta=0.001)
    
    train_losses = []
    val_f1_scores = []
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                _, predicted = torch.max(outputs, 1)
                
                val_predictions.extend(predicted.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        # è®¡ç®—F1åˆ†æ•°
        val_f1 = f1_score(val_targets, val_predictions, average='weighted')
        
        train_losses.append(train_loss / len(train_loader))
        val_f1_scores.append(val_f1)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_f1)
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_f1, model):
            print(f"Early stopping at epoch {epoch+1}")
            break
        
        if epoch % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], '
                  f'Train Loss: {train_losses[-1]:.4f}, '
                  f'Val F1: {val_f1:.4f}')
    
    return train_losses, val_f1_scores

def cross_validation_training(X, y, cv_folds=10, epochs=100):
    """äº¤å‰éªŒè¯è®­ç»ƒ - 10æŠ˜å…¨è®­ç»ƒï¼Œé€‰æ‹©æœ€ä½³ç­–ç•¥"""
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = np.bincount(y)
    class_weights = torch.FloatTensor(len(y) / (len(class_counts) * class_counts))
    print(f"ç±»åˆ«æƒé‡: {class_weights}")
    
    # åˆ†å±‚äº¤å‰éªŒè¯
    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    all_folds = list(skf.split(X, y))
    
    print(f"ğŸ“Š ç­–ç•¥: {cv_folds}æŠ˜äº¤å‰éªŒè¯ï¼Œè®­ç»ƒå…¨éƒ¨ï¼Œé€‰æ‹©æœ€ä½³")
    print(f"   æ¯æŠ˜éªŒè¯é›†å¤§å°: ~{len(y)//cv_folds} ({100/cv_folds:.1f}%)")
    print(f"   æ¯æŠ˜è®­ç»ƒé›†å¤§å°: ~{len(y)*(cv_folds-1)//cv_folds} ({100*(cv_folds-1)/cv_folds:.1f}%)")
    
    fold_results = []
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒå…¨éƒ¨ {cv_folds} æŠ˜...")
    
    for fold_idx in range(cv_folds):
        train_idx, val_idx = all_folds[fold_idx]
        print(f"\n=== Fold {fold_idx+1}/{cv_folds} ===")
        
        # åˆ†å‰²æ•°æ®
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        print(f"è®­ç»ƒé›†: {len(X_train)}, éªŒè¯é›†: {len(X_val)}")
        
        # è½¬æ¢ä¸ºtensor
        X_train_tensor = torch.FloatTensor(X_train.values)
        y_train_tensor = torch.LongTensor(y_train)
        X_val_tensor = torch.FloatTensor(X_val.values)
        y_val_tensor = torch.LongTensor(y_val)
        
        # åˆ›å»ºDataLoader
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = FinancialMLP(input_dim=X.shape[1]).to(device)
        
        # è®­ç»ƒæ¨¡å‹
        train_losses, val_f1_scores = train_model(
            model, train_loader, val_loader, 
            epochs=epochs, class_weights=class_weights
        )
        
        # è·å–æ—©åœè½®æ•°å’Œæœ€ä½³F1åˆ†æ•°
        actual_epochs = len(train_losses)
        fold_best_f1 = max(val_f1_scores) if val_f1_scores else 0  # ä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³F1
        
        # è·å–æœ€ç»ˆé¢„æµ‹ï¼ˆç”¨äºè°ƒè¯•å¯¹æ¯”ï¼‰
        model.eval()
        with torch.no_grad():
            outputs = model(X_val_tensor.to(device))
            _, predictions = torch.max(outputs, 1)
            predictions = predictions.cpu().numpy()
        
        # è®¡ç®—æœ€ç»ˆè¯„ä¼°F1ï¼ˆä»…ç”¨äºå¯¹æ¯”è°ƒè¯•ï¼‰
        final_eval_f1 = f1_score(y_val, predictions, average='weighted')
        
        print(f"ğŸ¯ æœ€ä½³éªŒè¯F1: {fold_best_f1:.4f} (å°†ä½œä¸ºæœ€ç»ˆç»“æœ)")
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­æœ€ä½³F1: {fold_best_f1:.4f}")
        print(f"æœ€ç»ˆè¯„ä¼°F1: {final_eval_f1:.4f}")
        
        # å­˜å‚¨ç»“æœ - ä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³F1
        fold_results.append({
            'fold_idx': fold_idx,
            'f1_score': fold_best_f1,  # âœ… ä½¿ç”¨æ­£ç¡®çš„æœ€ä½³F1
            'epochs': actual_epochs,
            'predictions': predictions,
            'val_indices': val_idx,
            'train_size': len(X_train),
            'val_size': len(X_val),
            'final_eval_f1': final_eval_f1  # ä¿ç•™ç”¨äºè°ƒè¯•
        })
        
        print(f"Fold {fold_idx+1} F1 Score: {fold_best_f1:.4f}, Early stopped at epoch: {actual_epochs}")
    
    # é€‰æ‹©æœ€ä½³fold
    best_fold = max(fold_results, key=lambda x: x['f1_score'])
    best_fold_idx = best_fold['fold_idx']
    
    print(f"\nğŸ† æœ€ä½³è¡¨ç°: Fold {best_fold_idx+1}")
    print(f"   F1åˆ†æ•°: {best_fold['f1_score']:.4f}")
    print(f"   æ—©åœè½®æ•°: {best_fold['epochs']}")
    print(f"   è®­ç»ƒé›†å¤§å°: {best_fold['train_size']}")
    print(f"   éªŒè¯é›†å¤§å°: {best_fold['val_size']}")
    
    # æ”¶é›†æ‰€æœ‰ç»“æœç”¨äºè¿”å›
    fold_f1_scores = [r['f1_score'] for r in fold_results]
    fold_predictions = []
    fold_indices = []
    
    for result in fold_results:
        fold_predictions.extend(result['predictions'])
        fold_indices.extend(result['val_indices'])
    
    mean_f1 = np.mean(fold_f1_scores)
    std_f1 = np.std(fold_f1_scores)
    optimal_epochs = best_fold['epochs']  # ä½¿ç”¨æœ€ä½³foldçš„æ—©åœè½®æ•°
    
    print(f"\n=== äº¤å‰éªŒè¯ç»“æœæ‘˜è¦ ===")
    print(f"å…¨éƒ¨{cv_folds}æŠ˜F1åˆ†æ•°: {fold_f1_scores}")
    print(f"å„æŠ˜æ—©åœè½®æ•°: {[r['epochs'] for r in fold_results]}")
    print(f"å¹³å‡F1åˆ†æ•°: {mean_f1:.4f} Â± {std_f1:.4f}")
    print(f"ğŸ¯ é€‰ç”¨æœ€ä¼˜è®­ç»ƒè½®æ•°: {optimal_epochs} (æ¥è‡ªæœ€ä½³Fold {best_fold_idx+1})")
    
    return mean_f1, fold_f1_scores, fold_predictions, fold_indices, optimal_epochs, fold_results, best_fold

def train_final_model(X_train, y_train, optimal_epochs):
    """è®­ç»ƒæœ€ç»ˆæ¨¡å‹ - ä½¿ç”¨å…¨éƒ¨æ•°æ®å’Œäº¤å‰éªŒè¯ç¡®å®šçš„æœ€ä¼˜è½®æ•°"""
    
    print(f"\nğŸš€ è®­ç»ƒæœ€ç»ˆæ¨¡å‹ (ä½¿ç”¨å…¨éƒ¨{len(y_train)}ä¸ªè®­ç»ƒæ ·æœ¬)...")
    print(f"ğŸ¯ æœ€ä¼˜è®­ç»ƒè½®æ•°: {optimal_epochs} (åŸºäºäº¤å‰éªŒè¯æ—©åœç»“æœ)")
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = np.bincount(y_train)
    class_weights = torch.FloatTensor(len(y_train) / (len(class_counts) * class_counts))
    print(f"ç±»åˆ«æƒé‡: {class_weights}")
    
    # è½¬æ¢ä¸ºtensor - ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®
    X_train_tensor = torch.FloatTensor(X_train.values)
    y_train_tensor = torch.LongTensor(y_train)
    
    # åˆ›å»ºDataLoader
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = FinancialMLP(input_dim=X_train.shape[1]).to(device)
    
    # è®­ç»ƒè®¾ç½®
    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    
    print(f"å¼€å§‹è®­ç»ƒæœ€ç»ˆæ¨¡å‹ ({optimal_epochs} epochs, æ— éªŒè¯é›†)...")
    model.train()
    
    for epoch in range(optimal_epochs):
        epoch_loss = 0.0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / len(train_loader)
        
        if epoch % 5 == 0:
            print(f'Epoch [{epoch+1}/{optimal_epochs}], Training Loss: {avg_loss:.4f}')
    
    print("âœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š ä½¿ç”¨äº† {optimal_epochs} ä¸ªè®­ç»ƒè½®æ•°")
    return model

def predict_test_set(model, X_test, test_accounts):
    """å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹"""
    
    print("\nğŸ”® å¼€å§‹é¢„æµ‹æµ‹è¯•é›†...")
    
    model.eval()
    X_test_tensor = torch.FloatTensor(X_test.values).to(device)
    
    with torch.no_grad():
        outputs = model(X_test_tensor)
        probabilities = torch.softmax(outputs, dim=1)
        _, predictions = torch.max(outputs, 1)
    
    predictions = predictions.cpu().numpy()
    probabilities = probabilities.cpu().numpy()
    
    # åˆ›å»ºé¢„æµ‹ç»“æœDataFrame
    test_results = pd.DataFrame({
        'account': test_accounts,
        'predicted_label': predictions,
        'probability_good': probabilities[:, 0],
        'probability_bad': probabilities[:, 1]
    })
    
    print(f"æµ‹è¯•é›†é¢„æµ‹å®Œæˆ: {len(test_results)} ä¸ªè´¦æˆ·")
    print(f"é¢„æµ‹ä¸ºBadçš„è´¦æˆ·: {(predictions == 1).sum()} ({(predictions == 1).mean()*100:.2f}%)")
    
    return test_results

def main():
    """ä¸»å‡½æ•°"""
    
    # 1. åŠ è½½æ‰€æœ‰æ•°æ®
    print("åŠ è½½æ•°æ®...")
    data_path = "/Users/mannormal/4011/Qi Zihan/feature_extraction/generated_features/all_features_complete.csv"
    df = pd.read_csv(data_path)
    
    # åŠ è½½è®­ç»ƒæ ‡ç­¾ (account, flag)
    train_path = "/Users/mannormal/4011/Qi Zihan/original_data/train_acc.csv"
    train_df = pd.read_csv(train_path)
    
    # åŠ è½½æµ‹è¯•è´¦æˆ· (åªæœ‰account)
    test_path = "/Users/mannormal/4011/Qi Zihan/original_data/test_acc_predict.csv"
    test_df = pd.read_csv(test_path)
    
    print(f"ç‰¹å¾æ•°æ®: {df.shape}")
    print(f"è®­ç»ƒæ ‡ç­¾: {train_df.shape}")
    print(f"æµ‹è¯•è´¦æˆ·: {test_df.shape}")
    print(f"è®­ç»ƒæ•°æ®åˆ—å: {train_df.columns.tolist()}")
    print(f"æµ‹è¯•æ•°æ®åˆ—å: {test_df.columns.tolist()}")
    
    # 2. åˆ†ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_accounts = set(train_df['account'])
    test_accounts = set(test_df['account'])
    
    # è®­ç»ƒé›†ï¼šæœ‰æ ‡ç­¾çš„è´¦æˆ·
    df_train = df[df['account'].isin(train_accounts)].copy()
    df_train = df_train.merge(train_df[['account', 'flag']], on='account', how='inner')
    # é‡è¦ï¼šflagå·²ç»æ˜¯0(good)/1(bad)ï¼Œç›´æ¥ä½¿ç”¨
    df_train['label'] = df_train['flag']
    
    # æµ‹è¯•é›†ï¼šéœ€è¦é¢„æµ‹çš„è´¦æˆ·
    df_test = df[df['account'].isin(test_accounts)].copy()
    
    print(f"è®­ç»ƒé›†å¤§å°: {df_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {df_test.shape}")
    print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {df_train['label'].value_counts()}")
    
    # 3. ç‰¹å¾å·¥ç¨‹ï¼ˆè®­ç»ƒé›†ï¼‰
    feature_cols = [col for col in df.columns if col != 'account']
    print(f"\nå¼€å§‹å¤„ç†è®­ç»ƒé›†ç‰¹å¾...")
    X_train, scaler = preprocess_features(df_train, feature_cols)
    y_train = df_train['label'].values
    
    print(f"è®­ç»ƒé›†å¤„ç†åç‰¹å¾æ•°: {X_train.shape[1]}")
    
    # 4. äº¤å‰éªŒè¯è¯„ä¼°
    print("\n" + "="*60)
    print("ğŸ“Š å¼€å§‹äº¤å‰éªŒè¯è¯„ä¼°...")
    mean_f1, fold_f1_scores, cv_predictions, cv_indices, optimal_epochs, fold_results, best_fold = cross_validation_training(
        X_train, y_train, cv_folds=10, epochs=150
    )
    
    # 5. å¤„ç†æµ‹è¯•é›†ç‰¹å¾ï¼ˆä½¿ç”¨ç›¸åŒçš„scalerï¼‰
    print(f"\nå¼€å§‹å¤„ç†æµ‹è¯•é›†ç‰¹å¾...")
    X_test = df_test[feature_cols].copy()
    
    # åˆ é™¤åœ¨è®­ç»ƒé›†ä¸­è¢«åˆ é™¤çš„å¸¸æ•°ç‰¹å¾
    X_test = X_test[X_train.columns]  # ä¿æŒç‰¹å¾ä¸€è‡´æ€§
    
    # åº”ç”¨ç›¸åŒçš„é¢„å¤„ç†æ­¥éª¤ï¼ˆä½†ä¸é‡æ–°fit scalerï¼‰
    for col in X_test.columns:
        if 'profit' in col:
            X_test[col] = np.sign(X_test[col]) * np.log1p(np.abs(X_test[col]))
            # ä½¿ç”¨è®­ç»ƒé›†çš„åˆ†ä½æ•°è¿›è¡Œclip
            Q01 = X_train[col].quantile(0.01) if col in X_train.columns else X_test[col].min()
            Q99 = X_train[col].quantile(0.99) if col in X_train.columns else X_test[col].max()
            X_test[col] = np.clip(X_test[col], Q01, Q99)
        elif 'ratio' in col:
            X_test[col] = np.clip(X_test[col], 0, 50)
    
    # åº”ç”¨è®­ç»ƒå¥½çš„scaler
    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test),
        columns=X_test.columns,
        index=X_test.index
    )
    
    print(f"æµ‹è¯•é›†å¤„ç†åå½¢çŠ¶: {X_test_scaled.shape}")
    
    # 6. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    final_model = train_final_model(X_train, y_train, optimal_epochs)
    
    # 7. é¢„æµ‹æµ‹è¯•é›†
    test_results = predict_test_set(final_model, X_test_scaled, df_test['account'].values)
    
    # 8. ä¿å­˜ç»“æœ - ç”Ÿæˆä¸¤ä¸ªç‰ˆæœ¬
    result_dir = "/Users/mannormal/4011/Qi Zihan/result_analysis/prediction_results/"
    os.makedirs(result_dir, exist_ok=True)
    
    # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ (ç”¨äºåˆ†æ)
    # ä¸ºæ¯ä¸ªé¢„æµ‹åˆ†é…å¯¹åº”çš„fold F1åˆ†æ•°
    fold_f1_mapping = []
    current_fold = 0
    predictions_count = 0
    
    for result in fold_results:
        fold_size = len(result['predictions'])
        fold_f1_mapping.extend([result['f1_score']] * fold_size)
        predictions_count += fold_size
    
    cv_results_df = pd.DataFrame({
        'account': df_train.iloc[cv_indices]['account'].values,
        'true_label': df_train.iloc[cv_indices]['label'].values,
        'predicted_label': cv_predictions,
        'fold_f1_score': fold_f1_mapping
    })
    
    cv_filename = f"MLP_deep_cv_analysis_f1_score_{mean_f1:.4f}.csv"
    cv_results_df.to_csv(os.path.join(result_dir, cv_filename), index=False)
    
    # ========== ç¬¬ä¸€ä¸ªæäº¤æ–‡ä»¶ï¼šå…¨éƒ¨æ•°æ®è®­ç»ƒ ==========
    submission_df_full = pd.DataFrame({
        'account': test_results['account'],
        'Predict': test_results['predicted_label']
    })
    
    test_filename_full = f"MLP_deep_submission_FULL_DATA_f1_{mean_f1:.4f}_epochs_{optimal_epochs}.csv"
    submission_df_full.to_csv(os.path.join(result_dir, test_filename_full), index=False)
    
    # ========== ç¬¬äºŒä¸ªæäº¤æ–‡ä»¶ï¼šæœ€ä½³Foldé¢„æµ‹ ==========
    # ä½¿ç”¨æœ€ä½³foldçš„æ•°æ®é‡æ–°è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹
    print(f"\nï¿½ ä½¿ç”¨æœ€ä½³Fold {best_fold['fold_idx']+1} çš„è®¾ç½®é‡æ–°é¢„æµ‹æµ‹è¯•é›†...")
    
    # é‡æ–°è·å–æœ€ä½³foldçš„è®­ç»ƒæ•°æ®
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    all_folds = list(skf.split(X_train, y_train))
    best_train_idx, best_val_idx = all_folds[best_fold['fold_idx']]
    
    X_best_train = X_train.iloc[best_train_idx]
    y_best_train = y_train[best_train_idx]
    
    print(f"æœ€ä½³Foldè®­ç»ƒé›†å¤§å°: {len(X_best_train)} (åŸ90%æ•°æ®çš„ä¸€éƒ¨åˆ†)")
    print(f"æœ€ä½³Fold F1åˆ†æ•°: {best_fold['f1_score']:.4f}")
    print(f"æœ€ä½³Foldè®­ç»ƒè½®æ•°: {best_fold['epochs']}")
    
    # ç”¨æœ€ä½³foldçš„è®¾ç½®è®­ç»ƒæ¨¡å‹
    best_fold_model = train_final_model(X_best_train, y_best_train, best_fold['epochs'])
    
    # é¢„æµ‹æµ‹è¯•é›†
    best_fold_test_results = predict_test_set(best_fold_model, X_test_scaled, df_test['account'].values)
    
    submission_df_best = pd.DataFrame({
        'account': best_fold_test_results['account'],
        'Predict': best_fold_test_results['predicted_label']
    })
    
    test_filename_best = f"MLP_deep_submission_BEST_FOLD_{best_fold['fold_idx']+1}_f1_{best_fold['f1_score']:.4f}_epochs_{best_fold['epochs']}.csv"
    submission_df_best.to_csv(os.path.join(result_dir, test_filename_best), index=False)
    
    print(f"\nğŸ‰ å®Œæˆæ‰€æœ‰ä»»åŠ¡ï¼ç”Ÿæˆäº†ä¸¤ä¸ªæäº¤æ–‡ä»¶ï¼š")
    print(f"ğŸ“Š äº¤å‰éªŒè¯å¹³å‡F1: {mean_f1:.4f}")
    print(f"ğŸ† æœ€ä½³Fold F1: {best_fold['f1_score']:.4f}")
    print(f"ğŸ“ äº¤å‰éªŒè¯åˆ†ææ–‡ä»¶: {cv_filename}")
    print(f"ğŸ“„ å…¨éƒ¨æ•°æ®æäº¤æ–‡ä»¶: {test_filename_full}")
    print(f"ğŸ¥‡ æœ€ä½³Foldæäº¤æ–‡ä»¶: {test_filename_best}")
    
    # å¯¹æ¯”ä¸¤ä¸ªé¢„æµ‹ç»“æœ
    print(f"\nğŸ“Š ä¸¤ä¸ªæäº¤æ–‡ä»¶å¯¹æ¯”:")
    print(f"å…¨éƒ¨æ•°æ®æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ: {submission_df_full['Predict'].value_counts().to_dict()}")
    print(f"æœ€ä½³Foldæ¨¡å‹é¢„æµ‹åˆ†å¸ƒ: {submission_df_best['Predict'].value_counts().to_dict()}")
    
    # è®¡ç®—ä¸¤ä¸ªé¢„æµ‹çš„ä¸€è‡´æ€§
    agreement = (submission_df_full['Predict'] == submission_df_best['Predict']).mean()
    print(f"ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§: {agreement:.4f} ({agreement*100:.2f}%)")
    
    return mean_f1, cv_results_df, submission_df_full, submission_df_best, best_fold

if __name__ == "__main__":
    mean_f1_score, cv_results, submission_full, submission_best, best_fold_info = main()