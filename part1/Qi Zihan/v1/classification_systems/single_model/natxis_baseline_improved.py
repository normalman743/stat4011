import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import f1_score, accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler

print("=== ä½¿ç”¨å·²æå–ç‰¹å¾çš„ NATXIS BASELINE ç³»ç»Ÿ ===")

# 1. åŠ è½½åŸå§‹æ ‡ç­¾æ•°æ®
ta = pd.read_csv('../../original_data/train_acc.csv')
te = pd.read_csv('../../original_data/test_acc_predict.csv')

# 2. åŠ è½½å·²æå–çš„ç‰¹å¾ (ä½¿ç”¨å¢å¼ºç‰ˆ44ç‰¹å¾)
features_df = pd.read_csv('../../feature_extraction/generated_features/all_features_with_categories.csv')

print(f"è®­ç»ƒè´¦æˆ·æ•°: {len(ta)}")
print(f"æµ‹è¯•è´¦æˆ·æ•°: {len(te)}")
print(f"ç‰¹å¾æ•°æ®: {features_df.shape}")
print(f"ç‰¹å¾åˆ—æ•°: {len(features_df.columns)}")

# 3. æ•°æ®é¢„å¤„ç†
ta.loc[ta['flag'] == 0, 'flag'] = -1  # 0æ ‡ç­¾è½¬æ¢ä¸º-1

# 4. å‡†å¤‡è®­ç»ƒæ•°æ®
train_accounts = set(ta['account'].tolist())
test_accounts = set(te['account'].tolist())

# ç­›é€‰è®­ç»ƒæ•°æ®
train_features = features_df[features_df['account'].isin(train_accounts)].copy()
train_features = train_features.merge(ta[['account', 'flag']], on='account', how='inner')

print(f"æœ‰æ•ˆè®­ç»ƒæ•°æ®: {len(train_features)} ä¸ªè´¦æˆ·")
print(f"æ ‡ç­¾åˆ†å¸ƒ: {train_features['flag'].value_counts().to_dict()}")

# 5. é€‰æ‹©æ•°å€¼ç‰¹å¾ (æ’é™¤åˆ†ç±»ç‰¹å¾å’Œaccountåˆ—)
categorical_cols = ['account', 'flag', 'traditional_category', 'volume_category', 
                   'profit_category', 'interaction_category', 'behavior_category']
feature_cols = [col for col in train_features.columns if col not in categorical_cols]

print(f"ä½¿ç”¨ç‰¹å¾æ•°é‡: {len(feature_cols)}")
print("ç‰¹å¾åˆ—è¡¨:", feature_cols[:10], "...")  # æ˜¾ç¤ºå‰10ä¸ªç‰¹å¾

# 6. å‡†å¤‡ç‰¹å¾çŸ©é˜µ
X = train_features[feature_cols].fillna(0)  # å¡«å……ç¼ºå¤±å€¼
y = train_features['flag']

# 7. ç‰¹å¾æ ‡å‡†åŒ– (å¯é€‰)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 8. åˆ’åˆ†è®­ç»ƒéªŒè¯é›†
X_train, X_val, y_train, y_val = train_test_split(
    X_scaled, y, 
    test_size=0.2, 
    random_state=42,
    stratify=y
)

print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
print(f"éªŒè¯é›†å¤§å°: {len(X_val)}")

# 9. è®­ç»ƒæ¨¡å‹
print("è®­ç»ƒRandomForestæ¨¡å‹...")
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=15,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42,
    n_jobs=-1
)

model.fit(X_train, y_train)

# 10. éªŒè¯é›†è¯„ä¼°
y_val_pred = model.predict(X_val)
y_val_binary = np.where(y_val == -1, 0, 1)
y_pred_binary = np.where(y_val_pred == -1, 0, 1)

# è®¡ç®—æŒ‡æ ‡
accuracy = accuracy_score(y_val, y_val_pred)
f1_binary = f1_score(y_val_binary, y_pred_binary, average='binary')
f1_weighted = f1_score(y_val_binary, y_pred_binary, average='weighted')
f1_macro = f1_score(y_val_binary, y_pred_binary, average='macro')

print("\n" + "="*60)
print("æ”¹è¿›çš„ NATXIS BASELINE ç³»ç»Ÿç»“æœ")
print("="*60)
print(f"éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f}")
print(f"F1-Score (binary):   {f1_binary:.4f}")
print(f"F1-Score (weighted): {f1_weighted:.4f}")
print(f"F1-Score (macro):    {f1_macro:.4f}")

# 11. äº¤å‰éªŒè¯
print("\nè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯...")
cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='f1_weighted', n_jobs=-1)
print(f"äº¤å‰éªŒè¯F1åˆ†æ•°: {cv_scores}")
print(f"å¹³å‡CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# 12. ç‰¹å¾é‡è¦æ€§åˆ†æ
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nå‰10ä¸ªé‡è¦ç‰¹å¾:")
print(feature_importance.head(10).to_string(index=False))

# 13. è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_val, y_val_pred))

# 14. æµ‹è¯•é›†é¢„æµ‹
print("\né¢„æµ‹æµ‹è¯•é›†...")
test_features = features_df[features_df['account'].isin(test_accounts)].copy()
X_test = test_features[feature_cols].fillna(0)
X_test_scaled = scaler.transform(X_test)
y_test_pred = model.predict(X_test_scaled)

# ä¿å­˜é¢„æµ‹ç»“æœ
test_predictions = pd.DataFrame({
    'account': test_features['account'],
    'Predict': y_test_pred
})

output_path = '../../result_analysis/prediction_results/natxis_baseline_improved_predictions.csv'
test_predictions.to_csv(output_path, index=False)

print(f"\nğŸ† æ”¹è¿›åçš„ç³»ç»Ÿæ€»ç»“:")
print(f"çœŸå®éªŒè¯F1åˆ†æ•°: {f1_binary:.4f}")
print(f"äº¤å‰éªŒè¯å¹³å‡F1: {cv_scores.mean():.4f}")
print(f"æµ‹è¯•é¢„æµ‹å·²ä¿å­˜åˆ°: {output_path}")
print(f"æµ‹è¯•é¢„æµ‹åˆ†å¸ƒ: {pd.Series(y_test_pred).value_counts().to_dict()}")

print("\n=== NATXIS Baseline æ”¹è¿›ç‰ˆå®Œæˆ ===")