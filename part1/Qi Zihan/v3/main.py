#!/usr/bin/env python3
"""
V3ä¸»ç¨‹åº - äºŒåˆ†æ³•ä¼˜åŒ–ç³»ç»Ÿ
ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸“æ³¨äºè·å¾—æ›´é«˜çš„F1åˆ†æ•°
"""

import os
import pandas as pd
#from upload_module import upload_file
from simulator import simulate_f1 as upload_file
from confusion_calculator import calculate_confusion_from_f1
from binary_search import binary_optimize_accounts

def save_best_result(predictions_dict, f1_score, output_dir="/Users/mannormal/4011/Qi Zihan/v3"):
    """
    ä¿å­˜æœ€ä½³ç»“æœï¼Œæ–‡ä»¶åæ ¼å¼: v{score}.csvn
    
    Args:
        predictions_dict (dict): é¢„æµ‹ç»“æœ {account_id: 0/1}
        f1_score (float): F1åˆ†æ•°
        output_dir (str): è¾“å‡ºç›®å½•
    
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    
    # åˆ›å»ºDataFrame
    predictions_list = []
    for account_id, predict in predictions_dict.items():
        predictions_list.append({"ID": account_id, "Predict": predict})
    
    df = pd.DataFrame(predictions_list)
    
    # ç”Ÿæˆæ–‡ä»¶å
    score_str = f"{f1_score:.8f}".replace(".", "")[:8]  # å–å‰8ä½æ•°å­—
    filename = f"v{score_str}ensemble.csv"
    filepath = os.path.join(output_dir, filename)
    
    # ä¿å­˜æ–‡ä»¶
    df.to_csv(filepath, index=False)
    
    print(f"ğŸ’¾ ä¿å­˜æœ€ä½³ç»“æœ: {filename}")
    print(f"  F1åˆ†æ•°: {f1_score:.6f}")
    print(f"  é¢„æµ‹Bad: {len(df[df['Predict'] == 1])}")
    print(f"  é¢„æµ‹Good: {len(df[df['Predict'] == 0])}")
    
    return filepath

def load_initial_predictions():
    print("=== åˆ†å±‚äºŒåˆ†æ³•ä¼˜åŒ– ===")
    
    scores_df = pd.read_csv("/Users/mannormal/4011/account_scores.csv")
    
    layers = [
        {"id": 1, "name": "[0.0-0.1)", "range": (0.0, 0.1), "initial_guess": 0},
        {"id": 2, "name": "[0.1-0.2)", "range": (0.1, 0.2), "initial_guess": 0},
        {"id": 3, "name": "[0.2-0.5)", "range": (0.2, 0.5), "initial_guess": 0},
        {"id": 4, "name": "[0.5-0.8)", "range": (0.5, 0.8), "initial_guess": 1},
        {"id": 5, "name": "[0.8-1.0]", "range": (0.8, 1.0), "initial_guess": 1}
    ]
    layers.reverse()
    
    final_predictions = {}
    state_file = "/Users/mannormal/4011/Qi Zihan/v3/optimization_state.json"
    total_iterations = 0
    
    if os.path.exists(state_file):
        import json
        with open(state_file, 'r') as f:
            state = json.load(f)
        print(f"æ¢å¤çŠ¶æ€: ä»ç¬¬{state['current_layer']}å±‚å¼€å§‹")
        final_predictions = state['predictions']
        start_layer = state['current_layer']
        total_iterations = state.get('total_iterations', 0)
        print(f"å·²å®Œæˆæ€»è¿­ä»£æ¬¡æ•°: {total_iterations}")
    else:
        start_layer = 1
        final_predictions = {}
        total_iterations = 0
    
    for layer in layers:
        
        if layer['id'] < start_layer:
            continue
            
        print(f"\n=== å¤§è¿­ä»£ v{layer['id']}: {layer['name']} ===")
        
        min_score, max_score = layer["range"]
        if max_score == 1.0:
            layer_accounts = scores_df[(scores_df['predict'] >= min_score) & (scores_df['predict'] <= max_score)]
        else:
            layer_accounts = scores_df[(scores_df['predict'] >= min_score) & (scores_df['predict'] < max_score)]
        
        layer_account_ids = layer_accounts['ID'].tolist()
        print(f"å±‚å†…è´¦æˆ·æ•°: {len(layer_account_ids)}")
        
        if len(layer_account_ids) == 0:
            continue
        
        current_predictions = {}
        for account_id in scores_df['ID']:
            if account_id in final_predictions:
                # å·²ç¡®è®¤çš„è´¦æˆ·ä¿æŒç¡®è®¤çŠ¶æ€
                current_predictions[account_id] = final_predictions[account_id]
            elif account_id in layer_account_ids:
                # å½“å‰å±‚è´¦æˆ·è®¾ä¸ºåˆå§‹çŒœæµ‹ï¼ˆæœªç¡®è®¤çŠ¶æ€-1ä¼šåœ¨äºŒåˆ†æ³•ä¸­å¤„ç†ï¼‰
                current_predictions[account_id] = layer['initial_guess']
            else:
                # å…¶ä»–å±‚è´¦æˆ·è®¾ä¸ºç›¸åå€¼
                current_predictions[account_id] = 1 - layer['initial_guess']
        
        iteration = 1
        temp_file = f"/Users/mannormal/4011/Qi Zihan/v3/v{layer['id']}.{iteration}.csv"
        temp_df = pd.DataFrame([{"ID": aid, "Predict": pred} for aid, pred in current_predictions.items()])
        temp_df.to_csv(temp_file, index=False)
        
        layer_f1 = upload_file(temp_file)
        
        if layer_f1 is not None:
            predicted_bad = sum(current_predictions.values())
            confusion = calculate_confusion_from_f1(layer_f1, predicted_bad)
            
            if confusion:
                print(f"åˆå§‹F1={layer_f1:.6f}, TP={confusion['TP']}, FP={confusion['FP']}, FN={confusion['FN']}, TN={confusion['TN']}")
                
                # åŠ¨æ€è®¡ç®—è¿­ä»£æ¬¡æ•° (åŸºäºlog2 + ç¼“å†²)
                layer_iterations = max(5, int(len(layer_account_ids).bit_length()) + 3)
                print(f"è®¾ç½®è¿­ä»£æ¬¡æ•°: {layer_iterations} (åŸºäºå±‚å¤§å°{len(layer_account_ids)})")
                print(f"å½“å‰æ€»è¿­ä»£æ¬¡æ•°: {total_iterations}")
                
                optimized_predictions, account_status = binary_optimize_accounts(
                    layer_account_ids,
                    current_predictions,
                    upload_file,
                    max_iterations=None  # æ— é™åˆ¶ï¼Œæ‰¾åˆ°ä¸ºæ­¢
                )
                
                # ä¿å­˜ä¼˜åŒ–ç»“æœå’Œç¡®è®¤çŠ¶æ€
                for account_id in layer_account_ids:
                    final_predictions[account_id] = optimized_predictions[account_id]
                
                # ç»Ÿè®¡ç¡®è®¤çŠ¶æ€
                confirmed_count = sum(1 for status in account_status.values() if status != -1)
                unconfirmed_count = len(layer_account_ids) - confirmed_count
                print(f"å±‚ä¼˜åŒ–å®Œæˆ: ç¡®è®¤äº† {confirmed_count}/{len(layer_account_ids)} ä¸ªè´¦æˆ·ï¼Œæœªç¡®è®¤: {unconfirmed_count}")
        
        else:
            print(f"âŒ å±‚ v{layer['id']} ä¸Šä¼ å¤±è´¥ï¼Œä½¿ç”¨initial guess")
            for account_id in layer_account_ids:
                final_predictions[account_id] = layer['initial_guess']
        
        import json
        total_iterations += layer_iterations if 'layer_iterations' in locals() else 0
        
        # åˆ›å»ºå®Œæ•´çš„çŠ¶æ€æ–‡ä»¶ï¼ŒåŒ…å«ç¡®è®¤çŠ¶æ€
        all_account_status = {}
        if 'account_status' in locals():
            all_account_status.update(account_status)
        
        state = {
            'current_layer': layer['id'] + 1,
            'predictions': final_predictions,
            'account_status': all_account_status,  # æ–°å¢ç¡®è®¤çŠ¶æ€
            'total_iterations': total_iterations
        }
        with open(state_file, 'w') as f:
            json.dump(state, f)
        print(f"ä¿å­˜çŠ¶æ€: ç¬¬{layer['id']}å±‚å®Œæˆï¼Œç´¯è®¡è¿­ä»£: {total_iterations}")
        
        # Debug: æ˜¾ç¤ºç¡®è®¤çŠ¶æ€ç»Ÿè®¡
        if all_account_status:
            confirmed_good = sum(1 for status in all_account_status.values() if status == 0)
            confirmed_bad = sum(1 for status in all_account_status.values() if status == 1)
            unconfirmed = sum(1 for status in all_account_status.values() if status == -1)
            print(f"å½“å‰æ€»çŠ¶æ€: ç¡®è®¤good={confirmed_good}, ç¡®è®¤bad={confirmed_bad}, æœªç¡®è®¤={unconfirmed}")
    
    for account_id in scores_df['ID']:
        if account_id not in final_predictions:
            final_predictions[account_id] = 0
    
    bad_count = sum(final_predictions.values())
    good_count = len(final_predictions) - bad_count
    
    print(f"\nâœ… æ‰€æœ‰å±‚ä¼˜åŒ–å®Œæˆ")
    print(f"5å±‚é¢„æµ‹ç»“æœ: Bad={bad_count}, Good={good_count}")
    
    # v7.0æŒç»­ä¼˜åŒ– - æ¯æ¬¡F1ç ´æ–°é«˜å°±è¦†ç›–ä¿å­˜
    print(f"\n=== v7.0 æŒç»­ä¼˜åŒ– - ç›®æ ‡F1=1.0 ===")
    
    # åˆå§‹æµ‹è¯•
    v7_file = "/Users/mannormal/4011/Qi Zihan/v3/v7.0.csv"
    temp_df = pd.DataFrame([{"ID": aid, "Predict": pred} for aid, pred in final_predictions.items()])
    temp_df.to_csv(v7_file, index=False)
    
    best_f1 = upload_file(v7_file)
    best_predictions = final_predictions.copy()
    
    print(f"åˆå§‹F1={best_f1:.6f}")
    
    # æŒç»­ä¼˜åŒ–ç›´åˆ°F1=1æˆ–æ— æ³•æå‡
    max_rounds = 20
    scores_dict = dict(zip(scores_df['ID'], scores_df['predict']))
    tested_candidates = set()  # è®°å½•å·²æµ‹è¯•çš„å€™é€‰è´¦æˆ·ç»„åˆ
    no_improvement_rounds = 0  # è¿ç»­æ— æ”¹è¿›è½®æ¬¡è®¡æ•°
    
    for round_num in range(1, max_rounds + 1):
        print(f"\n--- v7.0 è½®æ¬¡ {round_num} ---")
        
        # è®¡ç®—å½“å‰æ··æ·†çŸ©é˜µ
        predicted_bad = sum(best_predictions.values())
        confusion = calculate_confusion_from_f1(best_f1, predicted_bad)
        
        if confusion['FP'] == 0 and confusion['FN'] == 0:
            print("ğŸ‰ è¾¾åˆ°å®Œç¾F1=1.0!")
            break
        
        print(f"TP={confusion['TP']}, FP={confusion['FP']}, FN={confusion['FN']}, TN={confusion['TN']}")
        
        # é€‰æ‹©ä¼˜åŒ–ç›®æ ‡ - ä½¿ç”¨æ¸è¿›ç­–ç•¥é¿å…é‡å¤æµ‹è¯•
        candidates = []
        
        # è®¡ç®—éœ€è¦æµ‹è¯•çš„è´¦æˆ·æ•°é‡ï¼ˆé€æ¸å¢åŠ èŒƒå›´ï¼‰
        fp_test_count = min(confusion['FP'], max(1, confusion['FP'] // (round_num + 1)))
        fn_test_count = min(confusion['FN'], max(1, confusion['FN'] // (round_num + 1)))
        
        if confusion['FP'] > 0:
            # æœ‰FPï¼šä¼˜åŒ–åˆ†æ•°æœ€ä½çš„badé¢„æµ‹è´¦æˆ·
            bad_accounts = [aid for aid, pred in best_predictions.items() if pred == 1]
            # è·³è¿‡å‰é¢è½®æ¬¡å·²æµ‹è¯•çš„è´¦æˆ·
            skip_count = (round_num - 1) * fp_test_count
            fp_candidates = sorted(bad_accounts, key=lambda x: scores_dict[x])[skip_count:skip_count + fp_test_count]
            candidates.extend(fp_candidates)
            print(f"ä¼˜åŒ–{len(fp_candidates)}ä¸ªå¯èƒ½çš„FPè´¦æˆ· (è·³è¿‡å‰{skip_count}ä¸ª)")
        
        if confusion['FN'] > 0:
            # æœ‰FNï¼šä¼˜åŒ–åˆ†æ•°æœ€é«˜çš„goodé¢„æµ‹è´¦æˆ·
            good_accounts = [aid for aid, pred in best_predictions.items() if pred == 0]
            # è·³è¿‡å‰é¢è½®æ¬¡å·²æµ‹è¯•çš„è´¦æˆ·
            skip_count = (round_num - 1) * fn_test_count
            fn_candidates = sorted(good_accounts, key=lambda x: scores_dict[x], reverse=True)[skip_count:skip_count + fn_test_count]
            candidates.extend(fn_candidates)
            print(f"ä¼˜åŒ–{len(fn_candidates)}ä¸ªå¯èƒ½çš„FNè´¦æˆ· (è·³è¿‡å‰{skip_count}ä¸ª)")
        
        if not candidates:
            print("æ²¡æœ‰æ›´å¤šå¯ä¼˜åŒ–çš„è´¦æˆ·")
            break
        
        # æ£€æŸ¥æ˜¯å¦å·²æµ‹è¯•è¿‡è¿™ç»„å€™é€‰è´¦æˆ·
        candidates_key = tuple(sorted(candidates))
        if candidates_key in tested_candidates:
            print("æ­¤ç»„åˆå·²æµ‹è¯•è¿‡ï¼Œå°è¯•æ‰©å¤§èŒƒå›´")
            # å¦‚æœå·²æµ‹è¯•è¿‡ï¼Œå¢åŠ æµ‹è¯•èŒƒå›´
            if confusion['FP'] > 0:
                bad_accounts = [aid for aid, pred in best_predictions.items() if pred == 1]
                fp_candidates = sorted(bad_accounts, key=lambda x: scores_dict[x])[:min(len(bad_accounts), confusion['FP'] + round_num)]
                candidates.extend(fp_candidates)
            if confusion['FN'] > 0:
                good_accounts = [aid for aid, pred in best_predictions.items() if pred == 0]
                fn_candidates = sorted(good_accounts, key=lambda x: scores_dict[x], reverse=True)[:min(len(good_accounts), confusion['FN'] + round_num)]
                candidates.extend(fn_candidates)
            candidates = list(set(candidates))  # å»é‡
            candidates_key = tuple(sorted(candidates))
            
            if candidates_key in tested_candidates:
                print("æ‰©å¤§èŒƒå›´åä»æ˜¯é‡å¤ç»„åˆï¼Œåœæ­¢ä¼˜åŒ–")
                break
        
        tested_candidates.add(candidates_key)
        
        # è¿›è¡Œä¼˜åŒ–
        optimized_predictions, v7_account_status = binary_optimize_accounts(
            candidates,
            best_predictions,
            upload_file,
            max_iterations=15
        )
        total_iterations += 15
        
        # æµ‹è¯•æ–°ç»“æœ
        test_df = pd.DataFrame([{"ID": aid, "Predict": pred} for aid, pred in optimized_predictions.items()])
        test_df.to_csv(v7_file, index=False)
        
        new_f1 = upload_file(v7_file)
        
        if new_f1 > best_f1:
            print(f"ğŸ‰ F1ç ´æ–°é«˜: {best_f1:.6f} â†’ {new_f1:.6f}")
            best_f1 = new_f1
            best_predictions = optimized_predictions.copy()
            print(f"è¦†ç›–ä¿å­˜ v7.0.csv")
            no_improvement_rounds = 0  # é‡ç½®æ— æ”¹è¿›è®¡æ•°
        else:
            print(f"æœ¬è½®æ— æå‡: {new_f1:.6f} â‰¤ {best_f1:.6f}")
            no_improvement_rounds += 1
            
        # å¦‚æœè¿ç»­3è½®æ— æ”¹è¿›ï¼Œåœæ­¢ä¼˜åŒ–
        if no_improvement_rounds >= 3:
            print("è¿ç»­3è½®æ— æ”¹è¿›ï¼Œåœæ­¢ä¼˜åŒ–")
            break
            
        if best_f1 >= 0.9999:
            print("ğŸ‰ æ¥è¿‘å®Œç¾F1!")
            break
    
    print(f"\nv7.0æœ€ç»ˆç»“æœ: F1={best_f1:.6f}")
    print(f"æ€»è®¡å®Œæˆè¿­ä»£æ¬¡æ•°: {total_iterations}")
    
    # æœ€ç»ˆä¿å­˜çŠ¶æ€
    import json
    final_state = {
        'current_layer': 6,  # è¡¨ç¤ºå·²å®Œæˆæ‰€æœ‰å±‚
        'predictions': best_predictions,
        'total_iterations': total_iterations,
        'final_f1': best_f1
    }
    with open(state_file, 'w') as f:
        json.dump(final_state, f, indent=2)
    print(f"ä¿å­˜æœ€ç»ˆçŠ¶æ€: æ€»è¿­ä»£{total_iterations}æ¬¡ï¼ŒF1={best_f1:.6f}")
    
    return best_predictions


def main():
    print("=== V3äºŒåˆ†æ³•ä¼˜åŒ–ç³»ç»Ÿ ===")
    result = load_initial_predictions()
    print(f"âœ… å®Œæˆ: {result}")
    reset_score = upload_file("/Users/mannormal/4011/Qi Zihan/v2/results/Transformer_basic_submission_FULL_DATA_f1_0.8913_epochs_130.256.csv")
    print(f"é‡æ–°ä¸Šä¼ æ—§ç‰ˆæœ¬æ–‡ä»¶æµ‹è¯•F1: {reset_score}")
if __name__ == "__main__":
    main()