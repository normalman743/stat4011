import pandas as pd
import numpy as np
import json
import os
from time import time

class AlgorithmSimulator:
    def __init__(self, n_accounts=10000, bad_ratio=0.1):
        self.n_accounts = n_accounts
        self.bad_ratio = bad_ratio
        self.true_bad = int(n_accounts * bad_ratio)
        self.true_good = n_accounts - self.true_bad
        
        # æµ‹è¯•å‚æ•°
        self.batch_size = 50
        self.f1_threshold = 0.01
        
        print("=== ç®—æ³•æ•ˆç‡æµ‹è¯•ç³»ç»Ÿ ===")
        print(f"æ€»è´¦æˆ·æ•°: {n_accounts}")
        print(f"çœŸå®åˆ†å¸ƒ: Bad={self.true_bad} ({bad_ratio*100:.1f}%), Good={self.true_good} ({(1-bad_ratio)*100:.1f}%)")
    
    def generate_test_data(self):
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        print("\\nç”Ÿæˆæµ‹è¯•æ•°æ®...")
        
        # 1. ç”ŸæˆçœŸå®æ ‡ç­¾ (1:bad, 0:good)
        true_labels = [1] * self.true_bad + [0] * self.true_good
        np.random.shuffle(true_labels)
        
        # 2. ä¸ºæ¯ä¸ªè´¦æˆ·ç”Ÿæˆæ¦‚ç‡åˆ†æ•°
        accounts = []
        for i in range(self.n_accounts):
            account_id = f"test_{i:06d}"
            true_label = true_labels[i]
            
            # æ ¹æ®çœŸå®æ ‡ç­¾å’Œè§„åˆ™ç”Ÿæˆåˆ†æ•°
            if true_label == 1:  # çœŸbadè´¦æˆ·
                # 70%æ¦‚ç‡è·å¾—é«˜åˆ†(0.6-0.9)ï¼Œ30%æ¦‚ç‡è·å¾—ä½åˆ†(0.1-0.4)
                if np.random.random() < 0.7:
                    score = np.random.uniform(0.6, 0.9)
                else:
                    score = np.random.uniform(0.1, 0.4)
            else:  # çœŸgoodè´¦æˆ·
                # 80%æ¦‚ç‡è·å¾—ä½åˆ†(0.1-0.4)ï¼Œ20%æ¦‚ç‡è·å¾—é«˜åˆ†(0.6-0.9)
                if np.random.random() < 0.8:
                    score = np.random.uniform(0.1, 0.4)
                else:
                    score = np.random.uniform(0.6, 0.9)
            
            accounts.append({
                'ID': account_id,
                'predict': score,
                'true_label': true_label  # è¿™ä¸ªåœ¨çœŸå®ç³»ç»Ÿä¸­ä¸å­˜åœ¨ï¼Œä»…ç”¨äºæ¨¡æ‹Ÿ
            })
        
        # ä¿å­˜æ•°æ®
        accounts_df = pd.DataFrame(accounts)
        accounts_df[['ID', 'predict']].to_csv('test_account_scores.csv', index=False, float_format='%.6f')
        
        # ç”ŸæˆåŸºçº¿æäº¤æ–‡ä»¶ï¼ˆåŸºäºæ¦‚ç‡é˜ˆå€¼0.5ï¼‰
        baseline_submission = []
        for acc in accounts:
            baseline_predict = 1 if acc['predict'] > 0.5 else 0
            baseline_submission.append({
                'ID': acc['ID'], 
                'Predict': baseline_predict,
                'true_label': acc['true_label']  # ä»…ç”¨äºæ¨¡æ‹Ÿ
            })
        
        baseline_df = pd.DataFrame(baseline_submission)
        baseline_df[['ID', 'Predict']].to_csv('test_baseline_submission.csv', index=False)
        
        # è®¡ç®—åŸºçº¿æ€§èƒ½
        baseline_cm = self.calculate_true_confusion_matrix(baseline_df)
        baseline_f1 = self.calculate_f1_from_cm(baseline_cm)
        
        print(f"æ•°æ®ç”Ÿæˆå®Œæˆ:")
        print(f"  test_account_scores.csv: {len(accounts)} è´¦æˆ·")
        print(f"  test_baseline_submission.csv: åŸºçº¿F1={baseline_f1:.6f}")
        print(f"  åŸºçº¿æ··æ·†çŸ©é˜µ: TP={baseline_cm['TP']}, FP={baseline_cm['FP']}, FN={baseline_cm['FN']}, TN={baseline_cm['TN']}")
        
        # éªŒè¯åˆ†æ•°åˆ†å¸ƒ
        high_score_accounts = [a for a in accounts if a['predict'] > 0.5]
        low_score_accounts = [a for a in accounts if a['predict'] <= 0.5]
        
        high_score_bad_ratio = sum(1 for a in high_score_accounts if a['true_label'] == 1) / len(high_score_accounts) if high_score_accounts else 0
        low_score_bad_ratio = sum(1 for a in low_score_accounts if a['true_label'] == 1) / len(low_score_accounts) if low_score_accounts else 0
        
        print(f"\\nåˆ†æ•°åˆ†å¸ƒéªŒè¯:")
        print(f"  é«˜åˆ†è´¦æˆ·(>0.5): {len(high_score_accounts)}ä¸ª, çœŸbadæ¯”ä¾‹: {high_score_bad_ratio:.3f}")
        print(f"  ä½åˆ†è´¦æˆ·(<=0.5): {len(low_score_accounts)}ä¸ª, çœŸbadæ¯”ä¾‹: {low_score_bad_ratio:.3f}")
        
        return accounts, baseline_df, baseline_f1, baseline_cm
    
    def calculate_true_confusion_matrix(self, submission_df):
        """è®¡ç®—çœŸå®æ··æ·†çŸ©é˜µï¼ˆæ¨¡æ‹Ÿç¯å¢ƒç‰¹æœ‰ï¼‰"""
        tp = len(submission_df[(submission_df['Predict'] == 1) & (submission_df['true_label'] == 1)])
        fp = len(submission_df[(submission_df['Predict'] == 1) & (submission_df['true_label'] == 0)])
        fn = len(submission_df[(submission_df['Predict'] == 0) & (submission_df['true_label'] == 1)])
        tn = len(submission_df[(submission_df['Predict'] == 0) & (submission_df['true_label'] == 0)])
        
        return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn}
    
    def calculate_f1_from_cm(self, cm):
        """ä»æ··æ·†çŸ©é˜µè®¡ç®—F1åˆ†æ•°"""
        precision = cm['TP'] / (cm['TP'] + cm['FP']) if (cm['TP'] + cm['FP']) > 0 else 0
        recall = cm['TP'] / (cm['TP'] + cm['FN']) if (cm['TP'] + cm['FN']) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        return f1
    
    def simulate_submission(self, submission_df):
        """æ¨¡æ‹Ÿæäº¤å¹¶è¿”å›F1åˆ†æ•°ï¼ˆæ›¿ä»£çœŸå®APIè°ƒç”¨ï¼‰"""
        cm = self.calculate_true_confusion_matrix(submission_df)
        f1 = self.calculate_f1_from_cm(cm)
        return f1
    
    def initialize_test_state(self, accounts, baseline_df, baseline_f1, baseline_cm):
        """åˆå§‹åŒ–æµ‹è¯•çŠ¶æ€"""
        print("\\nåˆå§‹åŒ–éªŒè¯çŠ¶æ€...")
        
        # æŒ‰ç­–ç•¥åˆ†ç»„è´¦æˆ·
        unconfirmed_accounts = []
        for acc in accounts:
            if 0 < acc['predict'] < 1:  # æœªç¡®è®¤çš„è´¦æˆ·
                unconfirmed_accounts.append({
                    'ID': acc['ID'],
                    'score': acc['predict'],
                    'current_predict': 1 if acc['predict'] > 0.5 else 0,
                    'true_label': acc['true_label']  # ä»…ç”¨äºéªŒè¯
                })
        
        # æ’åºï¼šä¼˜å…ˆå¤„ç†é«˜æ¦‚ç‡badè´¦æˆ·ï¼ˆä»badå¾€goodèµ°ï¼‰
        suspected_bad = sorted([a for a in unconfirmed_accounts if a['score'] > 0.5], 
                              key=lambda x: x['score'], reverse=True)
        suspected_good = sorted([a for a in unconfirmed_accounts if a['score'] < 0.5], 
                               key=lambda x: x['score'])
        
        state = {
            'round': 0,
            'baseline_f1': baseline_f1,
            'baseline_cm': baseline_cm,
            'baseline_bad': len(baseline_df[baseline_df['Predict'] == 1]),
            'baseline_good': len(baseline_df[baseline_df['Predict'] == 0]),
            'suspected_bad_queue': suspected_bad,
            'suspected_good_queue': suspected_good,
            'confirmed_accounts': {},
            'test_history': [],
            'true_labels': {acc['ID']: acc['true_label'] for acc in accounts}  # ä»…ç”¨äºéªŒè¯
        }
        
        print(f"  Suspected bad queue: {len(suspected_bad)}")
        print(f"  Suspected good queue: {len(suspected_good)}")
        
        return state
    
    def select_test_batch(self, state):
        """é€‰æ‹©æµ‹è¯•æ‰¹æ¬¡"""
        if len(state['suspected_bad_queue']) >= self.batch_size:
            batch = state['suspected_bad_queue'][:self.batch_size]
            test_direction = "bad_to_good"
            print(f"é€‰æ‹© {len(batch)} ä¸ªé«˜æ¦‚ç‡badè´¦æˆ·ï¼Œæµ‹è¯•æ”¹æˆgood (1â†’0)")
        elif len(state['suspected_good_queue']) >= self.batch_size:
            batch = state['suspected_good_queue'][:self.batch_size]
            test_direction = "good_to_bad"
            print(f"é€‰æ‹© {len(batch)} ä¸ªé«˜æ¦‚ç‡goodè´¦æˆ·ï¼Œæµ‹è¯•æ”¹æˆbad (0â†’1)")
        else:
            remaining = state['suspected_bad_queue'] + state['suspected_good_queue']
            if len(remaining) == 0:
                return None, None
            
            batch = remaining[:min(self.batch_size, len(remaining))]
            test_direction = "mixed"
            print(f"é€‰æ‹©å‰©ä½™ {len(batch)} ä¸ªè´¦æˆ·è¿›è¡Œæœ€ç»ˆæµ‹è¯•")
        
        return batch, test_direction
    
    def create_test_submission(self, state, test_batch, test_direction):
        """åˆ›å»ºæµ‹è¯•æäº¤"""
        baseline_df = pd.read_csv('test_baseline_submission.csv')
        
        submission_data = []
        test_account_ids = [acc['ID'] for acc in test_batch]
        
        for _, row in baseline_df.iterrows():
            account_id = row['ID']
            
            if account_id in state['confirmed_accounts']:
                predict = state['confirmed_accounts'][account_id]['label']
            elif account_id in test_account_ids:
                if test_direction == "bad_to_good":
                    predict = 0
                elif test_direction == "good_to_bad":
                    predict = 1
                else:  # mixed
                    acc_info = next(acc for acc in test_batch if acc['ID'] == account_id)
                    predict = 0 if acc_info['score'] > 0.5 else 1
            else:
                predict = row['Predict']
            
            submission_data.append({
                'ID': account_id, 
                'Predict': predict,
                'true_label': state['true_labels'][account_id]  # ä»…ç”¨äºæ¨¡æ‹Ÿ
            })
        
        return pd.DataFrame(submission_data)
    
    def analyze_test_results(self, state, test_batch, test_direction, new_f1):
        """åˆ†ææµ‹è¯•ç»“æœï¼ˆç®€åŒ–ç‰ˆï¼Œé‡ç‚¹éªŒè¯ç®—æ³•é€»è¾‘ï¼‰"""
        print(f"\\n=== åˆ†æç¬¬{state['round']}è½®æµ‹è¯•ç»“æœ ===")
        
        f1_change = new_f1 - state['baseline_f1']
        print(f"F1å˜åŒ–: {f1_change:+.4f}")
        
        # éªŒè¯ç®—æ³•å‡†ç¡®æ€§ï¼šè®¡ç®—æµ‹è¯•æ‰¹æ¬¡çš„çœŸå®æ ‡ç­¾åˆ†å¸ƒ
        true_bad_in_batch = sum(1 for acc in test_batch if acc['true_label'] == 1)
        true_good_in_batch = len(test_batch) - true_bad_in_batch
        
        print(f"æµ‹è¯•æ‰¹æ¬¡çœŸå®åˆ†å¸ƒ: {true_bad_in_batch}ä¸ªçœŸbad, {true_good_in_batch}ä¸ªçœŸgood")
        
        confirmed_count = 0
        
        # ç®€åŒ–çš„ç¡®è®¤é€»è¾‘
        if abs(f1_change) > self.f1_threshold:
            if test_direction == "bad_to_good" and f1_change < -self.f1_threshold:
                # F1ä¸‹é™æ˜¾è‘—ï¼Œç¡®è®¤è¿™æ‰¹è´¦æˆ·å¤§å¤šæ˜¯bad
                print(f"âœ… ç®—æ³•åˆ¤æ–­ï¼šè¿™æ‰¹è´¦æˆ·å¤§å¤šæ˜¯bad")
                for acc in test_batch:
                    state['confirmed_accounts'][acc['ID']] = {
                        'label': 1 if acc['score'] > np.median([a['score'] for a in test_batch]) else 0,
                        'confidence': 0.9,
                        'true_label': acc['true_label']  # ä»…ç”¨äºéªŒè¯
                    }
                    confirmed_count += 1
                    
            elif test_direction == "bad_to_good" and f1_change > self.f1_threshold:
                # F1æå‡ï¼Œç¡®è®¤è¿™æ‰¹è´¦æˆ·å¤§å¤šæ˜¯good
                print(f"âœ… ç®—æ³•åˆ¤æ–­ï¼šè¿™æ‰¹è´¦æˆ·å¤§å¤šæ˜¯good")
                for acc in test_batch:
                    state['confirmed_accounts'][acc['ID']] = {
                        'label': 0 if acc['score'] < np.median([a['score'] for a in test_batch]) else 1,
                        'confidence': 0.9,
                        'true_label': acc['true_label']  # ä»…ç”¨äºéªŒè¯
                    }
                    confirmed_count += 1
                    
            elif test_direction == "good_to_bad":
                # ç±»ä¼¼é€»è¾‘ï¼Œä½†æ–¹å‘ç›¸å
                if f1_change > self.f1_threshold:
                    print(f"âœ… ç®—æ³•åˆ¤æ–­ï¼šè¿™æ‰¹è´¦æˆ·å¤§å¤šæ˜¯bad")
                else:
                    print(f"âœ… ç®—æ³•åˆ¤æ–­ï¼šè¿™æ‰¹è´¦æˆ·å¤§å¤šæ˜¯good")
                
                for acc in test_batch:
                    predicted_label = 1 if f1_change > 0 else 0
                    state['confirmed_accounts'][acc['ID']] = {
                        'label': predicted_label,
                        'confidence': 0.8,
                        'true_label': acc['true_label']
                    }
                    confirmed_count += 1
        else:
            print(f"âš ï¸  F1å˜åŒ–å¤ªå°({f1_change:+.4f})ï¼Œæ— æ³•ç¡®å®š")
        
        # è®¡ç®—ç®—æ³•å‡†ç¡®æ€§
        if confirmed_count > 0:
            correct_predictions = sum(1 for acc_id, info in state['confirmed_accounts'].items() 
                                    if acc_id in [acc['ID'] for acc in test_batch] and 
                                    info['label'] == info['true_label'])
            accuracy = correct_predictions / confirmed_count
            print(f"æœ¬è½®ç®—æ³•å‡†ç¡®ç‡: {accuracy:.3f} ({correct_predictions}/{confirmed_count})")
        
        # ä»é˜Ÿåˆ—ä¸­ç§»é™¤å·²ç¡®è®¤çš„è´¦æˆ·
        confirmed_ids = set(state['confirmed_accounts'].keys())
        state['suspected_bad_queue'] = [acc for acc in state['suspected_bad_queue'] 
                                       if acc['ID'] not in confirmed_ids]
        state['suspected_good_queue'] = [acc for acc in state['suspected_good_queue'] 
                                        if acc['ID'] not in confirmed_ids]
        
        return confirmed_count
    
    def run_simulation(self, max_rounds=20):
        """è¿è¡Œå®Œæ•´çš„æ¨¡æ‹Ÿæµ‹è¯•"""
        start_time = time()
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        accounts, baseline_df, baseline_f1, baseline_cm = self.generate_test_data()
        
        # åˆå§‹åŒ–çŠ¶æ€
        state = self.initialize_test_state(accounts, baseline_df, baseline_f1, baseline_cm)
        
        print(f"\\n{'='*60}")
        print("å¼€å§‹ç®—æ³•éªŒè¯")
        print(f"{'='*60}")
        
        total_submissions = 0
        
        for round_num in range(1, max_rounds + 1):
            state['round'] = round_num
            
            print(f"\\n--- ç¬¬ {round_num} è½® ---")
            
            # é€‰æ‹©æµ‹è¯•æ‰¹æ¬¡
            test_batch, test_direction = self.select_test_batch(state)
            if not test_batch:
                print("æ‰€æœ‰è´¦æˆ·å·²ç¡®è®¤å®Œæ¯•ï¼")
                break
            
            # åˆ›å»ºæµ‹è¯•æäº¤
            test_submission = self.create_test_submission(state, test_batch, test_direction)
            
            # æ¨¡æ‹Ÿæäº¤å¹¶è·å–F1
            new_f1 = self.simulate_submission(test_submission)
            total_submissions += 1
            
            print(f"æ¨¡æ‹ŸF1åˆ†æ•°: {new_f1:.6f}")
            
            # åˆ†æç»“æœ
            confirmed_count = self.analyze_test_results(state, test_batch, test_direction, new_f1)
            
            # è®°å½•å†å²
            state['test_history'].append({
                'round': round_num,
                'test_direction': test_direction,
                'batch_size': len(test_batch),
                'f1_score': new_f1,
                'confirmed_count': confirmed_count
            })
            
            # æ£€æŸ¥å®ŒæˆçŠ¶æ€
            total_unconfirmed = len(state['suspected_bad_queue']) + len(state['suspected_good_queue'])
            total_confirmed = len(state['confirmed_accounts'])
            
            print(f"è¿›åº¦: å·²ç¡®è®¤ {total_confirmed}, å‰©ä½™ {total_unconfirmed}")
            
            if total_unconfirmed == 0:
                print("\\nğŸ‰ æ‰€æœ‰è´¦æˆ·éªŒè¯å®Œæˆï¼")
                break
        
        end_time = time()
        
        # æœ€ç»ˆç»Ÿè®¡
        self.generate_simulation_report(state, total_submissions, end_time - start_time)
        
        return state
    
    def generate_simulation_report(self, state, total_submissions, elapsed_time):
        """ç”Ÿæˆæ¨¡æ‹Ÿæµ‹è¯•æŠ¥å‘Š"""
        print(f"\\n{'='*60}")
        print("ç®—æ³•éªŒè¯æŠ¥å‘Š")
        print(f"{'='*60}")
        
        total_confirmed = len(state['confirmed_accounts'])
        
        # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
        if total_confirmed > 0:
            correct_predictions = sum(1 for info in state['confirmed_accounts'].values() 
                                    if info['label'] == info['true_label'])
            overall_accuracy = correct_predictions / total_confirmed
        else:
            overall_accuracy = 0
        
        # æŒ‰è½®æ¬¡ç»Ÿè®¡
        confirmed_by_round = [h['confirmed_count'] for h in state['test_history']]
        f1_progression = [h['f1_score'] for h in state['test_history']]
        
        print(f"æ€§èƒ½æŒ‡æ ‡:")
        print(f"  æ€»è½®æ•°: {state['round']}")
        print(f"  æ€»æäº¤æ¬¡æ•°: {total_submissions}")
        print(f"  ç”¨æ—¶: {elapsed_time:.2f} ç§’")
        print(f"  å¹³å‡æ¯è½®: {elapsed_time/state['round']:.3f} ç§’")
        
        print(f"\\nç®—æ³•æ•ˆæœ:")
        print(f"  æ€»ç¡®è®¤è´¦æˆ·: {total_confirmed}/{self.n_accounts} ({total_confirmed/self.n_accounts*100:.1f}%)")
        print(f"  ç®—æ³•æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.3f}")
        print(f"  å‰©ä½™æœªç¡®è®¤: {self.n_accounts - total_confirmed}")
        
        print(f"\\næ”¶æ•›åˆ†æ:")
        print(f"  æ¯è½®ç¡®è®¤æ•°: {confirmed_by_round}")
        print(f"  F1è¿›å±•: {[f'{f1:.4f}' for f1 in f1_progression[:5]]}...")
        
        # ç†è®ºvså®é™…
        theoretical_max_rounds = int(np.ceil(np.log2(self.n_accounts)))
        efficiency = theoretical_max_rounds / state['round'] if state['round'] > 0 else 0
        
        print(f"\\nå¤æ‚åº¦åˆ†æ:")
        print(f"  ç†è®ºæœ€å¤§è½®æ•°: {theoretical_max_rounds} (logâ‚‚({self.n_accounts}))")
        print(f"  å®é™…è½®æ•°: {state['round']}")
        print(f"  ç®—æ³•æ•ˆç‡: {efficiency:.2f}")
        
        # ä¿å­˜æŠ¥å‘Š
        report = {
            'parameters': {
                'n_accounts': self.n_accounts,
                'bad_ratio': self.bad_ratio,
                'batch_size': self.batch_size,
                'f1_threshold': self.f1_threshold
            },
            'results': {
                'total_rounds': state['round'],
                'total_submissions': total_submissions,
                'elapsed_time': elapsed_time,
                'total_confirmed': total_confirmed,
                'overall_accuracy': overall_accuracy,
                'theoretical_max_rounds': theoretical_max_rounds,
                'efficiency': efficiency
            },
            'history': state['test_history']
        }
        
        with open('test_simulation_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\\nğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: test_simulation_report.json")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    # æµ‹è¯•ä¸åŒè§„æ¨¡
    test_cases = [
        {'n_accounts': 1000, 'bad_ratio': 0.1},
        # {'n_accounts': 5000, 'bad_ratio': 0.1},
        # {'n_accounts': 10000, 'bad_ratio': 0.1}
    ]
    
    for i, params in enumerate(test_cases, 1):
        print(f"\\n{'#'*80}")
        print(f"æµ‹è¯•æ¡ˆä¾‹ {i}: {params['n_accounts']} è´¦æˆ·, {params['bad_ratio']*100:.0f}% bad")
        print(f"{'#'*80}")
        
        simulator = AlgorithmSimulator(**params)
        final_state = simulator.run_simulation()
        
        print(f"\\næµ‹è¯•æ¡ˆä¾‹ {i} å®Œæˆï¼")

if __name__ == "__main__":
    main()