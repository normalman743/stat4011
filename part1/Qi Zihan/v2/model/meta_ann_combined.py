import pandas as pd
import numpy as np
import os
import argparse
from tqdm import tqdm
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim
import warnings
import random
from dataclasses import dataclass
from pathlib import Path
from scipy.optimize import minimize_scalar
try:
    from imblearn.over_sampling import SMOTE
    IMBLEARN_AVAILABLE = True
except ImportError:
    print("imbalanced-learn not installed. To enable SMOTE, please install it via 'pip install imbalanced-learn'")
    input()
    print("âš ï¸ Warning: imbalanced-learn not available. SMOTE functionality will be disabled.")
    IMBLEARN_AVAILABLE = False

warnings.filterwarnings('ignore')

# Constants for consistent label handling
GOOD_LABEL = 0  # Good accounts
BAD_LABEL = 1   # Bad accounts (fraud)

version = 'v3.2refined'  # Refined: removed redundant phases, enhanced training output, immediate test prediction

@dataclass
class ModelConfig:
    """Configuration class for model hyperparameters"""
    n_rf_models: int = 50  # å‡å°‘åŸºç¡€æ¨¡å‹æ•°é‡
    meta_ann_hidden: int = 64  # ç®€åŒ–ç½‘ç»œç»“æ„
    cv_folds: int = 5
    meta_ann_epochs: int = 500
    meta_ann_patience: int = 30
    meta_ann_dropout: float = 0.3
    meta_ann_lr: float = 1e-3
    meta_ann_weight_decay: float = 1e-4
    holdout_ratio: float = 0.2  # æ–°å¢20% hold-outéªŒè¯é›†
    
class PathConfig:
    """Configuration for file paths"""
    def __init__(self, base_dir: str = '/Users/mannormal/4011/Qi Zihan'):
        self.base_dir = Path(base_dir)
        self.features_path = self.base_dir / 'v2/feature_extraction/result/features_cleaned_no_leakage1.csv'
        self.train_path = self.base_dir / 'original_data/train_acc.csv'
        self.test_path = self.base_dir / 'original_data/test_acc_predict.csv'
        self.results_dir = self.base_dir / 'v2/results'
        self.models_dir = self.base_dir / 'v2/models'
        self.strategy_base_dir = self.base_dir / 'v1/classification_strategies'
        
        # Create directories if they don't exist
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.models_dir.mkdir(parents=True, exist_ok=True)

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.backends.mps.is_available():
        # MPSè®¾å¤‡è®¾ç½®ç¡®å®šæ€§ï¼ˆå¦‚æœæ”¯æŒçš„è¯ï¼‰
        torch.mps.manual_seed(seed)

seed_num = 13
set_seed(seed_num)

# Initialize configurations
CONFIG = ModelConfig()
PATHS = PathConfig()

print("=== ULTRA Multi-Strategy Ensemble System with Meta-ANN ===")

# =====================================================
# æ•°æ®åŠ è½½å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
# =====================================================
def load_strategy_categories():
    strategy_paths = {
        'traditional': '/Users/mannormal/4011/Qi Zihan/v1/classification_strategies/traditional_4types/traditional_category_mapping.csv',
        'volume': '/Users/mannormal/4011/Qi Zihan/v1/classification_strategies/volume_based/volume_category_mapping.csv',
        'profit': '/Users/mannormal/4011/Qi Zihan/v1/classification_strategies/profit_based/profit_category_mapping.csv',
        'interaction': '/Users/mannormal/4011/Qi Zihan/v1/classification_strategies/interaction_based/interaction_category_mapping.csv',
        'behavior': '/Users/mannormal/4011/Qi Zihan/v1/classification_strategies/behavior_based/behavior_category_mapping.csv'
    }
    
    strategy_data = {}
    print("\n=== Loading Classification Strategies ===")
    for strategy_name, path in strategy_paths.items():
        if os.path.exists(path):
            strategy_data[strategy_name] = pd.read_csv(path)
            print(f"âœ… {strategy_name}: {len(strategy_data[strategy_name])} accounts")
        else:
            print(f"âŒ {strategy_name}: File not found")
    
    return strategy_data

def classify_account_type_improved(row):
    # è®¡ç®—å‰å‘åå‘äº¤æ˜“å¼ºåº¦
    forward_strength = (row['A_fprofit'] + row['B_fprofit']) / max(row['A_fsize'] + row['B_fsize'], 1)
    backward_strength = (row['A_bprofit'] + row['B_bprofit']) / max(row['A_bsize'] + row['B_bsize'], 1)
    
    # A/Bç±»å‹åå¥½ç¨‹åº¦
    a_dominance = (row['A_fprofit'] + row['A_bprofit']) / max(row['A_fprofit'] + row['A_bprofit'] + row['B_fprofit'] + row['B_bprofit'], 1)
    
    # ç½‘ç»œæ´»è·ƒåº¦ - ä½¿ç”¨ç°æœ‰ç‰¹å¾æ›¿ä»£å·²åˆ é™¤çš„ä¸­å¿ƒæ€§ç‰¹å¾
    network_activity = row['out_degree'] + row['in_degree'] + row['neighbor_count_1hop']
    
    # æ´»è·ƒåº¦
    activity_intensity = row['activity_intensity']
    
    # ä¼˜åŒ–åçš„é˜ˆå€¼ - åŸºäºæ•°æ®åˆ†æç»“æœ
    if network_activity > 0.528 and activity_intensity > 0.00189:  # 75%åˆ†ä½æ•°
        return 'type1'  # æ ¸å¿ƒæ¢çº½èŠ‚ç‚¹
    elif a_dominance > 0.479 and forward_strength > backward_strength:  # 80%åˆ†ä½æ•°
        return 'type2'  # Aç±»ä¸»å¯¼çš„å‘é€æ–¹
    elif a_dominance < 0.476 and backward_strength > forward_strength:  # 20%åˆ†ä½æ•°
        return 'type3'  # Bç±»ä¸»å¯¼çš„æ¥æ”¶æ–¹  
    else:
        return 'type4'  # æ··åˆäº¤æ˜“ç±»å‹

# =====================================================
# æ¨¡å‹è®­ç»ƒå‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼Œè¿”å›é¢„æµ‹ï¼‰
# =====================================================
def train_universal_ensemble(data, n_models=50):
    print(f"\n=== Training Universal Ensemble ({n_models} models) ===")
    data_copy = data.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    feature_cols = [col for col in data_copy.columns 
                   if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    sample_size = min(good_accounts, bad_accounts)
    
    predictions = []
    cv_scores = []
    X_all = data_copy[feature_cols].values
    y_all = data_copy['flag'].values
    
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    for i in tqdm(range(n_models), desc="Universal Models"):
        good_sample = data_copy[data_copy['flag'] == 1].sample(n=sample_size, replace=True, random_state=i)
        bad_sample = data_copy[data_copy['flag'] == 0].sample(n=sample_size, replace=True, random_state=i+1000)
        train_data = pd.concat([good_sample, bad_sample], axis=0)
        
        X_train = train_data[feature_cols].values
        y_train = train_data['flag'].values
        
        clf = RandomForestClassifier(
            n_estimators=100, 
            max_depth=15, 
            min_samples_split=10,
            random_state=i
        )
        clf.fit(X_train, y_train)
        
        cv_score = np.mean([clf.score(X_all[train_idx], y_all[train_idx]) 
                           for train_idx, _ in skf.split(X_all, y_all)])
        cv_scores.append(cv_score)
        
        y_pred = clf.predict_proba(X_all)[:, 1]  # æ¦‚ç‡é¢„æµ‹æ›´é€‚åˆåš meta-feature
        predictions.append(y_pred)
    
    return np.array(predictions), cv_scores

def train_strategy_ensemble(data, strategy_name, strategy_categories, n_models=10):
    print(f"\n=== Training {strategy_name.upper()} Strategy Ensemble ({n_models} models) ===")
    data_with_strategy = data.merge(strategy_categories, on='account', how='left')
    strategy_col = f"{strategy_name}_category"
    data_with_strategy[strategy_col] = data_with_strategy[strategy_col].fillna('unknown')
    
    data_copy = data_with_strategy.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    
    feature_cols = [col for col in data_copy.columns if col not in ['account', 'flag', 'account_type']]
    strategy_dummies = pd.get_dummies(data_copy[strategy_col], prefix=strategy_name)
    feature_data = pd.concat([
        data_copy[[col for col in feature_cols if not col.endswith('_category')]],
        strategy_dummies
    ], axis=1)
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    sample_size = min(good_accounts, bad_accounts)
    
    print(f"   Balanced sampling: {sample_size} per class")
    print(f"   Features: {feature_data.shape[1]} (base: {len([col for col in feature_cols if not col.endswith('_category')])}, strategy: {len(strategy_dummies.columns)})")
    
    predictions = []
    cv_scores = []
    X_all = feature_data.values
    y_all = data_copy['flag'].values
    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    
    for i in tqdm(range(n_models), desc=f"{strategy_name} Models"):
        good_sample = data_copy[data_copy['flag'] == 1].sample(n=sample_size, replace=True, random_state=i*100)
        bad_sample = data_copy[data_copy['flag'] == 0].sample(n=sample_size, replace=True, random_state=i*100+50)
        sample_indices = list(good_sample.index) + list(bad_sample.index)
        
        X_train = feature_data.loc[sample_indices].values
        y_train = pd.concat([good_sample, bad_sample])['flag'].values
        
        clf = RandomForestClassifier(
            n_estimators=120,
            max_depth=18,
            min_samples_split=8,
            min_samples_leaf=4,
            random_state=i*10,
            class_weight='balanced'
        )
        clf.fit(X_train, y_train)
        
        # è®¡ç®—åˆ†ç±»åˆ«F1åˆ†æ•°
        cv_f1_overall = []
        # cv_f1_good = []  # Not used
        # cv_f1_bad = []   # Not used
        
        for _, val_idx in skf.split(X_all, y_all):
            val_pred = clf.predict(X_all[val_idx])
            f1_overall = metrics.f1_score(y_all[val_idx], val_pred, zero_division=0)
            # f1_good = metrics.f1_score(y_all[val_idx], val_pred, pos_label=1, zero_division=0)  # Not used
            # f1_bad = metrics.f1_score(y_all[val_idx], val_pred, pos_label=0, zero_division=0)   # Not used
            
            cv_f1_overall.append(f1_overall)
            # cv_f1_good.append(f1_good)  # Not used
            # cv_f1_bad.append(f1_bad)    # Not used
        
        cv_score = np.mean(cv_f1_overall)
        cv_scores.append(cv_score)
        
        y_pred = clf.predict_proba(X_all)[:, 1]
        predictions.append(y_pred)
    
    predictions_array = np.array(predictions).T
    
    # è®¡ç®—å¹³å‡åˆ†ç±»åˆ«F1
    avg_f1_overall = np.mean(cv_scores)
    print(f"   Average CV F1 (Overall): {avg_f1_overall:.4f}")
    
    return predictions_array, cv_scores

# =====================================================
# PyTorch Meta-ANN with ResNet Connections - ä¿®æ”¹ç‚¹1
# =====================================================
class MetaANN(nn.Module):
    def __init__(self, n_base, n_feat, dropout=0.3):
        super().__init__()
        self.a = nn.Parameter(torch.ones(n_feat))
        self.b = nn.Parameter(torch.zeros(n_feat))
        
        self.input_dim = n_base + n_feat
        
        # ç®€åŒ–ç½‘ç»œç»“æ„ï¼š64 -> 32 -> 16 -> 2
        self.layer1 = nn.Sequential(
            nn.Linear(self.input_dim, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.layer2 = nn.Sequential(
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.layer3 = nn.Sequential(
            nn.Linear(32, 16),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.out = nn.Linear(16, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x_base, x_feat):
        # ç‰¹å¾ç¼©æ”¾
        x_feat_scaled = self.a * x_feat + self.b
        
        # ç‰¹å¾èåˆ
        x = torch.cat([x_base, x_feat_scaled], dim=1)
        
        # å‰å‘ä¼ æ’­ï¼š64 -> 32 -> 16 -> 1
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        
        return self.sigmoid(self.out(x))

def train_pytorch_meta_ann(base_predictions, original_features, y_true, n_epochs=500, patience=30):
    """
    ä½¿ç”¨PyTorchè®­ç»ƒMeta-ANN
    base_predictions: (n_samples, n_models) - åŸºç¡€æ¨¡å‹é¢„æµ‹
    original_features: (n_samples, n_features) - åŸå§‹ç‰¹å¾
    y_true: (n_samples,) - çœŸå®æ ‡ç­¾
    """
    print(f"\nğŸ¤– Training PyTorch Meta-ANN")
    print(f"Base predictions shape: {base_predictions.shape}")
    print(f"Original features shape: {original_features.shape}")
    
    # æ ‡å‡†åŒ–åŸå§‹ç‰¹å¾
    scaler = StandardScaler()
    original_features = scaler.fit_transform(original_features)
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_base_tensor = torch.tensor(base_predictions, dtype=torch.float32).to(device)
    X_feat_tensor = torch.tensor(original_features, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y_true.reshape(-1,1), dtype=torch.float32).to(device)
    
    # åˆ›å»ºæ¨¡å‹
    model = MetaANN(
        n_base=base_predictions.shape[1], 
        n_feat=original_features.shape[1],
        dropout=0.3
    ).to(device)
    
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    criterion = nn.BCELoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=10)

    # äº¤å‰éªŒè¯åˆ†å‰²ç”¨äºæ—©åœ - ä½¿ç”¨å’ŒCVä¸­è¡¨ç°æœ€å¥½çš„fold
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥ç”¨å’ŒPHASE 4ä¸­æœ€ä½³foldç›¸åŒçš„åˆ†å‰²
    all_splits = list(skf.split(base_predictions, y_true))
    train_idx, val_idx = all_splits[2]  # fold 3 (0-indexed), å¯¹åº”PHASE 4çš„æœ€ä½³fold
    
    Xb_train, Xb_val = X_base_tensor[train_idx], X_base_tensor[val_idx]
    Xf_train, Xf_val = X_feat_tensor[train_idx], X_feat_tensor[val_idx]
    y_train, y_val = y_tensor[train_idx], y_tensor[val_idx]
    
    best_val_f1 = 0
    patience_counter = 0
    train_f1_history = []
    val_f1_history = []
    
    print("\nEpoch | Train F1 | Val F1   | Good F1  | Bad F1   | LR       | Status")
    print("-" * 70)
    
    for epoch in range(n_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        optimizer.zero_grad()
        y_pred_train = model(Xb_train, Xf_train)
        loss = criterion(y_pred_train, y_train)
        loss.backward()
        optimizer.step()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        with torch.no_grad():
            y_pred_val = model(Xb_val, Xf_val)
            
            # è®¡ç®—F1åˆ†æ•°
            y_train_prob = model(Xb_train, Xf_train).cpu().numpy()
            y_val_prob = y_pred_val.cpu().numpy()
            
            train_pred = (y_train_prob > 0.5).astype(int).flatten()
            val_pred = (y_val_prob > 0.5).astype(int).flatten()
            
            # æ•´ä½“F1å’Œåˆ†ç±»åˆ«F1
            train_f1 = metrics.f1_score(y_true[train_idx], train_pred, zero_division=0)
            val_f1 = metrics.f1_score(y_true[val_idx], val_pred, zero_division=0)
            
            # åˆ†åˆ«è®¡ç®—Goodç±»(1)å’ŒBadç±»(0)çš„F1
            val_f1_good = metrics.f1_score(y_true[val_idx], val_pred, pos_label=1, zero_division=0)
            val_f1_bad = metrics.f1_score(y_true[val_idx], val_pred, pos_label=0, zero_division=0)
            
            train_f1_history.append(train_f1)
            val_f1_history.append(val_f1)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_f1)
        current_lr = optimizer.param_groups[0]['lr']
        
        # æ—©åœæ£€æŸ¥
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            best_model_state = model.state_dict().copy()
            status = "âœ… Best"
        else:
            patience_counter += 1
            status = f"â³ {patience_counter}/{patience}"
        
        # æ‰“å°è¿›åº¦
        if epoch % 50 == 0 or patience_counter == 0:
            print(f"{epoch:5d} | {train_f1:8.4f} | {val_f1:8.4f} | {val_f1_good:8.4f} | {val_f1_bad:8.4f} | {current_lr:.2e} | {status}")
        
        if patience_counter >= patience:
            print(f"\nğŸ›‘ Early stopping at epoch {epoch}")
            print(f"ğŸ† Best validation F1: {best_val_f1:.4f}")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_model_state)
    model.eval()
    
    # åœ¨å…¨éƒ¨è®­ç»ƒæ•°æ®ä¸Šé¢„æµ‹
    with torch.no_grad():
        y_final_pred = model(X_base_tensor, X_feat_tensor).cpu().numpy()
        y_final_label = (y_final_pred > 0.5).astype(int).flatten()
        
        final_f1 = metrics.f1_score(y_true, y_final_label, zero_division=0)
        final_f1_good = metrics.f1_score(y_true, y_final_label, pos_label=1, zero_division=0)
        final_f1_bad = metrics.f1_score(y_true, y_final_label, pos_label=0, zero_division=0)
        final_acc = metrics.accuracy_score(y_true, y_final_label)
        
    print(f"\nğŸ“Š Meta-ANN Final Results:")
    print(f"   Accuracy: {final_acc:.4f}")
    print(f"   Overall F1: {final_f1:.4f}")
    print(f"   Good Class F1 (pos_label=1): {final_f1_good:.4f}")
    print(f"   Bad Class F1 (pos_label=0): {final_f1_bad:.4f}")
    print(f"   Best Val F1: {best_val_f1:.4f}")
    print(f"   Overfitting: {train_f1_history[-1] - best_val_f1:+.4f}")
    
    return y_final_pred, model, scaler, {
        'final_f1': final_f1,
        'final_f1_good': final_f1_good,
        'final_f1_bad': final_f1_bad,
        'final_acc': final_acc,
        'best_val_f1': best_val_f1,
        'train_f1_history': train_f1_history,
        'val_f1_history': val_f1_history
    }

# =====================================================
# Threshold Optimization Function  
# =====================================================
def find_optimal_threshold(y_true, y_pred_proba):
    """Find optimal threshold using Youden's J statistic"""
    from sklearn.metrics import roc_curve
    
    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    
    # Youden's J statistic = TPR - FPR
    j_scores = tpr - fpr
    
    # Find threshold that maximizes J
    best_idx = np.argmax(j_scores)
    best_threshold = thresholds[best_idx]
    best_j_score = j_scores[best_idx]
    
    return {
        'threshold': best_threshold,
        'j_score': best_j_score,
        'sensitivity': tpr[best_idx],
        'specificity': 1 - fpr[best_idx]
    }

# =====================================================
# SMOTE Data Augmentation Functions
# =====================================================
def apply_smote_augmentation(X, y, random_state=42):
    """Apply SMOTE to balance the dataset"""
    if not IMBLEARN_AVAILABLE:
        print("   âš ï¸ SMOTE skipped - imbalanced-learn not installed")
        return X, y
    
    original_counts = np.bincount(y)
    print(f"   Original class distribution: {dict(enumerate(original_counts))}")
    
    # Apply SMOTE
    smote = SMOTE(
        sampling_strategy='auto',  # Balance to majority class
        random_state=random_state,
        k_neighbors=min(5, min(original_counts) - 1) if min(original_counts) > 1 else 1
    )
    
    try:
        X_resampled, y_resampled = smote.fit_resample(X, y)
        new_counts = np.bincount(y_resampled)
        print(f"   SMOTE class distribution: {dict(enumerate(new_counts))}")
        print(f"   SMOTE increase: {len(X_resampled) - len(X):.0f} samples ({(len(X_resampled)/len(X) - 1)*100:.1f}%)")
        return X_resampled, y_resampled
    except Exception as e:
        print(f"   âš ï¸ SMOTE failed: {e}")
        return X, y

# =====================================================
# Enhanced Random Forest Ensemble Training
# =====================================================
def train_enhanced_rf_ensemble(data, n_models=100):
    """è®­ç»ƒå¢å¼ºçš„éšæœºæ£®æ—é›†æˆ - ä¼˜åŒ–ç‰ˆæœ¬"""
    print(f"\nğŸŒ³ Training Enhanced RF Ensemble ({n_models} models)")
    
    data_copy = data.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    feature_cols = [col for col in data_copy.columns 
                   if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    sample_size = min(good_accounts, bad_accounts)
    
    print(f"   ğŸ’¡ Data Info:")
    print(f"      Good accounts: {good_accounts}")
    print(f"      Bad accounts: {bad_accounts}")
    print(f"      Balanced sampling: {sample_size} per class")
    print(f"      Features: {len(feature_cols)}")
    print(f"      Imbalance ratio: 1:{bad_accounts//good_accounts}")
    
    X_all = data_copy[feature_cols].values
    y_all = data_copy['flag'].values
    
    predictions = []
    cv_scores = []
    
    # ä¼˜åŒ–çš„éšæœºæ£®æ—é…ç½® - æ›´æ·±æ›´å¤æ‚
    rf_configs = [
        {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 8, 'min_samples_leaf': 3},
        {'n_estimators': 180, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 2},
        {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4},
        {'n_estimators': 220, 'max_depth': 35, 'min_samples_split': 12, 'min_samples_leaf': 5},
    ]
    
    for i in tqdm(range(n_models), desc="RF Models"):
        # æ›´æ¿€è¿›çš„é‡‡æ ·ç­–ç•¥ - å¢åŠ æ ·æœ¬å¤šæ ·æ€§
        bootstrap_ratio = 0.8 + 0.4 * np.random.random()  # 0.8-1.2å€é‡‡æ ·
        actual_sample_size = int(sample_size * bootstrap_ratio)
        
        good_sample = data_copy[data_copy['flag'] == 1].sample(
            n=actual_sample_size, replace=True, random_state=i
        )
        bad_sample = data_copy[data_copy['flag'] == 0].sample(
            n=actual_sample_size, replace=True, random_state=i+3000
        )
        train_data = pd.concat([good_sample, bad_sample], axis=0)
        
        X_train = train_data[feature_cols].values
        y_train = train_data['flag'].values
        
        # å¾ªç¯ä½¿ç”¨ä¸åŒé…ç½®
        config = rf_configs[i % len(rf_configs)]
        
        clf = RandomForestClassifier(
            **config,
            random_state=i,
            class_weight='balanced_subsample',  # æ›´å¥½çš„ç±»å¹³è¡¡
            max_features='sqrt',  # ç‰¹å¾é€‰æ‹©ç­–ç•¥
            bootstrap=True,
            oob_score=True,
            n_jobs=1
        )
        clf.fit(X_train, y_train)
        
        # ä½¿ç”¨Out-of-Bagè¯„ä¼° + äº¤å‰éªŒè¯
        # oob_score = clf.oob_score_ if hasattr(clf, 'oob_score_') else 0  # Not used
        
        # 5æŠ˜äº¤å‰éªŒè¯
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)
        cv_f1_scores = []
        for _, val_idx in skf.split(X_all, y_all):
            val_pred = clf.predict(X_all[val_idx])
            f1 = metrics.f1_score(y_all[val_idx], val_pred, zero_division=0)
            cv_f1_scores.append(f1)
        
        cv_score = np.mean(cv_f1_scores)
        cv_scores.append(cv_score)
        
        # æ¦‚ç‡é¢„æµ‹
        y_pred_proba = clf.predict_proba(X_all)[:, 1]
        predictions.append(y_pred_proba)
    
    predictions_array = np.array(predictions).T  # (n_samples, n_models)
    avg_cv_score = np.mean(cv_scores)
    
    print(f"   ğŸ“Š Results:")
    print(f"      Average CV F1: {avg_cv_score:.4f}")
    print(f"      CV F1 std: {np.std(cv_scores):.4f}")
    print(f"      CV F1 range: [{np.min(cv_scores):.4f}, {np.max(cv_scores):.4f}]")
    
    return predictions_array, cv_scores, feature_cols

# =====================================================
# Strategy-Specific RF Ensemble Training (200 models distributed)
# =====================================================
def train_strategy_specific_rf_ensemble(data, strategy_data, test_data=None):
    """è®­ç»ƒæŒ‰åˆ†ç±»ç­–ç•¥åˆ†é…çš„ä¸“ç”¨RFé›†æˆ - 200ä¸ªæ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒï¼ŒåŒæ—¶ç”Ÿæˆæµ‹è¯•é¢„æµ‹"""
    print(f"\nğŸ¯ Training Strategy-Specific RF Ensemble (200 models distributed)")
    if test_data is not None:
        print(f"   ğŸ“Š Also generating test predictions during training")
    
    # RFåˆ†é…ç­–ç•¥ - æŒ‰æ¯”ä¾‹ç¼©å‡åˆ°50ä¸ªæ¨¡å‹
    rf_allocation = {
        'account_type': {'type4': 6, 'type3': 4, 'type1': 3, 'type2': 2},  # 15ä¸ª
        'traditional': {'isolated': 4, 'backward_only': 3, 'both_directions': 2, 'forward_only': 2},  # 11ä¸ª
        'interaction': {'B_in_B_out': 3, 'A_in_B_in_B_out': 2, 'A_in_A_out_B_in_B_out': 2, 
                       'A_out_B_in_B_out': 1, 'B_in': 1, 'small_categories': 1},  # 10ä¸ª
        'behavior': {'inactive': 3, 'low_activity': 3, 'medium_activity_unidirectional': 1, 
                    'medium_activity_bidirectional': 1},  # 8ä¸ª
        'volume': {'no_transactions': 2, 'medium_volume': 2, 'low_volume': 1, 'high_volume': 1},  # 6ä¸ª
        'profit': {'loss_or_zero': 1, 'very_high_profit': 1}  # 2ä¸ª
    }
    
    data_copy = data.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    feature_cols = [col for col in data_copy.columns 
                   if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]
    
    X_all = data_copy[feature_cols].values
    y_all = data_copy['flag'].values
    
    all_predictions = []
    all_test_predictions = []  # Store test predictions
    all_cv_scores = []
    model_count = 0
    
    # Prepare test data if provided
    if test_data is not None:
        test_data_copy = test_data.copy()
        X_test = test_data_copy[feature_cols].values
    
    print(f"   ğŸ“Š Distribution Overview:")
    total_models = sum(sum(allocation.values()) for allocation in rf_allocation.values())
    print(f"      Total models planned: {total_models}")
    
    # éå†æ¯ä¸ªåˆ†ç±»ç­–ç•¥
    for strategy_name, allocation in rf_allocation.items():
        print(f"\n   ğŸ”¸ Training {strategy_name.upper()} strategy models...")
        
        if strategy_name == 'account_type':
            # ä½¿ç”¨å†…ç½®è´¦æˆ·åˆ†ç±»
            data_copy['current_strategy'] = data_copy.apply(classify_account_type_improved, axis=1)
        else:
            # ä½¿ç”¨å¤–éƒ¨ç­–ç•¥æ–‡ä»¶
            if strategy_name in strategy_data and not strategy_data[strategy_name].empty:
                strategy_df = strategy_data[strategy_name]
                strategy_mapping = dict(zip(strategy_df.iloc[:, 0], strategy_df.iloc[:, 1]))
                data_copy['current_strategy'] = data_copy['account'].map(strategy_mapping)
            else:
                print(f"      âš ï¸  {strategy_name} strategy data not available, skipping...")
                continue
        
        # ä¸ºæ¯ä¸ªç±»åˆ«è®­ç»ƒåˆ†é…çš„RFæ¨¡å‹
        for category, n_models in allocation.items():
            if category == 'small_categories':
                # å¤„ç†interactionç­–ç•¥çš„å°ç±»åˆ«
                if strategy_name == 'interaction':
                    small_cats = ['A_in_B_in', 'B_out', 'A_out_B_out', 'A_in_B_out', 
                                 'A_in_A_out_B_out', 'A_in_A_out_B_in', 'A_out_B_in', 'A_out']
                    category_data = data_copy[data_copy['current_strategy'].isin(small_cats)]
                else:
                    continue
            else:
                category_data = data_copy[data_copy['current_strategy'] == category]
            
            if len(category_data) < 10:  # å¦‚æœç±»åˆ«æ ·æœ¬å¤ªå°‘ï¼Œè·³è¿‡
                print(f"      âš ï¸  {category}: Only {len(category_data)} samples, skipping...")
                continue
            
            print(f"      ğŸ¯ {category}: {len(category_data)} samples, {n_models} models")
            
            # ä¸ºå½“å‰ç±»åˆ«è®­ç»ƒn_modelsä¸ªRF
            if test_data is not None:
                category_predictions, category_test_predictions, category_cv_scores = train_category_specific_models(
                    category_data, data_copy, feature_cols, n_models, model_count, X_test
                )
                all_test_predictions.extend(category_test_predictions)
            else:
                category_predictions, category_cv_scores = train_category_specific_models(
                    category_data, data_copy, feature_cols, n_models, model_count
                )
            
            all_predictions.extend(category_predictions)
            all_cv_scores.extend(category_cv_scores)
            model_count += n_models
    
    # è½¬æ¢ä¸ºæ•°ç»„æ ¼å¼
    predictions_array = np.array(all_predictions).T  # (n_samples, n_models)
    
    print(f"\n   ğŸ“Š Strategy-Specific Training Results:")
    print(f"      Total models trained: {len(all_predictions)}")
    print(f"      Average CV F1: {np.mean(all_cv_scores):.4f}")
    print(f"      CV F1 std: {np.std(all_cv_scores):.4f}")
    print(f"      CV F1 range: [{np.min(all_cv_scores):.4f}, {np.max(all_cv_scores):.4f}]")
    
    # å¤„ç†æµ‹è¯•é¢„æµ‹
    test_predictions_array = None
    if test_data is not None and len(all_test_predictions) > 0:
        test_predictions_array = np.array(all_test_predictions).T  # (n_test_samples, n_models)
        print(f"      Test predictions generated: {test_predictions_array.shape}")
    
    # æ·»åŠ è¯¦ç»†çš„F1è¯„ä¼°
    if len(all_predictions) > 0:
        print(f"\n   ğŸ” Detailed F1 Evaluation on Training Data:")
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œç»¼åˆè¯„ä¼°
        y_true = data_copy['flag'].values
        
        # é›†æˆé¢„æµ‹ (ç®€å•å¹³å‡)
        ensemble_pred_proba = np.mean(predictions_array, axis=1)
        ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)
        
        # è®¡ç®—å„ç§F1åˆ†æ•°
        from sklearn.metrics import f1_score
        
        f1_bad = f1_score(y_true, ensemble_pred, pos_label=1)   # badç±»F1 (æ ‡ç­¾1ä¸ºbad)
        f1_good = f1_score(y_true, ensemble_pred, pos_label=0)  # goodç±»F1 (æ ‡ç­¾0ä¸ºgood)
        f1_macro = f1_score(y_true, ensemble_pred, average='macro')
        f1_weighted = f1_score(y_true, ensemble_pred, average='weighted')
        
        print(f"      Bad F1 (pos_label=1): {f1_bad:.4f}")
        print(f"      Good F1 (pos_label=0): {f1_good:.4f}")
        print(f"      Macro F1: {f1_macro:.4f}")
        print(f"      Weighted F1: {f1_weighted:.4f}")
        
        # è¿‡æ»¤æ‰æ•ˆæœç‰¹åˆ«å·®çš„æ¨¡å‹ï¼ˆF1 < 0.5ï¼‰
        good_cv_scores = [score for score in all_cv_scores if score >= 0.5]
        if len(good_cv_scores) < len(all_cv_scores):
            print(f"      âš ï¸  {len(all_cv_scores) - len(good_cv_scores)} models with F1 < 0.5 (possibly from small categories)")
            print(f"      Good models (F1â‰¥0.5): {len(good_cv_scores)}, Avg F1: {np.mean(good_cv_scores):.4f}")
    else:
        print(f"      âš ï¸  No models trained successfully")
    
    if test_data is not None:
        return predictions_array, test_predictions_array, all_cv_scores, feature_cols
    else:
        return predictions_array, all_cv_scores, feature_cols

def train_category_specific_models(category_data, full_data, feature_cols, n_models, base_seed, X_test=None):
    """ä¸ºç‰¹å®šç±»åˆ«è®­ç»ƒä¸“ç”¨RFæ¨¡å‹ï¼ŒåŒæ—¶ç”Ÿæˆæµ‹è¯•é¢„æµ‹"""
    
    # æ£€æŸ¥ç±»åˆ«æ•°æ®æ˜¯å¦æœ‰good/badæ ·æœ¬
    good_accounts = len(category_data[category_data['flag'] == 1])
    bad_accounts = len(category_data[category_data['flag'] == 0])
    
    if good_accounts == 0 or bad_accounts == 0:
        # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œä½¿ç”¨å…¨å±€æ•°æ®è¿›è¡Œå¹³è¡¡é‡‡æ ·
        print(f"        âš ï¸  Category has only one class, using global sampling")
        if X_test is not None:
            return [], [], []
        else:
            return [], []
    
    sample_size = min(good_accounts, bad_accounts, 500)  # é™åˆ¶æœ€å¤§æ ·æœ¬æ•°
    
    # ç”¨ç±»åˆ«æ•°æ®è®­ç»ƒï¼Œä½†å¯¹å…¨æ•°æ®é›†é¢„æµ‹
    X_category = category_data[feature_cols].values
    y_category = category_data['flag'].values
    X_all = full_data[feature_cols].values
    
    predictions = []
    test_predictions = []
    cv_scores = []
    
    # RFé…ç½®ï¼ˆé’ˆå¯¹ç±»åˆ«ä¼˜åŒ–ï¼‰
    rf_configs = [
        {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2},
        {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 3},
        {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 6, 'min_samples_leaf': 1},
    ]
    
    for i in range(n_models):
        current_seed = base_seed + i
        
        # å¹³è¡¡é‡‡æ ·
        good_sample = category_data[category_data['flag'] == 1].sample(
            n=sample_size, replace=True, random_state=current_seed
        )
        bad_sample = category_data[category_data['flag'] == 0].sample(
            n=sample_size, replace=True, random_state=current_seed + 5000
        )
        train_data = pd.concat([good_sample, bad_sample], axis=0)
        
        X_train = train_data[feature_cols].values
        y_train = train_data['flag'].values
        
        # é€‰æ‹©é…ç½®
        config = rf_configs[i % len(rf_configs)]
        
        clf = RandomForestClassifier(
            **config,
            random_state=current_seed,
            class_weight='balanced',
            max_features='sqrt',
            bootstrap=True,
            n_jobs=1
        )
        clf.fit(X_train, y_train)
        
        # äº¤å‰éªŒè¯è¯„ä¼°ï¼ˆåœ¨ç±»åˆ«æ•°æ®ä¸Šï¼‰
        if len(np.unique(y_category)) > 1:  # ç¡®ä¿æœ‰ä¸¤ä¸ªç±»åˆ«
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=current_seed)
            cv_f1_scores = []
            for _, val_idx in skf.split(X_category, y_category):
                if len(val_idx) > 0:
                    val_pred = clf.predict(X_category[val_idx])
                    f1 = metrics.f1_score(y_category[val_idx], val_pred, zero_division=0)
                    cv_f1_scores.append(f1)
            
            cv_score = np.mean(cv_f1_scores) if cv_f1_scores else 0
        else:
            cv_score = 0
        
        cv_scores.append(cv_score)
        
        # æ¦‚ç‡é¢„æµ‹ï¼ˆå¯¹å…¨æ•°æ®é›†ï¼‰
        if hasattr(clf, 'predict_proba'):
            proba_pred = clf.predict_proba(X_all)
            y_pred_proba = proba_pred[:, 1] if proba_pred.shape[1] > 1 else proba_pred[:, 0]
        else:
            y_pred_proba = clf.predict(X_all).astype(float)
        
        predictions.append(y_pred_proba)
        
        # Generate test predictions if X_test is provided
        if X_test is not None:
            if hasattr(clf, 'predict_proba'):
                test_proba_pred = clf.predict_proba(X_test)
                test_pred_proba = test_proba_pred[:, 1] if test_proba_pred.shape[1] > 1 else test_proba_pred[:, 0]
            else:
                test_pred_proba = clf.predict(X_test).astype(float)
            test_predictions.append(test_pred_proba)
    
    if X_test is not None:
        return predictions, test_predictions, cv_scores
    else:
        return predictions, cv_scores

# =====================================================
# Note: generate_test_rf_predictions function removed
# Test predictions are now generated during training phase
# =====================================================

# =====================================================
# Meta-ANN (stacking ç¬¬äºŒå±‚) - ä¿æŒåŸæœ‰çš„sklearnç‰ˆæœ¬ä½œä¸ºå¯¹æ¯”
# =====================================================
def ultra_ensemble_meta_ann(all_predictions, y_true):
    """
    ç”¨ ANN ä½œä¸º meta-classifier
    all_predictions: (n_models, n_samples)
    y_true: (n_samples,)
    """
    X_meta = all_predictions.T  # shape: (n_samples, n_models)
    
    ann = MLPClassifier(
        hidden_layer_sizes=(64, 32),
        activation='relu',
        solver='adam',
        max_iter=200,
        random_state=42
    )
    ann.fit(X_meta, y_true)
    
    y_pred = ann.predict(X_meta)
    f1 = metrics.f1_score(y_true, y_pred, average='binary', zero_division=0)
    print(f"Meta-ANN Training F1: {f1:.4f}")
    
    return y_pred, ann

# =====================================================
# è®­ç»ƒå•ä¸ªfoldçš„Meta-ANN - ä¿®æ”¹ç‚¹2ï¼šæ–°å¢å‡½æ•°
# =====================================================
def train_single_fold_meta_ann(X_base_train, X_feat_train, y_train, X_base_val, X_feat_val, y_val, 
                               fold_id, f1_type='bad', n_epochs=500, patience=30, use_smote=True, use_threshold_opt=True):
    """è®­ç»ƒå•ä¸ªfoldçš„Meta-ANNï¼Œä½¿ç”¨æ—©åœï¼Œæ˜¾ç¤ºè¯¦ç»†è®­ç»ƒè¿‡ç¨‹"""
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    print(f"\nğŸ¤– Training Meta-ANN for Fold {fold_id+1}")
    print(f"Loss Function: BCELoss")
    print(f"Optimizer: AdamW")
    print(f"F1 Selection Criterion: {f1_type}")
    print(f"Device: {device}")
    print(f"Training samples: {X_base_train.shape[0]}, Validation samples: {X_base_val.shape[0]}")
    print(f"SMOTE Enabled: {use_smote and IMBLEARN_AVAILABLE}")
    print(f"Threshold Optimization: {use_threshold_opt}")
    
    print(f"\nEpoch | Train F1 | Val F1   | Good F1  | Bad F1   | Macro F1 | Weighted F1 | LR       | Status")
    print("-" * 95)
    
    # Apply SMOTE augmentation if enabled
    if use_smote and IMBLEARN_AVAILABLE:
        print(f"   ğŸ”„ Applying SMOTE to training data...")
        # Combine base and feature data for SMOTE
        X_combined_train = np.concatenate([X_base_train, X_feat_train], axis=1)
        X_combined_augmented, y_train_augmented = apply_smote_augmentation(
            X_combined_train, y_train, random_state=fold_id*100
        )
        
        # Split back into base and feature components
        X_base_train = X_combined_augmented[:, :X_base_train.shape[1]]
        X_feat_train = X_combined_augmented[:, X_base_train.shape[1]:]
        y_train = y_train_augmented
        
        print(f"   Updated training samples: {X_base_train.shape[0]}")
    else:
        print(f"   SMOTE disabled - using original training data")
    
    # ç‰¹å¾å¤„ç† (features are already scaled, so just copy)
    X_feat_train_scaled = X_feat_train.copy()
    X_feat_val_scaled = X_feat_val.copy()
    # Create dummy scaler for compatibility
    scaler_fold = StandardScaler()
    scaler_fold.mean_ = np.zeros(X_feat_train.shape[1])
    scaler_fold.scale_ = np.ones(X_feat_train.shape[1])
    
    # åˆ›å»ºæ¨¡å‹
    model_fold = MetaANN(
        n_base=X_base_train.shape[1], 
        n_feat=X_feat_train_scaled.shape[1],
        dropout=0.3
    ).to(device)
    
    optimizer = optim.AdamW(model_fold.parameters(), lr=1e-3, weight_decay=1e-4)
    criterion = nn.BCELoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=10)
    
    # è½¬æ¢ä¸ºå¼ é‡
    X_base_train_t = torch.tensor(X_base_train, dtype=torch.float32).to(device)
    X_feat_train_t = torch.tensor(X_feat_train_scaled, dtype=torch.float32).to(device)
    y_train_t = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32).to(device)
    
    X_base_val_t = torch.tensor(X_base_val, dtype=torch.float32).to(device)
    X_feat_val_t = torch.tensor(X_feat_val_scaled, dtype=torch.float32).to(device)
    
    best_val_f1 = 0
    patience_counter = 0
    best_model_state = None
    
    # å®Œæ•´è®­ç»ƒå¸¦æ—©åœ
    for epoch in range(n_epochs):
        model_fold.train()
        optimizer.zero_grad()
        y_pred = model_fold(X_base_train_t, X_feat_train_t)
        loss = criterion(y_pred, y_train_t)
        loss.backward()
        optimizer.step()
        
        # éªŒè¯
        model_fold.eval()
        with torch.no_grad():
            val_pred_prob = model_fold(X_base_val_t, X_feat_val_t).cpu().numpy()
            val_pred_label = (val_pred_prob > 0.5).astype(int).flatten()
            
            # Calculate F1 based on f1_type  
            if f1_type == 'bad':
                val_f1 = metrics.f1_score(y_val, val_pred_label, pos_label=1, zero_division=0)  # bad=1
            elif f1_type == 'macro':
                val_f1 = metrics.f1_score(y_val, val_pred_label, average='macro', zero_division=0)
            elif f1_type == 'weighted':
                val_f1 = metrics.f1_score(y_val, val_pred_label, average='weighted', zero_division=0)
            else:  # default to 'bad'
                val_f1 = metrics.f1_score(y_val, val_pred_label, pos_label=1, zero_division=0)  # bad=1
        
        # è®¡ç®—æ‰€æœ‰F1åˆ†æ•°ç”¨äºæ˜¾ç¤º
        val_label = (val_pred_prob > 0.5).astype(int).flatten()
        with torch.no_grad():
            train_pred_prob = model_fold(X_base_train_t, X_feat_train_t).cpu().numpy()
        train_label = (train_pred_prob > 0.5).astype(int).flatten()
        
        # è®¡ç®—å„ç§F1
        if f1_type == 'bad':
            train_f1_display = metrics.f1_score(y_train, train_label, pos_label=1, zero_division=0)
        elif f1_type == 'macro':
            train_f1_display = metrics.f1_score(y_train, train_label, average='macro', zero_division=0)
        elif f1_type == 'weighted':
            train_f1_display = metrics.f1_score(y_train, train_label, average='weighted', zero_division=0)
        else:
            train_f1_display = metrics.f1_score(y_train, train_label, pos_label=1, zero_division=0)
            
        val_f1_good = metrics.f1_score(y_val, val_label, pos_label=0, zero_division=0)
        val_f1_bad = metrics.f1_score(y_val, val_label, pos_label=1, zero_division=0)  
        val_f1_macro = metrics.f1_score(y_val, val_label, average='macro', zero_division=0)
        val_f1_weighted = metrics.f1_score(y_val, val_label, average='weighted', zero_division=0)

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_f1)
        current_lr = optimizer.param_groups[0]['lr']
        
        # æ—©åœæ£€æŸ¥
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            best_model_state = model_fold.state_dict().copy()
            status = "âœ… Best"
        else:
            patience_counter += 1
            status = f"â³ {patience_counter}/{patience}"
        
        # æ‰“å°è¿›åº¦ - æ¯5ä¸ªepochæˆ–æœ€ä½³epoch
        if epoch % 5 == 0 or patience_counter == 0 or epoch < 10:
            print(f"{epoch:5d} | {train_f1_display:8.4f} | {val_f1:8.4f} | {val_f1_good:8.4f} | {val_f1_bad:8.4f} | {val_f1_macro:8.4f} | {val_f1_weighted:8.4f} | {current_lr:.2e} | {status}")
        
        if patience_counter >= patience:
            print(f"\nğŸ›‘ Early stopping at epoch {epoch}")
            print(f"ğŸ† Best validation F1: {best_val_f1:.4f}")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        model_fold.load_state_dict(best_model_state)
    
    # æœ€ç»ˆè¯„ä¼° with threshold optimization
    model_fold.eval()
    with torch.no_grad():
        train_pred = model_fold(X_base_train_t, X_feat_train_t).cpu().numpy().flatten()
        val_pred = model_fold(X_base_val_t, X_feat_val_t).cpu().numpy().flatten()
        
        # Threshold optimization if enabled
        if use_threshold_opt:
            print(f"   ğŸ¯ Finding optimal threshold...")
            threshold_result = find_optimal_threshold(y_val, val_pred)
            optimal_threshold = threshold_result['threshold']
            print(f"   Optimal threshold: {optimal_threshold:.4f} (J-score: {threshold_result['j_score']:.4f})")
        else:
            optimal_threshold = 0.5
        
        train_label = (train_pred > optimal_threshold).astype(int)
        val_label = (val_pred > optimal_threshold).astype(int)
        
        # Calculate all f1 scores
        if f1_type == 'bad':
            train_f1 = metrics.f1_score(y_train, train_label, pos_label=1, zero_division=0)  # bad=1
            val_f1 = metrics.f1_score(y_val, val_label, pos_label=1, zero_division=0)       # bad=1
        elif f1_type == 'macro':
            train_f1 = metrics.f1_score(y_train, train_label, average='macro', zero_division=0)
            val_f1 = metrics.f1_score(y_val, val_label, average='macro', zero_division=0)
        elif f1_type == 'weighted':
            train_f1 = metrics.f1_score(y_train, train_label, average='weighted', zero_division=0)
            val_f1 = metrics.f1_score(y_val, val_label, average='weighted', zero_division=0)
        else:
            train_f1 = metrics.f1_score(y_train, train_label, pos_label=1, zero_division=0)  # bad=1
            val_f1 = metrics.f1_score(y_val, val_label, pos_label=1, zero_division=0)       # bad=1
            
        # Always calculate all types for output filename (ç»Ÿä¸€æ ‡ç­¾å®šä¹‰ï¼šgood=0, bad=1)
        val_f1_good = metrics.f1_score(y_val, val_label, pos_label=0, zero_division=0)  # good=0
        val_f1_bad = metrics.f1_score(y_val, val_label, pos_label=1, zero_division=0)   # bad=1
        val_f1_macro = metrics.f1_score(y_val, val_label, average='macro', zero_division=0)
        val_f1_weighted = metrics.f1_score(y_val, val_label, average='weighted', zero_division=0)
        
        train_acc = metrics.accuracy_score(y_train, train_label)
        val_acc = metrics.accuracy_score(y_val, val_label)
    
    result = {
        'model': model_fold,
        'scaler': scaler_fold,
        'train_f1': train_f1,
        'val_f1': val_f1,
        'val_f1_good': val_f1_good,
        'val_f1_bad': val_f1_bad,
        'val_f1_macro': val_f1_macro,
        'val_f1_weighted': val_f1_weighted,
        'train_acc': train_acc,
        'val_acc': val_acc,
        'overfitting': train_f1 - val_f1,
        'fold_id': fold_id
    }
    
    # Add threshold if optimization was used
    if use_threshold_opt:
        result['best_threshold'] = optimal_threshold
    
    return result

# =====================================================
# ä¸»ç¨‹åº - Enhanced PyTorch Version
# =====================================================

def main(f1_type='bad'):
    print(f"=== ULTRA Multi-Strategy Ensemble with PyTorch Meta-ANN (F1 Type: {f1_type}) ===")
    print(f"f1_type :{f1_type} (å¯é€‰ï¼š'bad', 'macro', 'weighted')")
    # æ•°æ®åŠ è½½
    print("\n=== Loading Data ===")
    features_path = '/Users/mannormal/4011/Qi Zihan/v2/feature_extraction/result/features_cleaned_no_leakage1.csv'
    all_features_df = pd.read_csv(features_path)

    pwd = '/Users/mannormal/4011/Qi Zihan/original_data/'
    ta = pd.read_csv(pwd + 'train_acc.csv')
    te = pd.read_csv(pwd + 'test_acc_predict.csv')
    
    # =====================================================
    # 20% Hold-outéªŒè¯é›†åˆ†å‰² - æ–°å¢éƒ¨åˆ†
    # =====================================================
    print("\n=== Creating 20% Hold-out Validation Set ===")
    from sklearn.model_selection import train_test_split
    
    # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾æ•°æ®
    cols_to_drop = []
    if 'flag' in all_features_df.columns:
        cols_to_drop.append('flag')
    if 'data_type' in all_features_df.columns:
        cols_to_drop.append('data_type')
    
    if cols_to_drop:
        print(f"âš ï¸  ç‰¹å¾æ•°æ®ä¸­çš„ä»¥ä¸‹åˆ—å°†è¢«åˆ é™¤: {cols_to_drop}")
        all_features_df = all_features_df.drop(cols_to_drop, axis=1)
    
    ta.loc[ta['flag'] == 0, 'flag'] = -1
    full_training_df = pd.merge(all_features_df, ta[['account', 'flag']], on='account', how='inner')
    full_training_df['account_type'] = full_training_df.apply(classify_account_type_improved, axis=1)
    
    # åˆ†å±‚æŠ½æ ·åˆ›å»ºhold-outéªŒè¯é›†
    y_for_split = np.where(full_training_df['flag'].values == -1, 0, 1)
    train_indices, holdout_indices = train_test_split(
        range(len(full_training_df)), 
        test_size=CONFIG.holdout_ratio, 
        stratify=y_for_split, 
        random_state=42
    )
    
    training_df = full_training_df.iloc[train_indices].copy()
    holdout_df = full_training_df.iloc[holdout_indices].copy()
    
    print(f"Total data: {full_training_df.shape[0]}")
    print(f"Training data: {training_df.shape[0]} ({len(train_indices)/len(full_training_df)*100:.1f}%)")
    print(f"Hold-out validation: {holdout_df.shape[0]} ({len(holdout_indices)/len(full_training_df)*100:.1f}%)")
    print(f"Training flag distribution: {dict(training_df['flag'].value_counts())}")
    print(f"Hold-out flag distribution: {dict(holdout_df['flag'].value_counts())}")
    
    strategy_data = load_strategy_categories()
    
    print(f"Account type distribution: {dict(training_df['account_type'].value_counts())}")
    
    # å‡†å¤‡åŸå§‹ç‰¹å¾
    feature_cols = [col for col in training_df.columns 
                   if col not in ['account', 'flag', 'account_type']]
    original_features = training_df[feature_cols].values
    y_true = np.where(training_df['flag'].values == -1, 0, 1)
    
    print(f"Original features shape: {original_features.shape}")
    print(f"Class distribution: {dict(zip(*np.unique(y_true, return_counts=True)))}")

    # =====================================================
    # Phase 1: å¢å¼ºéšæœºæ£®æ—é›†æˆ
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 1: Enhanced Random Forest Ensemble")
    print(f"{'='*80}")
    
    # ğŸ¯ ä½¿ç”¨ç­–ç•¥ç‰¹å®šè®­ç»ƒ (200ä¸ªåˆ†å¸ƒå¼æ¨¡å‹) - åŒæ—¶å‡†å¤‡æµ‹è¯•æ•°æ®ä»¥ç”Ÿæˆæµ‹è¯•é¢„æµ‹
    print(f"ğŸ¯ Training Mode: Strategy-Specific (200 distributed models)")
    
    # é¢„å…ˆå‡†å¤‡æµ‹è¯•æ•°æ®ä»¥ä¾¿åœ¨è®­ç»ƒæ—¶åŒæ­¥ç”Ÿæˆæµ‹è¯•é¢„æµ‹
    test_df = pd.merge(all_features_df, te[['account']], on='account', how='inner')
    test_df['account_type'] = test_df.apply(classify_account_type_improved, axis=1)
    
    rf_predictions, test_rf_predictions, rf_cv_scores, _ = train_strategy_specific_rf_ensemble(
        training_df, strategy_data, test_data=test_df
    )
    
    # åˆå¹¶æ‰€æœ‰é¢„æµ‹ (ç­–ç•¥ç‰¹å®šè®­ç»ƒå·²åŒ…å«æ‰€æœ‰æ¨¡å‹)
    print(f"\nğŸ“Š Combining Predictions:")
    print(f"   Strategy-specific predictions: {rf_predictions.shape}")
    
    combined_base_predictions = rf_predictions
    print(f"   ğŸ“Š Total models: {combined_base_predictions.shape[1]} (distributed across 6 strategies)")
    
    # =====================================================
    # Phase 2: CVè®­ç»ƒå¹¶é€‰æ‹©æœ€ä½³æ¨¡å‹ + æµ‹è¯•é›†é¢„æµ‹
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 2: Cross-Validation Meta-ANN Training & Test Prediction")
    print(f"{'='*80}")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_feature_cols = [col for col in test_df.columns if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]
    test_original_features = test_df[test_feature_cols].values
    test_combined_predictions = test_rf_predictions
    
    # 5æŠ˜äº¤å‰éªŒè¯è¯„ä¼°
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_results = []
    fold_models = []
    all_test_submissions = []  # å­˜å‚¨æ‰€æœ‰foldçš„æµ‹è¯•é¢„æµ‹
    
    print("\nFold | Train F1 | Val F1   | Good F1  | Bad F1   | Train Acc| Val Acc  | Overfitting")
    print("-" * 80)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(combined_base_predictions, y_true)):
        # åˆ†å‰²æ•°æ®
        X_base_train = combined_base_predictions[train_idx]
        X_base_val = combined_base_predictions[val_idx]
        X_feat_train = original_features[train_idx]
        X_feat_val = original_features[val_idx]
        y_train_fold = y_true[train_idx]
        y_val_fold = y_true[val_idx]
        
        # è®­ç»ƒMeta-ANN with early stopping, SMOTE, and threshold optimization
        fold_result = train_single_fold_meta_ann(
            X_base_train, X_feat_train, y_train_fold, 
            X_base_val, X_feat_val, y_val_fold, 
            fold_id=fold, f1_type=f1_type, n_epochs=500, patience=30,
            use_smote=True, use_threshold_opt=True
        )
        
        fold_models.append(fold_result)
        cv_results.append(fold_result)
        
        overfitting = fold_result['overfitting']
        overfit_status = "ğŸ”´ High" if overfitting > 0.1 else "ğŸŸ¡ Med" if overfitting > 0.05 else "ğŸŸ¢ Low"
        
        print(f"{fold+1:4d} | {fold_result['train_f1']:8.4f} | {fold_result['val_f1']:8.4f} | {fold_result['val_f1_good']:8.4f} | {fold_result['val_f1_bad']:8.4f} | {fold_result['train_acc']:8.4f} | {fold_result['val_acc']:8.4f} | {overfit_status}")
        
        # ç«‹å³è¿›è¡Œæµ‹è¯•é›†é¢„æµ‹
        print(f"   ğŸ”® Generating test predictions for Fold {fold+1}...")
        
        try:
            model = fold_result['model']
            scaler = fold_result['scaler']
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
            test_base_features = test_combined_predictions
            test_original_features_scaled = scaler.transform(test_original_features)
            
            # Meta-ANNé¢„æµ‹
            model.eval()
            with torch.no_grad():
                X_base_tensor = torch.FloatTensor(test_base_features).to(device)
                X_feat_tensor = torch.FloatTensor(test_original_features_scaled).to(device)
                
                test_pred_proba = model(X_base_tensor, X_feat_tensor).cpu().numpy().flatten()
                
                # Use optimized threshold if available
                threshold = fold_result.get('best_threshold', 0.5)
                test_pred_labels = (test_pred_proba > threshold).astype(int)
                
                print(f"      Using threshold: {threshold:.4f}")
            
            # åˆ›å»ºæäº¤æ–‡ä»¶
            submission_df = pd.DataFrame({
                'ID': test_df['account'].values,
                'Predict': test_pred_labels
            })
            
            # ç»Ÿè®¡ç»“æœ
            pred_counts = submission_df['Predict'].value_counts()
            print(f"      Good (1): {pred_counts.get(1, 0)} ({pred_counts.get(1, 0)/len(submission_df)*100:.1f}%)")
            print(f"      Bad (0): {pred_counts.get(0, 0)} ({pred_counts.get(0, 0)/len(submission_df)*100:.1f}%)")
            
            # ç”Ÿæˆæ–‡ä»¶å
            fold_f1 = fold_result['val_f1']
            fold_f1_good = fold_result['val_f1_good'] 
            fold_f1_bad = fold_result['val_f1_bad']
            fold_f1_macro = fold_result['val_f1_macro']
            fold_f1_weighted = fold_result['val_f1_weighted']
            
            filename = f"{version}_fold{fold+1}_{f1_type}_f1_{fold_f1:.4f}_good_{fold_f1_good:.4f}_bad_{fold_f1_bad:.4f}_macro_{fold_f1_macro:.4f}_weighted_{fold_f1_weighted:.4f}_seed_{seed_num}.csv"
            filepath = f"/Users/mannormal/4011/Qi Zihan/v2/results/{filename}"
            
            # ä¿å­˜æ–‡ä»¶
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            submission_df.to_csv(filepath, index=False)
            
            all_test_submissions.append({
                'fold': fold + 1,
                'val_f1': fold_f1,
                'filename': filename,
                'filepath': filepath,
                'submission_df': submission_df,
                'pred_counts': pred_counts
            })
            
            print(f"      âœ… Saved: {filename}")
            
        except Exception as e:
            print(f"      âŒ Error generating prediction for Fold {fold+1}: {str(e)}")
            continue
    
    # é€‰æ‹©æœ€ä½³foldæ¨¡å‹ - ä¿®æ”¹ç‚¹4ï¼šé€‰æ‹©é€»è¾‘
    best_fold_idx = np.argmax([result['val_f1'] for result in cv_results])
    best_fold_model = fold_models[best_fold_idx]
    
    print(f"\nğŸ† Best Fold: {best_fold_idx + 1} (Val F1: {best_fold_model['val_f1']:.4f})")
    
    # CVç»Ÿè®¡
    avg_train_f1 = np.mean([r['train_f1'] for r in cv_results])
    avg_val_f1 = np.mean([r['val_f1'] for r in cv_results])
    avg_val_f1_good = np.mean([r['val_f1_good'] for r in cv_results])
    avg_val_f1_bad = np.mean([r['val_f1_bad'] for r in cv_results])
    # Remove unused variables
    # avg_val_f1_macro = np.mean([r['val_f1_macro'] for r in cv_results])  
    # avg_val_f1_weighted = np.mean([r['val_f1_weighted'] for r in cv_results])
    avg_train_acc = np.mean([r['train_acc'] for r in cv_results])
    avg_val_acc = np.mean([r['val_acc'] for r in cv_results])
    avg_overfitting = avg_train_f1 - avg_val_f1
    
    print("-" * 80)
    print(f"Avg  | {avg_train_f1:8.4f} | {avg_val_f1:8.4f} | {avg_val_f1_good:8.4f} | {avg_val_f1_bad:8.4f} | {avg_train_acc:8.4f} | {avg_val_acc:8.4f} | {avg_overfitting:+7.4f}")
    
    print(f"\nğŸ¤– Meta-ANN Performance (Using Best CV Fold):")
    print(f"   Best Fold Val F1: {best_fold_model['val_f1']:.4f}")
    print(f"   Average CV F1: {avg_val_f1:.4f}")
    print(f"   CV F1 std: {np.std([r['val_f1'] for r in cv_results]):.4f}")
    print(f"   Generalization Gap: {avg_overfitting:+.4f}")
    
    # =====================================================
    # æœ€ç»ˆç»“æœæ±‡æ€»
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ† FINAL RESULTS SUMMARY")
    print(f"{'='*80}")
    
    print(f"\nğŸ“Š Base Models Performance:")
    print(f"   Enhanced RF Ensemble: {np.mean(rf_cv_scores):.4f} F1")
    print(f"   Strategy-specific models integrated into ensemble")
    
    print(f"\nğŸ¤– Meta-ANN Performance:")
    print(f"   Best CV Fold F1: {best_fold_model['val_f1']:.4f}")
    print(f"   Average CV F1: {avg_val_f1:.4f}")
    print(f"   Generalization Gap: {avg_overfitting:+.4f}")
    
    if avg_overfitting > 0.1:
        print("   ğŸ”´ HIGH overfitting - consider regularization")
    elif avg_overfitting > 0.05:
        print("   ğŸŸ¡ MEDIUM overfitting - monitor closely")
    else:
        print("   ğŸŸ¢ LOW overfitting - good generalization")
    
    print(f"\nğŸ¯ Model Architecture:")
    print(f"   Base models: {combined_base_predictions.shape[1]} (Strategy-specific distributed)")
    print(f"   Original features: {original_features.shape[1]}")
    print(f"   Meta-ANN: Simplified with {combined_base_predictions.shape[1]+original_features.shape[1]} â†’ 64 â†’ 32 â†’ 16 â†’ 1")
    
    # =====================================================
    # Hold-outéªŒè¯é›†è¯„ä¼° - æ–°å¢éƒ¨åˆ†
    # =====================================================
    print(f"\nğŸ” Hold-out Validation Evaluation:")
    
    # å‡†å¤‡hold-outæ•°æ®
    holdout_feature_cols = [col for col in holdout_df.columns 
                           if col not in ['account', 'flag', 'account_type']]
    holdout_original_features = holdout_df[holdout_feature_cols].values
    holdout_y_true = np.where(holdout_df['flag'].values == -1, 0, 1)
    
    # ç”Ÿæˆhold-outçš„RFé¢„æµ‹ï¼ˆä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
    print(f"   Generating RF predictions for hold-out set...")
    # è¿™é‡Œéœ€è¦ç”¨è®­ç»ƒå¥½çš„RFæ¨¡å‹å¯¹hold-outæ•°æ®è¿›è¡Œé¢„æµ‹
    # ç”±äºRFæ¨¡å‹å·²ç»è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°ç”Ÿæˆhold-outé¢„æµ‹
    # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä¿å­˜RFæ¨¡å‹è¿›è¡Œé¢„æµ‹
    
    print(f"   Hold-out set: {holdout_df.shape[0]} samples")
    print(f"   Hold-out class distribution: {dict(zip(*np.unique(holdout_y_true, return_counts=True)))}")

    # =====================================================
    # Phase 3: æµ‹è¯•é›†é¢„æµ‹åˆ†ææ¯”è¾ƒ
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 3: Test Set Predictions Analysis & Comparison")
    print(f"{'='*80}")
    
    print(f"ğŸ“Š Test Data Info:")
    print(f"   Test accounts: {test_df.shape[0]}")
    print(f"   Test account type distribution: {dict(test_df['account_type'].value_counts())}")
    
    if len(all_test_submissions) == 0:
        print("âŒ No test predictions generated!")
        return None
    
    # åˆ†æå„foldé¢„æµ‹ç»“æœ
    print(f"\nğŸ” Prediction Analysis:")
    print(f"   Total fold predictions generated: {len(all_test_submissions)}")
    
    # æŒ‰éªŒè¯F1æ’åº
    sorted_submissions = sorted(all_test_submissions, key=lambda x: x['val_f1'], reverse=True)
    
    print(f"\nğŸ“Š Prediction Summary (sorted by Val F1):")
    print("Rank | Fold | Val F1   | Good (1) | Bad (0)  | Filename")
    print("-" * 80)
    
    for rank, sub in enumerate(sorted_submissions, 1):
        pred_counts = sub['pred_counts']
        good_count = pred_counts.get(1, 0)
        bad_count = pred_counts.get(0, 0)
        good_pct = good_count / len(sub['submission_df']) * 100
        bad_pct = bad_count / len(sub['submission_df']) * 100
        
        print(f"{rank:4d} | {sub['fold']:4d} | {sub['val_f1']:8.4f} | {good_count:4d}({good_pct:4.1f}%) | {bad_count:4d}({bad_pct:4.1f}%) | {sub['filename']}")
    
    # è®¾ç½®æœ€ä½³æäº¤
    best_submission = {
        'val_f1': sorted_submissions[0]['val_f1'],
        'filename': sorted_submissions[0]['filename'],
        'fold': sorted_submissions[0]['fold'],
        'submission_df': sorted_submissions[0]['submission_df']
    }
    
    print(f"\nğŸ† Best submission selected:")
    print(f"   Fold {best_submission['fold']}: {best_submission['filename']}")
    print(f"   Validation F1: {best_submission['val_f1']:.4f}")
    
    submission_df = best_submission['submission_df']
    
    # =====================================================
    # è¿”å›ç»“æœæ±‡æ€»
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ TRAINING COMPLETED - RESULTS SUMMARY")
    print(f"{'='*80}")
    
    print(f"ğŸ“Š Final Results:")
    print(f"   Best Validation F1: {best_submission['val_f1']:.4f}")
    print(f"   Total Fold Models: {len(cv_results)}")
    print(f"   Generated Submissions: {len(sorted_submissions)}")
    print(f"   Primary Submission: {best_submission['filename']}")
    print(f"   Reduced Model Complexity: 50 RF models + simplified 64â†’32â†’16â†’1 Meta-ANN")
    print(f"   Hold-out Validation: {CONFIG.holdout_ratio*100:.0f}% of data reserved for unbiased evaluation")
    
    return {
        'cv_results': cv_results,
        'best_submission': best_submission,
        'all_submissions': sorted_submissions,  # ä½¿ç”¨sorted_submissionsæ›¿ä»£all_submissions
        'rf_cv_scores': rf_cv_scores,
        'rf_predictions': rf_predictions,
        'training_df': training_df,
        'holdout_df': holdout_df,
        'config_changes': {
            'model_reduction': '198â†’50 RF models',
            'architecture_simplification': 'ResNetâ†’Linear (64â†’32â†’16â†’1)',
            'holdout_validation': '20% data reserved',
            'expected_benefit': 'Reduced overfitting, better generalization'
        }
    }
    
if __name__ == "__main__":
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='Ultra ResNet Meta-ANN Training')
    parser.add_argument('--f1_type', type=str, default='bad', 
                        choices=['bad', 'macro', 'weighted'],
                        help='F1 score type for model selection (default: bad)')
    
    args = parser.parse_args()

    results = main(f1_type="bad")

    print(f"\n{'='*80}")
    print("âœ… Simplified Meta-ANN Training Complete!")
    print(f"ğŸ¯ RF Models Trained: {len(results['rf_cv_scores'])} (reduced from 198 to ~50)")
    print(f"ğŸ¯ Average RF CV F1: {np.mean(results['rf_cv_scores']):.4f}")
    print(f"ğŸ¯ Best Submission: {results['best_submission']['filename']}")
    print(f"ğŸ“Š Best Val F1: {results['best_submission']['val_f1']:.4f}")
    print(f"ğŸ“Š Generated {len(results['all_submissions'])} fold predictions")
    print(f"ğŸ”§ Model Simplified: 64â†’32â†’16â†’1 architecture")
    print(f"ğŸ“Š Hold-out Validation: 20% data reserved")
    print(f"{'='*80}")
    
    # æ˜¾ç¤ºæ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶
    print(f"\nğŸ‰ SUBMISSIONS READY!")
    for i, sub in enumerate(results['all_submissions'], 1):
        print(f"ğŸ“„ Fold {sub['fold']}: {sub['filename']} (Val F1: {sub['val_f1']:.4f})")
    print(f"ğŸŒ± Seed: {seed_num}")
    print(f"\nğŸ¯ KEY IMPROVEMENTS:")
    print(f"   â€¢ Reduced model count: 198â†’50 (-75% complexity)")
    print(f"   â€¢ Simplified architecture: ResNetâ†’Linear (64â†’32â†’16â†’1)")
    print(f"   â€¢ Added 20% hold-out validation for unbiased evaluation")
    print(f"   â€¢ SMOTE data augmentation for balanced training")
    print(f"   â€¢ Threshold optimization using Youden's J statistic")
    print(f"   â€¢ Combined optimizations for better performance")