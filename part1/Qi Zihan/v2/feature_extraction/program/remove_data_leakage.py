import pandas as pd
import numpy as np
import os
import warnings

warnings.filterwarnings('ignore')

def remove_data_leakage_features():
    """
    åˆ é™¤æ•°æ®æ³„éœ²ç‰¹å¾ã€é‡å¤åˆ—å’Œé›¶å€¼åˆ—
    """
    print("=== åˆ é™¤æ•°æ®æ³„éœ²ç‰¹å¾ ===")
    
    # è¯»å–ç‰¹å¾æ–‡ä»¶
    input_file = "/Users/mannormal/4011/Qi Zihan/v2/feature_extraction/result/features_normalized.csv"
    
    if not os.path.exists(input_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return None
        
    df = pd.read_csv(input_file)
    original_shape = df.shape
    print(f"åŽŸå§‹æ•°æ®: {original_shape[0]} è¡Œ, {original_shape[1]} åˆ—")
    
    # 1. åˆ é™¤æ˜Žç¡®çš„æ•°æ®æ³„éœ²ç‰¹å¾
    print("\n=== 1. åˆ é™¤æ˜Žç¡®çš„æ•°æ®æ³„éœ²ç‰¹å¾ ===")
    immediate_leakage_features = [
        'is_train',
        'is_test', 
        'neighbor_bad_ratio_train_only',
        'train_neighbor_ratio'
    ]
    
    # æ£€æŸ¥å“ªäº›æ³„éœ²ç‰¹å¾å®žé™…å­˜åœ¨
    existing_leakage = [col for col in immediate_leakage_features if col in df.columns]
    print(f"æ£€æµ‹åˆ°çš„æ³„éœ²ç‰¹å¾: {existing_leakage}")
    
    # åˆ é™¤æ³„éœ²ç‰¹å¾
    df_clean = df.drop(columns=existing_leakage, errors='ignore')
    print(f"åˆ é™¤ {len(existing_leakage)} ä¸ªæ³„éœ²ç‰¹å¾åŽ: {df_clean.shape[1]} åˆ—")
    
    # 2. æ£€æŸ¥å¹¶åˆ é™¤é‡å¤åˆ—
    print("\n=== 2. æ£€æŸ¥é‡å¤åˆ— ===")
    
    # èŽ·å–æ•°å€¼åˆ—ï¼ˆæŽ’é™¤accountåˆ—ï¼‰
    numeric_cols = [col for col in df_clean.columns if col != 'account']
    duplicate_pairs = []
    columns_to_drop = set()
    
    print("æ£€æŸ¥åˆ—ä¹‹é—´çš„ç›¸å…³æ€§...")
    for i, col1 in enumerate(numeric_cols):
        for j, col2 in enumerate(numeric_cols[i+1:], i+1):
            try:
                # æ£€æŸ¥æ˜¯å¦å®Œå…¨ç›¸åŒ
                if df_clean[col1].equals(df_clean[col2]):
                    duplicate_pairs.append((col1, col2))
                    columns_to_drop.add(col2)  # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåˆ é™¤ç¬¬äºŒä¸ª
                    print(f"  å‘çŽ°å®Œå…¨é‡å¤åˆ—: {col1} == {col2}")
                # æ£€æŸ¥é«˜åº¦ç›¸å…³ï¼ˆç›¸å…³ç³»æ•° > 0.999ï¼‰
                elif not df_clean[col1].isna().all() and not df_clean[col2].isna().all():
                    corr = df_clean[[col1, col2]].corr().iloc[0, 1]
                    if abs(corr) > 0.999 and not np.isnan(corr):
                        duplicate_pairs.append((col1, col2))
                        columns_to_drop.add(col2)
                        print(f"  å‘çŽ°é«˜åº¦ç›¸å…³åˆ—: {col1} â‰ˆ {col2} (ç›¸å…³ç³»æ•°: {corr:.6f})")
            except:
                continue
    
    if columns_to_drop:
        df_clean = df_clean.drop(columns=list(columns_to_drop))
        print(f"åˆ é™¤ {len(columns_to_drop)} ä¸ªé‡å¤/é«˜åº¦ç›¸å…³åˆ—: {list(columns_to_drop)}")
    else:
        print("æœªå‘çŽ°é‡å¤åˆ—")
    
    print(f"åˆ é™¤é‡å¤åˆ—åŽ: {df_clean.shape[1]} åˆ—")
    
    # 3. æ£€æŸ¥å¹¶åˆ é™¤é›¶å€¼åˆ—
    print("\n=== 3. æ£€æŸ¥é›¶å€¼åˆ— ===")
    
    zero_value_cols = []
    near_zero_cols = []
    
    for col in numeric_cols:
        if col in df_clean.columns:
            col_data = df_clean[col].dropna()
            if len(col_data) == 0:
                zero_value_cols.append(col)
                print(f"  å…¨éƒ¨ä¸ºç©ºå€¼: {col}")
            elif (col_data == 0).all():
                zero_value_cols.append(col)
                print(f"  å…¨éƒ¨ä¸ºé›¶å€¼: {col}")
            elif (col_data == 0).sum() / len(col_data) > 0.95:
                near_zero_cols.append(col)
                zero_ratio = (col_data == 0).sum() / len(col_data)
                print(f"  è¿‘é›¶åˆ— ({zero_ratio:.1%}ä¸ºé›¶): {col}")
    
    # åˆ é™¤é›¶å€¼åˆ—
    if zero_value_cols:
        df_clean = df_clean.drop(columns=zero_value_cols)
        print(f"åˆ é™¤ {len(zero_value_cols)} ä¸ªé›¶å€¼åˆ—: {zero_value_cols}")
    else:
        print("æœªå‘çŽ°é›¶å€¼åˆ—")
    
    # å¯¹äºŽè¿‘é›¶åˆ—ï¼Œè¯¢é—®æ˜¯å¦åˆ é™¤
    if near_zero_cols:
        print(f"å‘çŽ° {len(near_zero_cols)} ä¸ªè¿‘é›¶åˆ—ï¼ˆ>95%ä¸ºé›¶ï¼‰ï¼Œå»ºè®®åˆ é™¤ä»¥å‡å°‘å™ªå£°")
        # è‡ªåŠ¨åˆ é™¤è¿‘é›¶åˆ—
        df_clean = df_clean.drop(columns=near_zero_cols)
        print(f"åˆ é™¤ {len(near_zero_cols)} ä¸ªè¿‘é›¶åˆ—: {near_zero_cols}")
    
    print(f"åˆ é™¤é›¶å€¼åˆ—åŽ: {df_clean.shape[1]} åˆ—")
    
    # 4. æ£€æŸ¥æ–¹å·®è¿‡ä½Žçš„ç‰¹å¾
    print("\n=== 4. æ£€æŸ¥ä½Žæ–¹å·®ç‰¹å¾ ===")
    
    low_variance_cols = []
    variance_threshold = 1e-6
    
    for col in numeric_cols:
        if col in df_clean.columns:
            col_data = df_clean[col].dropna()
            if len(col_data) > 1:
                variance = col_data.var()
                if variance < variance_threshold:
                    low_variance_cols.append(col)
                    print(f"  ä½Žæ–¹å·®ç‰¹å¾: {col} (æ–¹å·®: {variance:.2e})")
    
    if low_variance_cols:
        df_clean = df_clean.drop(columns=low_variance_cols)
        print(f"åˆ é™¤ {len(low_variance_cols)} ä¸ªä½Žæ–¹å·®ç‰¹å¾")
    else:
        print("æœªå‘çŽ°ä½Žæ–¹å·®ç‰¹å¾")
    
    print(f"åˆ é™¤ä½Žæ–¹å·®ç‰¹å¾åŽ: {df_clean.shape[1]} åˆ—")
    
    # 5. æœ€ç»ˆæ£€æŸ¥å’Œç»Ÿè®¡
    print("\n=== 5. æœ€ç»ˆç»Ÿè®¡ ===")
    
    final_shape = df_clean.shape
    removed_cols = original_shape[1] - final_shape[1]
    
    print(f"åŽŸå§‹ç‰¹å¾æ•°: {original_shape[1]}")
    print(f"æœ€ç»ˆç‰¹å¾æ•°: {final_shape[1]}")
    print(f"åˆ é™¤ç‰¹å¾æ•°: {removed_cols}")
    print(f"åˆ é™¤æ¯”ä¾‹: {removed_cols/original_shape[1]*100:.1f}%")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡åˆ é™¤çš„ç‰¹å¾
    total_removed = len(existing_leakage) + len(columns_to_drop) + len(zero_value_cols) + len(near_zero_cols) + len(low_variance_cols)
    print(f"\nåˆ é™¤ç‰¹å¾è¯¦æƒ…:")
    print(f"  æ•°æ®æ³„éœ²ç‰¹å¾: {len(existing_leakage)} ä¸ª")
    print(f"  é‡å¤/é«˜ç›¸å…³ç‰¹å¾: {len(columns_to_drop)} ä¸ª")
    print(f"  é›¶å€¼ç‰¹å¾: {len(zero_value_cols)} ä¸ª")
    print(f"  è¿‘é›¶ç‰¹å¾: {len(near_zero_cols)} ä¸ª")
    print(f"  ä½Žæ–¹å·®ç‰¹å¾: {len(low_variance_cols)} ä¸ª")
    print(f"  æ€»è®¡: {total_removed} ä¸ª")
    
    # ä¿å­˜æ¸…ç†åŽçš„æ•°æ®
    output_file = "/Users/mannormal/4011/Qi Zihan/v2/feature_extraction/result/features_cleaned_no_leakage1.csv"
    df_clean.to_csv(output_file, index=False)
    
    print(f"\nâœ… æ•°æ®æ¸…ç†å®Œæˆï¼")
    print(f"æ¸…ç†åŽæ•°æ®ä¿å­˜åˆ°: {output_file}")
    
    # æ˜¾ç¤ºå‰©ä½™çš„ç‰¹å¾åˆ—è¡¨
    remaining_features = [col for col in df_clean.columns if col != 'account']
    print(f"\nå‰©ä½™ç‰¹å¾ ({len(remaining_features)} ä¸ª):")
    for i, col in enumerate(remaining_features, 1):
        print(f"{i:2d}. {col}")
    
    # ç”Ÿæˆæ¸…ç†æŠ¥å‘Š
    generate_cleaning_report(
        original_shape, final_shape, 
        existing_leakage, columns_to_drop, zero_value_cols, 
        near_zero_cols, low_variance_cols,
        output_file
    )
    
    return df_clean

def generate_cleaning_report(original_shape, final_shape, leakage_cols, 
                           duplicate_cols, zero_cols, near_zero_cols, low_var_cols, output_file):
    """ç”Ÿæˆè¯¦ç»†çš„æ¸…ç†æŠ¥å‘Š"""
    
    report_file = output_file.replace('.csv', '_cleaning_report.txt')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("æ•°æ®æ¸…ç†æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        
        f.write(f"æ¸…ç†å‰æ•°æ®: {original_shape[0]} è¡Œ, {original_shape[1]} åˆ—\n")
        f.write(f"æ¸…ç†åŽæ•°æ®: {final_shape[0]} è¡Œ, {final_shape[1]} åˆ—\n")
        f.write(f"åˆ é™¤ç‰¹å¾æ•°: {original_shape[1] - final_shape[1]} ä¸ª\n\n")
        
        f.write("åˆ é™¤çš„ç‰¹å¾è¯¦æƒ…:\n")
        f.write("-" * 30 + "\n")
        
        f.write(f"\n1. æ•°æ®æ³„éœ²ç‰¹å¾ ({len(leakage_cols)} ä¸ª):\n")
        for col in leakage_cols:
            f.write(f"   - {col}\n")
        
        f.write(f"\n2. é‡å¤/é«˜ç›¸å…³ç‰¹å¾ ({len(duplicate_cols)} ä¸ª):\n")
        for col in duplicate_cols:
            f.write(f"   - {col}\n")
        
        f.write(f"\n3. é›¶å€¼ç‰¹å¾ ({len(zero_cols)} ä¸ª):\n")
        for col in zero_cols:
            f.write(f"   - {col}\n")
        
        f.write(f"\n4. è¿‘é›¶ç‰¹å¾ ({len(near_zero_cols)} ä¸ª):\n")
        for col in near_zero_cols:
            f.write(f"   - {col}\n")
        
        f.write(f"\n5. ä½Žæ–¹å·®ç‰¹å¾ ({len(low_var_cols)} ä¸ª):\n")
        for col in low_var_cols:
            f.write(f"   - {col}\n")
    
    print(f"æ¸…ç†æŠ¥å‘Šä¿å­˜åˆ°: {report_file}")

if __name__ == "__main__":
    try:
        result_df = remove_data_leakage_features()
        if result_df is not None:
            print("\nðŸŽ‰ æ•°æ®æ³„éœ²ç‰¹å¾æ¸…ç†æˆåŠŸå®Œæˆï¼")
            print("çŽ°åœ¨å¯ä»¥ç”¨æ¸…ç†åŽçš„æ•°æ®é‡æ–°è®­ç»ƒæ¨¡åž‹äº†ã€‚")
        else:
            print("âŒ æ¸…ç†å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºçŽ°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()