#!/usr/bin/env python3
"""
ç®€åŒ–F1åˆ†æ•°æ¨¡å‹ç”Ÿæˆå™¨
ç›´æ¥æ§åˆ¶precisionå’Œrecallç”Ÿæˆ0.71-0.98çš„ä¸åŒç­–ç•¥æ¨¡å‹
"""

import pandas as pd
import numpy as np
import os
import random

def load_perfect_labels(file_path):
    """åŠ è½½å®Œç¾æ ‡ç­¾æ–‡ä»¶"""
    try:
        df = pd.read_csv(file_path)
        print(f"âœ… æˆåŠŸåŠ è½½æ ‡ç­¾æ–‡ä»¶: {len(df)} ä¸ªè´¦æˆ·")
        
        bad_count = (df['Predict'] == 1).sum()
        good_count = (df['Predict'] == 0).sum()
        print(f"æ ‡ç­¾åˆ†å¸ƒ: Bad={bad_count}, Good={good_count}")
        
        return dict(zip(df['ID'], df['Predict']))
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ ‡ç­¾æ–‡ä»¶å¤±è´¥: {e}")
        return None

def calculate_recall_from_f1_precision(f1, precision):
    """æ ¹æ®F1å’Œprecisionè®¡ç®—recall"""
    if precision == 0:
        return 0
    # F1 = 2 * (precision * recall) / (precision + recall)
    # recall = (f1 * precision) / (2 * precision - f1)
    denominator = 2 * precision - f1
    if denominator <= 0:
        return None  # æ— æ•ˆç»„åˆ
    return (f1 * precision) / denominator

def calculate_precision_from_f1_recall(f1, recall):
    """æ ¹æ®F1å’Œrecallè®¡ç®—precision"""
    if recall == 0:
        return 0
    # precision = (f1 * recall) / (2 * recall - f1)
    denominator = 2 * recall - f1
    if denominator <= 0:
        return None  # æ— æ•ˆç»„åˆ
    return (f1 * recall) / denominator

def generate_strategy_configs():
    """
    ç”Ÿæˆä¸‰ç§ç­–ç•¥çš„precision/recallé…ç½®
    è¿”å›: {strategy_name: [(precision, recall, f1), ...]}
    """
    configs = {
        'high_precision_low_recall': [],
        'balanced': [],
        'low_precision_high_recall': []
    }
    
    # F1åˆ†æ•°èŒƒå›´ï¼š0.71 åˆ° 0.98ï¼Œæ­¥é•¿0.01
    f1_scores = [0.032]  # åªç”¨ä¸€ä¸ªF1åˆ†æ•°0.032

    for f1 in f1_scores:
        # ç­–ç•¥1: é«˜precisionä½recall
        # è®¾å®šprecisionåœ¨0.85-0.95ä¹‹é—´
        precision_high = min(0.95, f1 + 0.15)  # ç¡®ä¿precisionä¸ä¼šè¿‡é«˜
        recall_high = calculate_recall_from_f1_precision(f1, precision_high)
        if recall_high and recall_high > 0:
            configs['high_precision_low_recall'].append((precision_high, recall_high, f1))
        
        # ç­–ç•¥2: å¹³è¡¡precisionå’Œrecall
        # è®¾å®šprecisionå’Œrecallç›¸ç­‰
        balanced_value = f1  # å½“precision=recallæ—¶ï¼ŒF1=precision=recall
        configs['balanced'].append((balanced_value, balanced_value, f1))
        
        # ç­–ç•¥3: ä½precisioné«˜recall  
        # è®¾å®šrecallåœ¨0.85-0.95ä¹‹é—´
        recall_low = min(0.95, f1 + 0.15)
        precision_low = calculate_precision_from_f1_recall(f1, recall_low)
        if precision_low and precision_low > 0:
            configs['low_precision_high_recall'].append((precision_low, recall_low, f1))
    
    return configs

def generate_predictions_from_metrics(perfect_labels, precision, recall, true_bad=727, true_good=6831):
    """
    æ ¹æ®precisionå’Œrecallç”Ÿæˆé¢„æµ‹ç»“æœ
    
    Args:
        perfect_labels (dict): å®Œç¾æ ‡ç­¾
        precision (float): ç›®æ ‡precision
        recall (float): ç›®æ ‡recall  
        true_bad (int): çœŸå®badæ•°é‡
        true_good (int): çœŸå®goodæ•°é‡
    
    Returns:
        dict: ç”Ÿæˆçš„é¢„æµ‹ç»“æœ
    """
    
    # è®¡ç®—æ··æ·†çŸ©é˜µå‚æ•°
    tp = int(recall * true_bad)  # TP = recall * çœŸå®badæ•°é‡
    fn = true_bad - tp           # FN = çœŸå®badæ•°é‡ - TP
    
    # ä»precisionè®¡ç®—FP: precision = TP / (TP + FP)
    # FP = TP / precision - TP = TP * (1/precision - 1)
    if precision > 0:
        fp = int(tp / precision - tp)
    else:
        fp = true_good  # precision=0æ„å‘³ç€æ‰€æœ‰é¢„æµ‹çš„badéƒ½æ˜¯é”™çš„
    
    fp = max(0, min(fp, true_good))  # é™åˆ¶FPåœ¨åˆç†èŒƒå›´å†…
    tn = true_good - fp
    
    print(f"  ç›®æ ‡: precision={precision:.3f}, recall={recall:.3f}")
    print(f"  æ··æ·†çŸ©é˜µ: TP={tp}, FP={fp}, FN={fn}, TN={tn}")
    
    # éªŒè¯è®¡ç®—
    actual_precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    actual_recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    actual_f1 = 2 * (actual_precision * actual_recall) / (actual_precision + actual_recall) if (actual_precision + actual_recall) > 0 else 0
    print(f"  å®é™…: precision={actual_precision:.3f}, recall={actual_recall:.3f}, F1={actual_f1:.3f}")
    
    # ç”Ÿæˆé¢„æµ‹ç»“æœ
    predictions = {}
    
    # è·å–æ‰€æœ‰è´¦æˆ·IDåˆ—è¡¨
    all_accounts = list(perfect_labels.keys())
    bad_accounts = [aid for aid in all_accounts if perfect_labels[aid] == 1]
    good_accounts = [aid for aid in all_accounts if perfect_labels[aid] == 0]
    
    # éšæœºé€‰æ‹©è¦é¢„æµ‹ä¸ºbadçš„è´¦æˆ·
    random.shuffle(bad_accounts)
    random.shuffle(good_accounts)
    
    # TP: ä»çœŸå®badä¸­éšæœºé€‰æ‹©tpä¸ªé¢„æµ‹ä¸ºbad
    tp_accounts = bad_accounts[:tp]
    # FN: å‰©ä½™çš„çœŸå®badé¢„æµ‹ä¸ºgood  
    fn_accounts = bad_accounts[tp:]
    
    # FP: ä»çœŸå®goodä¸­éšæœºé€‰æ‹©fpä¸ªé¢„æµ‹ä¸ºbad
    fp_accounts = good_accounts[:fp] 
    # TN: å‰©ä½™çš„çœŸå®goodé¢„æµ‹ä¸ºgood
    tn_accounts = good_accounts[fp:]
    
    # ç»„è£…é¢„æµ‹ç»“æœ
    for aid in tp_accounts:
        predictions[aid] = 1
    for aid in fn_accounts:
        predictions[aid] = 0
    for aid in fp_accounts:
        predictions[aid] = 1  
    for aid in tn_accounts:
        predictions[aid] = 0
    
    return predictions

def save_predictions(predictions, strategy, f1_score, output_dir):
    """ä¿å­˜é¢„æµ‹ç»“æœåˆ°CSV"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    strategy_dir = os.path.join(output_dir, strategy)
    os.makedirs(strategy_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    filename = f"{strategy}_f1_{f1_score:.2f}.csv"
    filepath = os.path.join(strategy_dir, filename)
    
    # åˆ›å»ºDataFrameå¹¶ä¿å­˜
    pred_list = [{"ID": aid, "Predict": pred} for aid, pred in predictions.items()]
    df = pd.DataFrame(pred_list)
    df.to_csv(filepath, index=False)
    
    # ç»Ÿè®¡ä¿¡æ¯
    bad_count = sum(predictions.values())
    good_count = len(predictions) - bad_count
    print(f"  ğŸ’¾ ä¿å­˜: {filename} (Bad={bad_count}, Good={good_count})")
    
    return filepath

def main():
    print("=== F1åˆ†æ•°æ¨¡å‹ç”Ÿæˆå™¨ ===")
    
    # 1. åŠ è½½å®Œç¾æ ‡ç­¾
    perfect_labels_path = "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/best.csv"
    perfect_labels = load_perfect_labels(perfect_labels_path)
    
    if not perfect_labels:
        return
    
    # 2. è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v6"
    os.makedirs(output_dir, exist_ok=True)
    
    # 3. ç”Ÿæˆç­–ç•¥é…ç½®
    print("\n=== ç”Ÿæˆç­–ç•¥é…ç½® ===")
    strategy_configs = generate_strategy_configs()
    
    for strategy, configs in strategy_configs.items():
        print(f"{strategy}: {len(configs)} ä¸ªé…ç½®")
    
    # 4. ç”Ÿæˆæ¨¡å‹æ–‡ä»¶
    print("\n=== ç”Ÿæˆæ¨¡å‹æ–‡ä»¶ ===")
    
    total_files = 0
    
    for strategy, configs in strategy_configs.items():
        print(f"\n--- ç”Ÿæˆ {strategy} æ¨¡å‹ ---")
        
        for precision, recall, f1 in configs:
            print(f"ç”Ÿæˆ F1={f1:.2f}")
            
            # ç”Ÿæˆé¢„æµ‹ç»“æœ
            predictions = generate_predictions_from_metrics(
                perfect_labels, precision, recall
            )
            
            # ä¿å­˜æ–‡ä»¶
            save_predictions(predictions, strategy, f1, output_dir)
            total_files += 1
    
    print(f"\nâœ… å®Œæˆï¼æ€»å…±ç”Ÿæˆäº† {total_files} ä¸ªæ¨¡å‹æ–‡ä»¶")
    print(f"æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
    print("ç›®å½•ç»“æ„:")
    print("  high_precision_low_recall/")
    print("  balanced/") 
    print("  low_precision_high_recall/")

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­sä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    random.seed(42)
    np.random.seed(42)
    
    main()