import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
import warnings
import random
import os
import pickle
from datetime import datetime

warnings.filterwarnings('ignore')

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.backends.mps.is_available():
        torch.mps.manual_seed(seed)

seed_num = 13
set_seed(seed_num)

print("=== è¿‡æ‹Ÿåˆå®éªŒï¼šæœ€å¤§åŒ– Bad F1 ===")

# =====================================================
# æ¨¡å‹ä¿å­˜å‡½æ•°
# =====================================================
def save_model_and_artifacts(model, scaler, results, save_dir='saved_models'):
    """
    ä¿å­˜æ¨¡å‹ã€é¢„å¤„ç†å™¨å’Œè®­ç»ƒç»“æœ
    """
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜æ¨¡å‹æƒé‡
    model_path = os.path.join(save_dir, f'model_weights_{timestamp}.pth')
    torch.save(model.state_dict(), model_path)
    print(f"æ¨¡å‹æƒé‡å·²ä¿å­˜: {model_path}")
    
    # ä¿å­˜å®Œæ•´æ¨¡å‹(åŒ…å«ç»“æ„)
    full_model_path = os.path.join(save_dir, f'full_model_{timestamp}.pth')
    torch.save(model, full_model_path)
    print(f"å®Œæ•´æ¨¡å‹å·²ä¿å­˜: {full_model_path}")
    
    # ä¿å­˜é¢„å¤„ç†å™¨
    scaler_path = os.path.join(save_dir, f'scaler_{timestamp}.pkl')
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"é¢„å¤„ç†å™¨å·²ä¿å­˜: {scaler_path}")
    
    # ä¿å­˜è®­ç»ƒç»“æœå’Œè¶…å‚æ•°
    results_path = os.path.join(save_dir, f'training_results_{timestamp}.pkl')
    save_results = {
        'final_f1_bad': results['final_f1_bad'],
        'best_bad_f1': results['best_bad_f1'],
        'best_epoch': results['best_epoch'],
        'final_acc': results['final_acc'],
        'model_architecture': {
            'input_dim': model.layer1[0].in_features,
            'hidden_dims': [64, 32, 16],
            'output_dim': 1
        },
        'training_config': {
            'n_epochs': 2000,
            'learning_rate': 1e-3,
            'optimizer': 'Adam',
            'regularization': 'None',
            'early_stopping': False,
            'seed': seed_num
        },
        'timestamp': timestamp
    }
    
    with open(results_path, 'wb') as f:
        pickle.dump(save_results, f)
    print(f"è®­ç»ƒç»“æœå·²ä¿å­˜: {results_path}")
    
    # åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶
    info_path = os.path.join(save_dir, f'model_info_{timestamp}.txt')
    with open(info_path, 'w', encoding='utf-8') as f:
        f.write(f"æ¨¡å‹è®­ç»ƒä¿¡æ¯\n")
        f.write(f"==================\n")
        f.write(f"è®­ç»ƒæ—¶é—´: {timestamp}\n")
        f.write(f"éšæœºç§å­: {seed_num}\n")
        f.write(f"æ¨¡å‹æ¶æ„: {model.layer1[0].in_features} â†’ 64 â†’ 32 â†’ 16 â†’ 1\n")
        f.write(f"è®­ç»ƒè½®æ•°: 2000\n")
        f.write(f"å­¦ä¹ ç‡: 1e-3\n")
        f.write(f"ä¼˜åŒ–å™¨: Adam\n")
        f.write(f"æ­£åˆ™åŒ–: æ— \n")
        f.write(f"æ—©åœ: æ— \n\n")
        f.write(f"æ€§èƒ½æŒ‡æ ‡\n")
        f.write(f"==================\n")
        f.write(f"æœ€ç»ˆBad F1: {results['final_f1_bad']:.4f}\n")
        f.write(f"å†å²æœ€ä½³Bad F1: {results['best_bad_f1']:.4f} (ç¬¬{results['best_epoch']}è½®)\n")
        f.write(f"æœ€ç»ˆå‡†ç¡®ç‡: {results['final_acc']:.4f}\n\n")
        f.write(f"æ–‡ä»¶è¯´æ˜\n")
        f.write(f"==================\n")
        f.write(f"model_weights_{timestamp}.pth - æ¨¡å‹æƒé‡æ–‡ä»¶\n")
        f.write(f"full_model_{timestamp}.pth - å®Œæ•´æ¨¡å‹æ–‡ä»¶\n")
        f.write(f"scaler_{timestamp}.pkl - æ•°æ®é¢„å¤„ç†å™¨\n")
        f.write(f"training_results_{timestamp}.pkl - è®­ç»ƒç»“æœå’Œé…ç½®\n")
        f.write(f"model_info_{timestamp}.txt - æ­¤ä¿¡æ¯æ–‡ä»¶\n")
    
    print(f"æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {info_path}")
    
    return {
        'model_path': model_path,
        'full_model_path': full_model_path,
        'scaler_path': scaler_path,
        'results_path': results_path,
        'info_path': info_path,
        'timestamp': timestamp
    }

def load_model_and_artifacts(timestamp, save_dir='saved_models'):
    """
    åŠ è½½ä¿å­˜çš„æ¨¡å‹å’Œç›¸å…³ç»„ä»¶
    """
    # åŠ è½½å®Œæ•´æ¨¡å‹
    full_model_path = os.path.join(save_dir, f'full_model_{timestamp}.pth')
    model = torch.load(full_model_path)
    print(f"æ¨¡å‹å·²åŠ è½½: {full_model_path}")
    
    # åŠ è½½é¢„å¤„ç†å™¨
    scaler_path = os.path.join(save_dir, f'scaler_{timestamp}.pkl')
    with open(scaler_path, 'rb') as f:
        scaler = pickle.load(f)
    print(f"é¢„å¤„ç†å™¨å·²åŠ è½½: {scaler_path}")
    
    # åŠ è½½è®­ç»ƒç»“æœ
    results_path = os.path.join(save_dir, f'training_results_{timestamp}.pkl')
    with open(results_path, 'rb') as f:
        results = pickle.load(f)
    print(f"è®­ç»ƒç»“æœå·²åŠ è½½: {results_path}")
    
    return model, scaler, results

# =====================================================
# ç®€åŒ–çš„Meta-ANNæ¨¡å‹ï¼ˆå»é™¤æ­£åˆ™åŒ–ï¼‰
# =====================================================
class SimplifiedMetaANN(nn.Module):
    def __init__(self, n_feat):
        super().__init__()
        
        # 64 -> 32 -> 16 -> 1 æ¶æ„ï¼Œæ— æ­£åˆ™åŒ–
        self.layer1 = nn.Sequential(
            nn.Linear(n_feat, 64),
            nn.BatchNorm1d(64),
            nn.ReLU()
            # å»é™¤dropout
        )
        
        self.layer2 = nn.Sequential(
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU()
            # å»é™¤dropout
        )
        
        self.layer3 = nn.Sequential(
            nn.Linear(32, 16),
            nn.BatchNorm1d(16),
            nn.ReLU()
            # å»é™¤dropout
        )
        
        self.out = nn.Linear(16, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        return self.sigmoid(self.out(x))

def train_overfitting_model(features, y_true, n_epochs=2000):
    """
    å®Œå…¨è¿‡æ‹Ÿåˆè®­ç»ƒï¼Œæœ€å¤§åŒ–bad F1
    """
    print(f"\nè®­ç»ƒè¿‡æ‹Ÿåˆæ¨¡å‹")
    print(f"ç‰¹å¾ç»´åº¦: {features.shape}")
    print(f"è®­ç»ƒè½®æ•°: {n_epochs}")
    print(f"æ­£åˆ™åŒ–: æ— ")
    print(f"æ—©åœ: æ— ")
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")
    
    # è½¬æ¢ä¸ºå¼ é‡
    X_tensor = torch.tensor(features_scaled, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y_true.reshape(-1,1), dtype=torch.float32).to(device)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimplifiedMetaANN(n_feat=features_scaled.shape[1]).to(device)
    
    # ä¼˜åŒ–å™¨ï¼ˆå»é™¤æƒé‡è¡°å‡ï¼‰
    optimizer = optim.Adam(model.parameters(), lr=1e-3)  # å»é™¤weight_decay
    criterion = nn.BCELoss()
    
    print(f"\nå¼€å§‹è®­ç»ƒ...")
    print("Epoch | Train F1 | Good F1  | Bad F1   | Train Acc| Loss     | Status")
    print("-" * 75)
    
    best_bad_f1 = 0
    best_epoch = 0
    
    for epoch in range(n_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        optimizer.zero_grad()
        y_pred = model(X_tensor)
        loss = criterion(y_pred, y_tensor)
        loss.backward()
        optimizer.step()
        
        # è¯„ä¼°é˜¶æ®µ
        model.eval()
        with torch.no_grad():
            y_pred_prob = model(X_tensor).cpu().numpy().flatten()
            y_pred_label = (y_pred_prob > 0.5).astype(int)
            
            # è®¡ç®—å„ç§F1åˆ†æ•°
            train_f1_overall = metrics.f1_score(y_true, y_pred_label, average='binary', zero_division=0)
            train_f1_good = metrics.f1_score(y_true, y_pred_label, pos_label=0, zero_division=0)  # good=0
            train_f1_bad = metrics.f1_score(y_true, y_pred_label, pos_label=1, zero_division=0)   # bad=1
            train_acc = metrics.accuracy_score(y_true, y_pred_label)
            
            # è®°å½•æœ€ä½³bad F1
            if train_f1_bad > best_bad_f1:
                best_bad_f1 = train_f1_bad
                best_epoch = epoch
                best_model_state = model.state_dict().copy()
                status = "ğŸ† Best Bad F1"
            else:
                status = ""
        
        # æ‰“å°è¿›åº¦
        if epoch % 100 == 0 or status:
            print(f"{epoch:5d} | {train_f1_overall:8.4f} | {train_f1_good:8.4f} | {train_f1_bad:8.4f} | {train_acc:8.4f} | {loss.item():8.4f} | {status}")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if 'best_model_state' in locals():
        model.load_state_dict(best_model_state)
    
    # æœ€ç»ˆè¯„ä¼°
    model.eval()
    with torch.no_grad():
        final_pred_prob = model(X_tensor).cpu().numpy().flatten()
        final_pred_label = (final_pred_prob > 0.5).astype(int)
        
        final_f1_overall = metrics.f1_score(y_true, final_pred_label, average='binary', zero_division=0)
        final_f1_good = metrics.f1_score(y_true, final_pred_label, pos_label=0, zero_division=0)  # good=0
        final_f1_bad = metrics.f1_score(y_true, final_pred_label, pos_label=1, zero_division=0)   # bad=1
        final_f1_macro = metrics.f1_score(y_true, final_pred_label, average='macro', zero_division=0)
        final_f1_weighted = metrics.f1_score(y_true, final_pred_label, average='weighted', zero_division=0)
        final_acc = metrics.accuracy_score(y_true, final_pred_label)
        
        # åˆ†ç±»æŠ¥å‘Š
        print(f"\n" + "="*75)
        print("æœ€ç»ˆè¿‡æ‹Ÿåˆç»“æœ:")
        print(f"   è®­ç»ƒå‡†ç¡®ç‡: {final_acc:.4f}")
        print(f"   æ•´ä½“F1: {final_f1_overall:.4f}")
        print(f"   Goodç±»F1 (pos_label=0): {final_f1_good:.4f}")
        print(f"   Badç±»F1 (pos_label=1): {final_f1_bad:.4f}")
        print(f"   å®å¹³å‡F1: {final_f1_macro:.4f}")
        print(f"   åŠ æƒå¹³å‡F1: {final_f1_weighted:.4f}")
        print(f"   æœ€ä½³Bad F1å‡ºç°åœ¨ç¬¬{best_epoch}è½®: {best_bad_f1:.4f}")
        
        # æ··æ·†çŸ©é˜µ
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(y_true, final_pred_label)
        print(f"\næ··æ·†çŸ©é˜µ:")
        print(f"   é¢„æµ‹\\å®é™…  Good(0)  Bad(1)")
        print(f"   Good(0)    {cm[0,0]:6d}  {cm[0,1]:6d}")
        print(f"   Bad(1)     {cm[1,0]:6d}  {cm[1,1]:6d}")
        
        # é¢„æµ‹åˆ†å¸ƒ
        pred_counts = np.bincount(final_pred_label)
        total_samples = len(final_pred_label)
        print(f"\né¢„æµ‹åˆ†å¸ƒ:")
        print(f"   é¢„æµ‹ä¸ºGood(0): {pred_counts[0]} ({pred_counts[0]/total_samples*100:.1f}%)")
        print(f"   é¢„æµ‹ä¸ºBad(1): {pred_counts[1]} ({pred_counts[1]/total_samples*100:.1f}%)")
        
        # çœŸå®åˆ†å¸ƒ
        true_counts = np.bincount(y_true)
        print(f"\nçœŸå®åˆ†å¸ƒ:")
        print(f"   çœŸå®Good(0): {true_counts[0]} ({true_counts[0]/total_samples*100:.1f}%)")
        print(f"   çœŸå®Bad(1): {true_counts[1]} ({true_counts[1]/total_samples*100:.1f}%)")
        
    return {
        'model': model,
        'scaler': scaler,
        'final_f1_bad': final_f1_bad,
        'best_bad_f1': best_bad_f1,
        'best_epoch': best_epoch,
        'final_acc': final_acc,
        'final_predictions': final_pred_label,
        'final_probabilities': final_pred_prob
    }

def main():
    # =====================================================
    # æ•°æ®åŠ è½½
    # =====================================================
    print("\n=== åŠ è½½æ•°æ® ===")
    
    # ç‰¹å¾æ•°æ®
    features_path = '/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v2/feature_extraction/result/features_cleaned_no_leakage1.csv'
    all_features_df = pd.read_csv(features_path)
    print(f"ç‰¹å¾æ•°æ®: {all_features_df.shape}")
    
    # å®Œæ•´æ ‡ç­¾æ•°æ®
    labels_path = '/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/èåˆäºŒåˆ†æ¨¡å‹_æœ€ç»ˆç‰ˆ copy.csv'
    labels_df = pd.read_csv(labels_path)
    print(f"æ ‡ç­¾æ•°æ®: {labels_df.shape}")
    
    # é‡å‘½ååˆ—ä»¥åŒ¹é…
    labels_df = labels_df.rename(columns={'ID': 'account', 'Predict': 'flag'})
    
    # å»é™¤ç‰¹å¾æ•°æ®ä¸­å¯èƒ½å­˜åœ¨çš„flagåˆ—
    cols_to_drop = []
    if 'flag' in all_features_df.columns:
        cols_to_drop.append('flag')
    if 'data_type' in all_features_df.columns:
        cols_to_drop.append('data_type')
    
    if cols_to_drop:
        print(f"ä»ç‰¹å¾æ•°æ®ä¸­åˆ é™¤åˆ—: {cols_to_drop}")
        all_features_df = all_features_df.drop(cols_to_drop, axis=1)
    
    # åˆå¹¶æ•°æ®
    full_df = pd.merge(all_features_df, labels_df[['account', 'flag']], on='account', how='inner')
    print(f"åˆå¹¶åæ•°æ®: {full_df.shape}")
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    flag_counts = full_df['flag'].value_counts()
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(flag_counts)}")
    
    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    feature_cols = [col for col in full_df.columns if col not in ['account', 'flag']]
    features = full_df[feature_cols].values
    y_true = full_df['flag'].values
    
    print(f"æœ€ç»ˆç‰¹å¾çŸ©é˜µ: {features.shape}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: Good(0): {np.sum(y_true==0)}, Bad(1): {np.sum(y_true==1)}")
    print(f"ç±»åˆ«ä¸å¹³è¡¡æ¯”ä¾‹: 1:{np.sum(y_true==0)/max(np.sum(y_true==1), 1):.2f}")
    
    # =====================================================
    # è¿‡æ‹Ÿåˆè®­ç»ƒ
    # =====================================================
    print(f"\n{'='*75}")
    print("å¼€å§‹è¿‡æ‹Ÿåˆå®éªŒ")
    print(f"{'='*75}")
    
    results = train_overfitting_model(features, y_true, n_epochs=2000)
    
    # =====================================================
    # ç»“æœæ€»ç»“
    # =====================================================
    print(f"\n{'='*75}")
    print("è¿‡æ‹Ÿåˆå®éªŒå®Œæˆ")
    print(f"{'='*75}")
    
    print(f"\nğŸ¯ å…³é”®æŒ‡æ ‡:")
    print(f"   æœ€ç»ˆBad F1: {results['final_f1_bad']:.4f}")
    print(f"   å†å²æœ€ä½³Bad F1: {results['best_bad_f1']:.4f} (ç¬¬{results['best_epoch']}è½®)")
    print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {results['final_acc']:.4f}")
    
    print(f"\nğŸ“Š å®éªŒè®¾ç½®:")
    print(f"   æ•°æ®é›†: å®Œæ•´è®­ç»ƒ+æµ‹è¯•é›† ({full_df.shape[0]} æ ·æœ¬)")
    print(f"   æ¨¡å‹: ç®€åŒ–Meta-ANN (æ— æ­£åˆ™åŒ–)")
    print(f"   æ¶æ„: {features.shape[1]} â†’ 64 â†’ 32 â†’ 16 â†’ 1")
    print(f"   è®­ç»ƒè½®æ•°: 2000")
    print(f"   ä¼˜åŒ–å™¨: Adam (lr=1e-3, æ— æƒé‡è¡°å‡)")
    print(f"   æ—©åœ: æ— ")
    print(f"   æ•°æ®å¢å¼º: æ— ")
    print(f"   éªŒè¯é›†: æ—  (å®Œå…¨è¿‡æ‹Ÿåˆ)")
    
    print(f"\nâœ… è¿‡æ‹Ÿåˆå®éªŒç»“æŸ!")
    print(f"   ç†è®ºä¸Šé™Bad F1: {results['best_bad_f1']:.4f}")
    
    # =====================================================
    # ä¿å­˜æ¨¡å‹å’Œç›¸å…³ç»„ä»¶
    # =====================================================
    print(f"\n{'='*75}")
    print("ä¿å­˜æ¨¡å‹å’Œç›¸å…³ç»„ä»¶")
    print(f"{'='*75}")
    
    save_paths = save_model_and_artifacts(results['model'], results['scaler'], results)
    
    print(f"\nğŸ’¾ æ¨¡å‹ä¿å­˜å®Œæˆ!")
    print(f"   æ—¶é—´æˆ³: {save_paths['timestamp']}")
    print(f"   ä¿å­˜ç›®å½•: saved_models/")
    print(f"   æ¨¡å‹æƒé‡: {os.path.basename(save_paths['model_path'])}")
    print(f"   å®Œæ•´æ¨¡å‹: {os.path.basename(save_paths['full_model_path'])}")
    print(f"   é¢„å¤„ç†å™¨: {os.path.basename(save_paths['scaler_path'])}")
    print(f"   è®­ç»ƒç»“æœ: {os.path.basename(save_paths['results_path'])}")
    print(f"   æ¨¡å‹ä¿¡æ¯: {os.path.basename(save_paths['info_path'])}")
    
    return results

if __name__ == "__main__":
    results = main()
    
    # ç¤ºä¾‹ï¼šå¦‚ä½•åŠ è½½ä¿å­˜çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
    # æ³¨é‡Šæ‰ä¸‹é¢çš„ä»£ç ï¼Œéœ€è¦æ—¶å–æ¶ˆæ³¨é‡Š
    """
    # åŠ è½½ä¿å­˜çš„æ¨¡å‹ç¤ºä¾‹
    print(f"\n{'='*75}")
    print("æ¨¡å‹åŠ è½½ç¤ºä¾‹")
    print(f"{'='*75}")
    
    # ä½¿ç”¨ä¿å­˜çš„æ—¶é—´æˆ³åŠ è½½æ¨¡å‹
    saved_timestamp = "20241216_143052"  # æ›¿æ¢ä¸ºå®é™…çš„æ—¶é—´æˆ³
    
    try:
        loaded_model, loaded_scaler, loaded_results = load_model_and_artifacts(saved_timestamp)
        
        print(f"\nğŸ”„ æ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"   æ¨¡å‹æ¶æ„: {loaded_results['model_architecture']}")
        print(f"   è®­ç»ƒé…ç½®: {loaded_results['training_config']}")
        print(f"   æœ€ä½³æ€§èƒ½: Bad F1 = {loaded_results['best_bad_f1']:.4f}")
        
        # ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ç¤ºä¾‹
        # å‡è®¾æœ‰æ–°çš„ç‰¹å¾æ•°æ® new_features
        # new_features_scaled = loaded_scaler.transform(new_features)
        # loaded_model.eval()
        # with torch.no_grad():
        #     predictions = loaded_model(torch.tensor(new_features_scaled, dtype=torch.float32))
        #     pred_labels = (predictions.numpy() > 0.5).astype(int)
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    """