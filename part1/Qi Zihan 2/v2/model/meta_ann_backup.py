import pandas as pd
import numpy as np
import os
import argparse
from tqdm import tqdm
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim
import warnings
import random
from dataclasses import dataclass
from pathlib import Path

warnings.filterwarnings('ignore')

# Constants for consistent label handling
GOOD_LABEL = 0  # Good accounts
BAD_LABEL = 1   # Bad accounts (fraud)

@dataclass
class ModelConfig:
    """Configuration class for model hyperparameters"""
    n_rf_models: int = 200
    meta_ann_hidden: int = 128
    cv_folds: int = 5
    meta_ann_epochs: int = 500
    meta_ann_patience: int = 30
    meta_ann_dropout: float = 0.3
    meta_ann_lr: float = 1e-3
    meta_ann_weight_decay: float = 1e-4
    
class PathConfig:
    """Configuration for file paths"""
    def __init__(self, base_dir: str = '/Users/mannormal/4011/Qi Zihan'):
        self.base_dir = Path(base_dir)
        self.features_path = self.base_dir / 'v2/feature_extraction/result/features_cleaned_no_leakage1.csv'
        self.train_path = self.base_dir / 'original_data/train_acc.csv'
        self.test_path = self.base_dir / 'original_data/test_acc_predict.csv'
        self.results_dir = self.base_dir / 'v2/results'
        self.models_dir = self.base_dir / 'v2/models'
        self.strategy_base_dir = self.base_dir / 'v1/classification_strategies'
        
        # Create directories if they don't exist
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.models_dir.mkdir(parents=True, exist_ok=True)

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.backends.mps.is_available():
        # MPSè®¾å¤‡è®¾ç½®ç¡®å®šæ€§ï¼ˆå¦‚æœæ”¯æŒçš„è¯ï¼‰
        torch.mps.manual_seed(seed)

seed_num = 13
set_seed(seed_num)

# Initialize configurations
CONFIG = ModelConfig()
PATHS = PathConfig()

print("=== ULTRA Multi-Strategy Ensemble System with Meta-ANN ===")

# =====================================================
# æ•°æ®åŠ è½½å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
# =====================================================
def load_strategy_categories():
    strategy_paths = {
        'traditional': '/Users/mannormal/4011/Qi Zihan/v1/classification_strategies/traditional_4types/traditional_category_mapping.csv',
        'volume': '/Users/mannormal/4011/Qi Zihan/v1/classification_strategies/volume_based/volume_category_mapping.csv',
        'profit': '/Users/mannormal/4011/Qi Zihan/v1/classification_strategies/profit_based/profit_category_mapping.csv',
        'interaction': '/Users/mannormal/4011/Qi Zihan/v1/classification_strategies/interaction_based/interaction_category_mapping.csv',
        'behavior': '/Users/mannormal/4011/Qi Zihan/v1/classification_strategies/behavior_based/behavior_category_mapping.csv'
    }
    
    strategy_data = {}
    print("\n=== Loading Classification Strategies ===")
    for strategy_name, path in strategy_paths.items():
        if os.path.exists(path):
            strategy_data[strategy_name] = pd.read_csv(path)
            print(f"âœ… {strategy_name}: {len(strategy_data[strategy_name])} accounts")
        else:
            print(f"âŒ {strategy_name}: File not found")
    
    return strategy_data

def classify_account_type_improved(row):
    # è®¡ç®—å‰å‘åå‘äº¤æ˜“å¼ºåº¦
    forward_strength = (row['A_fprofit'] + row['B_fprofit']) / max(row['A_fsize'] + row['B_fsize'], 1)
    backward_strength = (row['A_bprofit'] + row['B_bprofit']) / max(row['A_bsize'] + row['B_bsize'], 1)
    
    # A/Bç±»å‹åå¥½ç¨‹åº¦
    a_dominance = (row['A_fprofit'] + row['A_bprofit']) / max(row['A_fprofit'] + row['A_bprofit'] + row['B_fprofit'] + row['B_bprofit'], 1)
    
    # ç½‘ç»œæ´»è·ƒåº¦ - ä½¿ç”¨ç°æœ‰ç‰¹å¾æ›¿ä»£å·²åˆ é™¤çš„ä¸­å¿ƒæ€§ç‰¹å¾
    network_activity = row['out_degree'] + row['in_degree'] + row['neighbor_count_1hop']
    
    # æ´»è·ƒåº¦
    activity_intensity = row['activity_intensity']
    
    # ä¼˜åŒ–åçš„é˜ˆå€¼ - åŸºäºæ•°æ®åˆ†æç»“æœ
    if network_activity > 0.528 and activity_intensity > 0.00189:  # 75%åˆ†ä½æ•°
        return 'type1'  # æ ¸å¿ƒæ¢çº½èŠ‚ç‚¹
    elif a_dominance > 0.479 and forward_strength > backward_strength:  # 80%åˆ†ä½æ•°
        return 'type2'  # Aç±»ä¸»å¯¼çš„å‘é€æ–¹
    elif a_dominance < 0.476 and backward_strength > forward_strength:  # 20%åˆ†ä½æ•°
        return 'type3'  # Bç±»ä¸»å¯¼çš„æ¥æ”¶æ–¹  
    else:
        return 'type4'  # æ··åˆäº¤æ˜“ç±»å‹

# =====================================================
# æ¨¡å‹è®­ç»ƒå‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼Œè¿”å›é¢„æµ‹ï¼‰
# =====================================================
def train_universal_ensemble(data, n_models=50):
    print(f"\n=== Training Universal Ensemble ({n_models} models) ===")
    data_copy = data.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    feature_cols = [col for col in data_copy.columns 
                   if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    sample_size = min(good_accounts, bad_accounts)
    
    predictions = []
    cv_scores = []
    X_all = data_copy[feature_cols].values
    y_all = data_copy['flag'].values
    
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    for i in tqdm(range(n_models), desc="Universal Models"):
        good_sample = data_copy[data_copy['flag'] == 1].sample(n=sample_size, replace=True, random_state=i)
        bad_sample = data_copy[data_copy['flag'] == 0].sample(n=sample_size, replace=True, random_state=i+1000)
        train_data = pd.concat([good_sample, bad_sample], axis=0)
        
        X_train = train_data[feature_cols].values
        y_train = train_data['flag'].values
        
        clf = RandomForestClassifier(
            n_estimators=100, 
            max_depth=15, 
            min_samples_split=10,
            random_state=i
        )
        clf.fit(X_train, y_train)
        
        cv_score = np.mean([clf.score(X_all[train_idx], y_all[train_idx]) 
                           for train_idx, _ in skf.split(X_all, y_all)])
        cv_scores.append(cv_score)
        
        y_pred = clf.predict_proba(X_all)[:, 1]  # æ¦‚ç‡é¢„æµ‹æ›´é€‚åˆåš meta-feature
        predictions.append(y_pred)
    
    return np.array(predictions), cv_scores

def train_strategy_ensemble(data, strategy_name, strategy_categories, n_models=10):
    print(f"\n=== Training {strategy_name.upper()} Strategy Ensemble ({n_models} models) ===")
    data_with_strategy = data.merge(strategy_categories, on='account', how='left')
    strategy_col = f"{strategy_name}_category"
    data_with_strategy[strategy_col] = data_with_strategy[strategy_col].fillna('unknown')
    
    data_copy = data_with_strategy.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    
    feature_cols = [col for col in data_copy.columns if col not in ['account', 'flag', 'account_type']]
    strategy_dummies = pd.get_dummies(data_copy[strategy_col], prefix=strategy_name)
    feature_data = pd.concat([
        data_copy[[col for col in feature_cols if not col.endswith('_category')]],
        strategy_dummies
    ], axis=1)
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    sample_size = min(good_accounts, bad_accounts)
    
    print(f"   Balanced sampling: {sample_size} per class")
    print(f"   Features: {feature_data.shape[1]} (base: {len([col for col in feature_cols if not col.endswith('_category')])}, strategy: {len(strategy_dummies.columns)})")
    
    predictions = []
    cv_scores = []
    X_all = feature_data.values
    y_all = data_copy['flag'].values
    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    
    for i in tqdm(range(n_models), desc=f"{strategy_name} Models"):
        good_sample = data_copy[data_copy['flag'] == 1].sample(n=sample_size, replace=True, random_state=i*100)
        bad_sample = data_copy[data_copy['flag'] == 0].sample(n=sample_size, replace=True, random_state=i*100+50)
        sample_indices = list(good_sample.index) + list(bad_sample.index)
        
        X_train = feature_data.loc[sample_indices].values
        y_train = pd.concat([good_sample, bad_sample])['flag'].values
        
        clf = RandomForestClassifier(
            n_estimators=120,
            max_depth=18,
            min_samples_split=8,
            min_samples_leaf=4,
            random_state=i*10,
            class_weight='balanced'
        )
        clf.fit(X_train, y_train)
        
        # è®¡ç®—åˆ†ç±»åˆ«F1åˆ†æ•°
        cv_f1_overall = []
        # cv_f1_good = []  # Not used
        # cv_f1_bad = []   # Not used
        
        for _, val_idx in skf.split(X_all, y_all):
            val_pred = clf.predict(X_all[val_idx])
            f1_overall = metrics.f1_score(y_all[val_idx], val_pred, zero_division=0)
            # f1_good = metrics.f1_score(y_all[val_idx], val_pred, pos_label=1, zero_division=0)  # Not used
            # f1_bad = metrics.f1_score(y_all[val_idx], val_pred, pos_label=0, zero_division=0)   # Not used
            
            cv_f1_overall.append(f1_overall)
            # cv_f1_good.append(f1_good)  # Not used
            # cv_f1_bad.append(f1_bad)    # Not used
        
        cv_score = np.mean(cv_f1_overall)
        cv_scores.append(cv_score)
        
        y_pred = clf.predict_proba(X_all)[:, 1]
        predictions.append(y_pred)
    
    predictions_array = np.array(predictions).T
    
    # è®¡ç®—å¹³å‡åˆ†ç±»åˆ«F1
    avg_f1_overall = np.mean(cv_scores)
    print(f"   Average CV F1 (Overall): {avg_f1_overall:.4f}")
    
    return predictions_array, cv_scores

# =====================================================
# PyTorch Meta-ANN with ResNet Connections - ä¿®æ”¹ç‚¹1
# =====================================================
class MetaANN(nn.Module):
    def __init__(self, n_base, n_feat, hidden=128, dropout=0.3):
        super().__init__()
        self.a = nn.Parameter(torch.ones(n_feat))
        self.b = nn.Parameter(torch.zeros(n_feat))
        
        self.input_dim = n_base + n_feat
        self.input_proj = nn.Linear(self.input_dim, hidden)
        
        # ResNet blocks
        self.res_block1 = self._make_res_block(hidden, dropout)
        self.res_block2 = self._make_res_block(hidden, dropout)
        self.res_block3 = self._make_res_block(hidden, dropout)
        
        self.out = nn.Linear(hidden, 1)
        self.sigmoid = nn.Sigmoid()
    
    def _make_res_block(self, dim, dropout):
        return nn.Sequential(
            nn.Linear(dim, dim),
            nn.BatchNorm1d(dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim, dim),
            nn.BatchNorm1d(dim),
        )
    
    def forward(self, x_base, x_feat):
        # ç‰¹å¾ç¼©æ”¾
        x_feat_scaled = self.a * x_feat + self.b
        
        # ç‰¹å¾èåˆ
        x = torch.cat([x_base, x_feat_scaled], dim=1)
        x = torch.relu(self.input_proj(x))
        
        # æ®‹å·®è¿æ¥
        residual = x
        x = residual + self.res_block1(x)
        x = torch.relu(x)
        
        residual = x
        x = residual + self.res_block2(x)
        x = torch.relu(x)
        
        residual = x
        x = residual + self.res_block3(x)
        x = torch.relu(x)
        
        return self.sigmoid(self.out(x))

def train_pytorch_meta_ann(base_predictions, original_features, y_true, n_epochs=500, patience=30):
    """
    ä½¿ç”¨PyTorchè®­ç»ƒMeta-ANN
    base_predictions: (n_samples, n_models) - åŸºç¡€æ¨¡å‹é¢„æµ‹
    original_features: (n_samples, n_features) - åŸå§‹ç‰¹å¾
    y_true: (n_samples,) - çœŸå®æ ‡ç­¾
    """
    print(f"\nğŸ¤– Training PyTorch Meta-ANN")
    print(f"Base predictions shape: {base_predictions.shape}")
    print(f"Original features shape: {original_features.shape}")
    
    # æ ‡å‡†åŒ–åŸå§‹ç‰¹å¾
    scaler = StandardScaler()
    original_features = scaler.fit_transform(original_features)
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_base_tensor = torch.tensor(base_predictions, dtype=torch.float32).to(device)
    X_feat_tensor = torch.tensor(original_features, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y_true.reshape(-1,1), dtype=torch.float32).to(device)
    
    # åˆ›å»ºæ¨¡å‹
    model = MetaANN(
        n_base=base_predictions.shape[1], 
        n_feat=original_features.shape[1],
        hidden=128,
        dropout=0.3
    ).to(device)
    
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    criterion = nn.BCELoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=10)

    # äº¤å‰éªŒè¯åˆ†å‰²ç”¨äºæ—©åœ
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    train_idx, val_idx = list(skf.split(base_predictions, y_true))[0]
    
    Xb_train, Xb_val = X_base_tensor[train_idx], X_base_tensor[val_idx]
    Xf_train, Xf_val = X_feat_tensor[train_idx], X_feat_tensor[val_idx]
    y_train, y_val = y_tensor[train_idx], y_tensor[val_idx]
    
    best_val_f1 = 0
    patience_counter = 0
    train_f1_history = []
    val_f1_history = []
    
    print("\nEpoch | Train F1 | Val F1   | Good F1  | Bad F1   | LR       | Status")
    print("-" * 70)
    
    for epoch in range(n_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        optimizer.zero_grad()
        y_pred_train = model(Xb_train, Xf_train)
        loss = criterion(y_pred_train, y_train)
        loss.backward()
        optimizer.step()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        with torch.no_grad():
            y_pred_val = model(Xb_val, Xf_val)
            
            # è®¡ç®—F1åˆ†æ•°
            y_train_prob = model(Xb_train, Xf_train).cpu().numpy()
            y_val_prob = y_pred_val.cpu().numpy()
            
            train_pred = (y_train_prob > 0.5).astype(int).flatten()
            val_pred = (y_val_prob > 0.5).astype(int).flatten()
            
            # æ•´ä½“F1å’Œåˆ†ç±»åˆ«F1
            train_f1 = metrics.f1_score(y_true[train_idx], train_pred, zero_division=0)
            val_f1 = metrics.f1_score(y_true[val_idx], val_pred, zero_division=0)
            
            # åˆ†åˆ«è®¡ç®—Goodç±»(1)å’ŒBadç±»(0)çš„F1
            val_f1_good = metrics.f1_score(y_true[val_idx], val_pred, pos_label=1, zero_division=0)
            val_f1_bad = metrics.f1_score(y_true[val_idx], val_pred, pos_label=0, zero_division=0)
            
            train_f1_history.append(train_f1)
            val_f1_history.append(val_f1)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_f1)
        current_lr = optimizer.param_groups[0]['lr']
        
        # æ—©åœæ£€æŸ¥
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            best_model_state = model.state_dict().copy()
            status = "âœ… Best"
        else:
            patience_counter += 1
            status = f"â³ {patience_counter}/{patience}"
        
        # æ‰“å°è¿›åº¦
        if epoch % 50 == 0 or patience_counter == 0:
            print(f"{epoch:5d} | {train_f1:8.4f} | {val_f1:8.4f} | {val_f1_good:8.4f} | {val_f1_bad:8.4f} | {current_lr:.2e} | {status}")
        
        if patience_counter >= patience:
            print(f"\nğŸ›‘ Early stopping at epoch {epoch}")
            print(f"ğŸ† Best validation F1: {best_val_f1:.4f}")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_model_state)
    model.eval()
    
    # åœ¨å…¨éƒ¨è®­ç»ƒæ•°æ®ä¸Šé¢„æµ‹
    with torch.no_grad():
        y_final_pred = model(X_base_tensor, X_feat_tensor).cpu().numpy()
        y_final_label = (y_final_pred > 0.5).astype(int).flatten()
        
        final_f1 = metrics.f1_score(y_true, y_final_label, zero_division=0)
        final_f1_good = metrics.f1_score(y_true, y_final_label, pos_label=1, zero_division=0)
        final_f1_bad = metrics.f1_score(y_true, y_final_label, pos_label=0, zero_division=0)
        final_acc = metrics.accuracy_score(y_true, y_final_label)
        
    print(f"\nğŸ“Š Meta-ANN Final Results:")
    print(f"   Accuracy: {final_acc:.4f}")
    print(f"   Overall F1: {final_f1:.4f}")
    print(f"   Good Class F1 (pos_label=1): {final_f1_good:.4f}")
    print(f"   Bad Class F1 (pos_label=0): {final_f1_bad:.4f}")
    print(f"   Best Val F1: {best_val_f1:.4f}")
    print(f"   Overfitting: {train_f1_history[-1] - best_val_f1:+.4f}")
    
    return y_final_pred, model, scaler, {
        'final_f1': final_f1,
        'final_f1_good': final_f1_good,
        'final_f1_bad': final_f1_bad,
        'final_acc': final_acc,
        'best_val_f1': best_val_f1,
        'train_f1_history': train_f1_history,
        'val_f1_history': val_f1_history
    }

# =====================================================
# Enhanced Random Forest Ensemble Training
# =====================================================
def train_enhanced_rf_ensemble(data, n_models=100):
    """è®­ç»ƒå¢å¼ºçš„éšæœºæ£®æ—é›†æˆ - ä¼˜åŒ–ç‰ˆæœ¬"""
    print(f"\nğŸŒ³ Training Enhanced RF Ensemble ({n_models} models)")
    
    data_copy = data.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    feature_cols = [col for col in data_copy.columns 
                   if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    sample_size = min(good_accounts, bad_accounts)
    
    print(f"   ğŸ’¡ Data Info:")
    print(f"      Good accounts: {good_accounts}")
    print(f"      Bad accounts: {bad_accounts}")
    print(f"      Balanced sampling: {sample_size} per class")
    print(f"      Features: {len(feature_cols)}")
    print(f"      Imbalance ratio: 1:{bad_accounts//good_accounts}")
    
    X_all = data_copy[feature_cols].values
    y_all = data_copy['flag'].values
    
    predictions = []
    cv_scores = []
    
    # ä¼˜åŒ–çš„éšæœºæ£®æ—é…ç½® - æ›´æ·±æ›´å¤æ‚
    rf_configs = [
        {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 8, 'min_samples_leaf': 3},
        {'n_estimators': 180, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 2},
        {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4},
        {'n_estimators': 220, 'max_depth': 35, 'min_samples_split': 12, 'min_samples_leaf': 5},
    ]
    
    for i in tqdm(range(n_models), desc="RF Models"):
        # æ›´æ¿€è¿›çš„é‡‡æ ·ç­–ç•¥ - å¢åŠ æ ·æœ¬å¤šæ ·æ€§
        bootstrap_ratio = 0.8 + 0.4 * np.random.random()  # 0.8-1.2å€é‡‡æ ·
        actual_sample_size = int(sample_size * bootstrap_ratio)
        
        good_sample = data_copy[data_copy['flag'] == 1].sample(
            n=actual_sample_size, replace=True, random_state=i
        )
        bad_sample = data_copy[data_copy['flag'] == 0].sample(
            n=actual_sample_size, replace=True, random_state=i+3000
        )
        train_data = pd.concat([good_sample, bad_sample], axis=0)
        
        X_train = train_data[feature_cols].values
        y_train = train_data['flag'].values
        
        # å¾ªç¯ä½¿ç”¨ä¸åŒé…ç½®
        config = rf_configs[i % len(rf_configs)]
        
        clf = RandomForestClassifier(
            **config,
            random_state=i,
            class_weight='balanced_subsample',  # æ›´å¥½çš„ç±»å¹³è¡¡
            max_features='sqrt',  # ç‰¹å¾é€‰æ‹©ç­–ç•¥
            bootstrap=True,
            oob_score=True,
            n_jobs=1
        )
        clf.fit(X_train, y_train)
        
        # ä½¿ç”¨Out-of-Bagè¯„ä¼° + äº¤å‰éªŒè¯
        # oob_score = clf.oob_score_ if hasattr(clf, 'oob_score_') else 0  # Not used
        
        # 5æŠ˜äº¤å‰éªŒè¯
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)
        cv_f1_scores = []
        for _, val_idx in skf.split(X_all, y_all):
            val_pred = clf.predict(X_all[val_idx])
            f1 = metrics.f1_score(y_all[val_idx], val_pred, zero_division=0)
            cv_f1_scores.append(f1)
        
        cv_score = np.mean(cv_f1_scores)
        cv_scores.append(cv_score)
        
        # æ¦‚ç‡é¢„æµ‹
        y_pred_proba = clf.predict_proba(X_all)[:, 1]
        predictions.append(y_pred_proba)
    
    predictions_array = np.array(predictions).T  # (n_samples, n_models)
    avg_cv_score = np.mean(cv_scores)
    
    print(f"   ğŸ“Š Results:")
    print(f"      Average CV F1: {avg_cv_score:.4f}")
    print(f"      CV F1 std: {np.std(cv_scores):.4f}")
    print(f"      CV F1 range: [{np.min(cv_scores):.4f}, {np.max(cv_scores):.4f}]")
    
    return predictions_array, cv_scores, feature_cols

# =====================================================
# Strategy-Specific RF Ensemble Training (200 models distributed)
# =====================================================
def train_strategy_specific_rf_ensemble(data, strategy_data, test_data=None):
    """è®­ç»ƒæŒ‰åˆ†ç±»ç­–ç•¥åˆ†é…çš„ä¸“ç”¨RFé›†æˆ - 200ä¸ªæ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒï¼ŒåŒæ—¶ç”Ÿæˆæµ‹è¯•é¢„æµ‹"""
    print(f"\nğŸ¯ Training Strategy-Specific RF Ensemble (200 models distributed)")
    if test_data is not None:
        print(f"   ğŸ“Š Also generating test predictions during training")
    
    # RFåˆ†é…ç­–ç•¥
    rf_allocation = {
        'account_type': {'type4': 25, 'type3': 15, 'type1': 10, 'type2': 10},  # 60ä¸ª
        'traditional': {'isolated': 15, 'backward_only': 10, 'both_directions': 8, 'forward_only': 7},  # 40ä¸ª
        'interaction': {'B_in_B_out': 12, 'A_in_B_in_B_out': 8, 'A_in_A_out_B_in_B_out': 6, 
                       'A_out_B_in_B_out': 5, 'B_in': 2, 'small_categories': 2},  # 35ä¸ª
        'behavior': {'inactive': 12, 'low_activity': 10, 'medium_activity_unidirectional': 5, 
                    'medium_activity_bidirectional': 3},  # 30ä¸ª
        'volume': {'no_transactions': 8, 'medium_volume': 6, 'low_volume': 4, 'high_volume': 2},  # 20ä¸ª
        'profit': {'loss_or_zero': 8, 'very_high_profit': 7}  # 15ä¸ª
    }
    
    data_copy = data.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    feature_cols = [col for col in data_copy.columns 
                   if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]
    
    X_all = data_copy[feature_cols].values
    y_all = data_copy['flag'].values
    
    all_predictions = []
    all_test_predictions = []  # Store test predictions
    all_cv_scores = []
    model_count = 0
    
    # Prepare test data if provided
    if test_data is not None:
        test_data_copy = test_data.copy()
        X_test = test_data_copy[feature_cols].values
    
    print(f"   ğŸ“Š Distribution Overview:")
    total_models = sum(sum(allocation.values()) for allocation in rf_allocation.values())
    print(f"      Total models planned: {total_models}")
    
    # éå†æ¯ä¸ªåˆ†ç±»ç­–ç•¥
    for strategy_name, allocation in rf_allocation.items():
        print(f"\n   ğŸ”¸ Training {strategy_name.upper()} strategy models...")
        
        if strategy_name == 'account_type':
            # ä½¿ç”¨å†…ç½®è´¦æˆ·åˆ†ç±»
            data_copy['current_strategy'] = data_copy.apply(classify_account_type_improved, axis=1)
        else:
            # ä½¿ç”¨å¤–éƒ¨ç­–ç•¥æ–‡ä»¶
            if strategy_name in strategy_data and not strategy_data[strategy_name].empty:
                strategy_df = strategy_data[strategy_name]
                strategy_mapping = dict(zip(strategy_df.iloc[:, 0], strategy_df.iloc[:, 1]))
                data_copy['current_strategy'] = data_copy['account'].map(strategy_mapping)
            else:
                print(f"      âš ï¸  {strategy_name} strategy data not available, skipping...")
                continue
        
        # ä¸ºæ¯ä¸ªç±»åˆ«è®­ç»ƒåˆ†é…çš„RFæ¨¡å‹
        for category, n_models in allocation.items():
            if category == 'small_categories':
                # å¤„ç†interactionç­–ç•¥çš„å°ç±»åˆ«
                if strategy_name == 'interaction':
                    small_cats = ['A_in_B_in', 'B_out', 'A_out_B_out', 'A_in_B_out', 
                                 'A_in_A_out_B_out', 'A_in_A_out_B_in', 'A_out_B_in', 'A_out']
                    category_data = data_copy[data_copy['current_strategy'].isin(small_cats)]
                else:
                    continue
            else:
                category_data = data_copy[data_copy['current_strategy'] == category]
            
            if len(category_data) < 10:  # å¦‚æœç±»åˆ«æ ·æœ¬å¤ªå°‘ï¼Œè·³è¿‡
                print(f"      âš ï¸  {category}: Only {len(category_data)} samples, skipping...")
                continue
            
            print(f"      ğŸ¯ {category}: {len(category_data)} samples, {n_models} models")
            
            # ä¸ºå½“å‰ç±»åˆ«è®­ç»ƒn_modelsä¸ªRF
            if test_data is not None:
                category_predictions, category_test_predictions, category_cv_scores = train_category_specific_models(
                    category_data, data_copy, feature_cols, n_models, model_count, X_test
                )
                all_test_predictions.extend(category_test_predictions)
            else:
                category_predictions, category_cv_scores = train_category_specific_models(
                    category_data, data_copy, feature_cols, n_models, model_count
                )
            
            all_predictions.extend(category_predictions)
            all_cv_scores.extend(category_cv_scores)
            model_count += n_models
    
    # è½¬æ¢ä¸ºæ•°ç»„æ ¼å¼
    predictions_array = np.array(all_predictions).T  # (n_samples, n_models)
    
    print(f"\n   ğŸ“Š Strategy-Specific Training Results:")
    print(f"      Total models trained: {len(all_predictions)}")
    print(f"      Average CV F1: {np.mean(all_cv_scores):.4f}")
    print(f"      CV F1 std: {np.std(all_cv_scores):.4f}")
    print(f"      CV F1 range: [{np.min(all_cv_scores):.4f}, {np.max(all_cv_scores):.4f}]")
    
    # å¤„ç†æµ‹è¯•é¢„æµ‹
    test_predictions_array = None
    if test_data is not None and len(all_test_predictions) > 0:
        test_predictions_array = np.array(all_test_predictions).T  # (n_test_samples, n_models)
        print(f"      Test predictions generated: {test_predictions_array.shape}")
    
    # æ·»åŠ è¯¦ç»†çš„F1è¯„ä¼°
    if len(all_predictions) > 0:
        print(f"\n   ğŸ” Detailed F1 Evaluation on Training Data:")
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œç»¼åˆè¯„ä¼°
        y_true = data_copy['flag'].values
        
        # é›†æˆé¢„æµ‹ (ç®€å•å¹³å‡)
        ensemble_pred_proba = np.mean(predictions_array, axis=1)
        ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)
        
        # è®¡ç®—å„ç§F1åˆ†æ•°
        from sklearn.metrics import f1_score
        
        f1_bad = f1_score(y_true, ensemble_pred, pos_label=1)   # badç±»F1 (æ ‡ç­¾1ä¸ºbad)
        f1_good = f1_score(y_true, ensemble_pred, pos_label=0)  # goodç±»F1 (æ ‡ç­¾0ä¸ºgood)
        f1_macro = f1_score(y_true, ensemble_pred, average='macro')
        f1_weighted = f1_score(y_true, ensemble_pred, average='weighted')
        
        print(f"      Bad F1 (pos_label=1): {f1_bad:.4f}")
        print(f"      Good F1 (pos_label=0): {f1_good:.4f}")
        print(f"      Macro F1: {f1_macro:.4f}")
        print(f"      Weighted F1: {f1_weighted:.4f}")
        
        # è¿‡æ»¤æ‰æ•ˆæœç‰¹åˆ«å·®çš„æ¨¡å‹ï¼ˆF1 < 0.5ï¼‰
        good_cv_scores = [score for score in all_cv_scores if score >= 0.5]
        if len(good_cv_scores) < len(all_cv_scores):
            print(f"      âš ï¸  {len(all_cv_scores) - len(good_cv_scores)} models with F1 < 0.5 (possibly from small categories)")
            print(f"      Good models (F1â‰¥0.5): {len(good_cv_scores)}, Avg F1: {np.mean(good_cv_scores):.4f}")
    else:
        print(f"      âš ï¸  No models trained successfully")
    
    if test_data is not None:
        return predictions_array, test_predictions_array, all_cv_scores, feature_cols
    else:
        return predictions_array, all_cv_scores, feature_cols

def train_category_specific_models(category_data, full_data, feature_cols, n_models, base_seed, X_test=None):
    """ä¸ºç‰¹å®šç±»åˆ«è®­ç»ƒä¸“ç”¨RFæ¨¡å‹ï¼ŒåŒæ—¶ç”Ÿæˆæµ‹è¯•é¢„æµ‹"""
    
    # æ£€æŸ¥ç±»åˆ«æ•°æ®æ˜¯å¦æœ‰good/badæ ·æœ¬
    good_accounts = len(category_data[category_data['flag'] == 1])
    bad_accounts = len(category_data[category_data['flag'] == 0])
    
    if good_accounts == 0 or bad_accounts == 0:
        # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œä½¿ç”¨å…¨å±€æ•°æ®è¿›è¡Œå¹³è¡¡é‡‡æ ·
        print(f"        âš ï¸  Category has only one class, using global sampling")
        if X_test is not None:
            return [], [], []
        else:
            return [], []
    
    sample_size = min(good_accounts, bad_accounts, 500)  # é™åˆ¶æœ€å¤§æ ·æœ¬æ•°
    
    # ç”¨ç±»åˆ«æ•°æ®è®­ç»ƒï¼Œä½†å¯¹å…¨æ•°æ®é›†é¢„æµ‹
    X_category = category_data[feature_cols].values
    y_category = category_data['flag'].values
    X_all = full_data[feature_cols].values
    
    predictions = []
    test_predictions = []
    cv_scores = []
    
    # RFé…ç½®ï¼ˆé’ˆå¯¹ç±»åˆ«ä¼˜åŒ–ï¼‰
    rf_configs = [
        {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2},
        {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 3},
        {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 6, 'min_samples_leaf': 1},
    ]
    
    for i in range(n_models):
        current_seed = base_seed + i
        
        # å¹³è¡¡é‡‡æ ·
        good_sample = category_data[category_data['flag'] == 1].sample(
            n=sample_size, replace=True, random_state=current_seed
        )
        bad_sample = category_data[category_data['flag'] == 0].sample(
            n=sample_size, replace=True, random_state=current_seed + 5000
        )
        train_data = pd.concat([good_sample, bad_sample], axis=0)
        
        X_train = train_data[feature_cols].values
        y_train = train_data['flag'].values
        
        # é€‰æ‹©é…ç½®
        config = rf_configs[i % len(rf_configs)]
        
        clf = RandomForestClassifier(
            **config,
            random_state=current_seed,
            class_weight='balanced',
            max_features='sqrt',
            bootstrap=True,
            n_jobs=1
        )
        clf.fit(X_train, y_train)
        
        # äº¤å‰éªŒè¯è¯„ä¼°ï¼ˆåœ¨ç±»åˆ«æ•°æ®ä¸Šï¼‰
        if len(np.unique(y_category)) > 1:  # ç¡®ä¿æœ‰ä¸¤ä¸ªç±»åˆ«
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=current_seed)
            cv_f1_scores = []
            for _, val_idx in skf.split(X_category, y_category):
                if len(val_idx) > 0:
                    val_pred = clf.predict(X_category[val_idx])
                    f1 = metrics.f1_score(y_category[val_idx], val_pred, zero_division=0)
                    cv_f1_scores.append(f1)
            
            cv_score = np.mean(cv_f1_scores) if cv_f1_scores else 0
        else:
            cv_score = 0
        
        cv_scores.append(cv_score)
        
        # æ¦‚ç‡é¢„æµ‹ï¼ˆå¯¹å…¨æ•°æ®é›†ï¼‰
        if hasattr(clf, 'predict_proba'):
            proba_pred = clf.predict_proba(X_all)
            y_pred_proba = proba_pred[:, 1] if proba_pred.shape[1] > 1 else proba_pred[:, 0]
        else:
            y_pred_proba = clf.predict(X_all).astype(float)
        
        predictions.append(y_pred_proba)
        
        # Generate test predictions if X_test is provided
        if X_test is not None:
            if hasattr(clf, 'predict_proba'):
                test_proba_pred = clf.predict_proba(X_test)
                test_pred_proba = test_proba_pred[:, 1] if test_proba_pred.shape[1] > 1 else test_proba_pred[:, 0]
            else:
                test_pred_proba = clf.predict(X_test).astype(float)
            test_predictions.append(test_pred_proba)
    
    if X_test is not None:
        return predictions, test_predictions, cv_scores
    else:
        return predictions, cv_scores

# =====================================================
# Note: generate_test_rf_predictions function removed
# Test predictions are now generated during training phase
# =====================================================

# =====================================================
# Meta-ANN (stacking ç¬¬äºŒå±‚) - ä¿æŒåŸæœ‰çš„sklearnç‰ˆæœ¬ä½œä¸ºå¯¹æ¯”
# =====================================================
def ultra_ensemble_meta_ann(all_predictions, y_true):
    """
    ç”¨ ANN ä½œä¸º meta-classifier
    all_predictions: (n_models, n_samples)
    y_true: (n_samples,)
    """
    X_meta = all_predictions.T  # shape: (n_samples, n_models)
    
    ann = MLPClassifier(
        hidden_layer_sizes=(64, 32),
        activation='relu',
        solver='adam',
        max_iter=200,
        random_state=42
    )
    ann.fit(X_meta, y_true)
    
    y_pred = ann.predict(X_meta)
    f1 = metrics.f1_score(y_true, y_pred, average='binary', zero_division=0)
    print(f"Meta-ANN Training F1: {f1:.4f}")
    
    return y_pred, ann

# =====================================================
# è®­ç»ƒå•ä¸ªfoldçš„Meta-ANN - ä¿®æ”¹ç‚¹2ï¼šæ–°å¢å‡½æ•°
# =====================================================
def train_single_fold_meta_ann(X_base_train, X_feat_train, y_train, X_base_val, X_feat_val, y_val, 
                               fold_id, f1_type='bad', n_epochs=500, patience=30):
    """è®­ç»ƒå•ä¸ªfoldçš„Meta-ANNï¼Œä½¿ç”¨æ—©åœ"""
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # ç‰¹å¾ç¼©æ”¾
    scaler_fold = StandardScaler()
    X_feat_train_scaled = scaler_fold.fit_transform(X_feat_train)
    X_feat_val_scaled = scaler_fold.transform(X_feat_val)
    
    # åˆ›å»ºæ¨¡å‹
    model_fold = MetaANN(
        n_base=X_base_train.shape[1], 
        n_feat=X_feat_train_scaled.shape[1],
        hidden=128,
        dropout=0.3
    ).to(device)
    
    optimizer = optim.AdamW(model_fold.parameters(), lr=1e-3, weight_decay=1e-4)
    criterion = nn.BCELoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=10)
    
    # è½¬æ¢ä¸ºå¼ é‡
    X_base_train_t = torch.tensor(X_base_train, dtype=torch.float32).to(device)
    X_feat_train_t = torch.tensor(X_feat_train_scaled, dtype=torch.float32).to(device)
    y_train_t = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32).to(device)
    
    X_base_val_t = torch.tensor(X_base_val, dtype=torch.float32).to(device)
    X_feat_val_t = torch.tensor(X_feat_val_scaled, dtype=torch.float32).to(device)
    
    best_val_f1 = 0
    patience_counter = 0
    best_model_state = None
    
    # å®Œæ•´è®­ç»ƒå¸¦æ—©åœ
    for epoch in range(n_epochs):
        model_fold.train()
        optimizer.zero_grad()
        y_pred = model_fold(X_base_train_t, X_feat_train_t)
        loss = criterion(y_pred, y_train_t)
        loss.backward()
        optimizer.step()
        
        # éªŒè¯
        model_fold.eval()
        with torch.no_grad():
            val_pred_prob = model_fold(X_base_val_t, X_feat_val_t).cpu().numpy()
            val_pred_label = (val_pred_prob > 0.5).astype(int).flatten()
            
            # Calculate F1 based on f1_type  
            if f1_type == 'bad':
                val_f1 = metrics.f1_score(y_val, val_pred_label, pos_label=1, zero_division=0)  # bad=1
            elif f1_type == 'macro':
                val_f1 = metrics.f1_score(y_val, val_pred_label, average='macro', zero_division=0)
            elif f1_type == 'weighted':
                val_f1 = metrics.f1_score(y_val, val_pred_label, average='weighted', zero_division=0)
            else:  # default to 'bad'
                val_f1 = metrics.f1_score(y_val, val_pred_label, pos_label=1, zero_division=0)  # bad=1
        
        scheduler.step(val_f1)
        
        # æ—©åœæ£€æŸ¥
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            best_model_state = model_fold.state_dict().copy()
        else:
            patience_counter += 1
        
        if patience_counter >= patience:
            print(f"\nğŸ›‘ Early stopping at epoch {epoch}")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        model_fold.load_state_dict(best_model_state)
    
    # æœ€ç»ˆè¯„ä¼°
    model_fold.eval()
    with torch.no_grad():
        train_pred = model_fold(X_base_train_t, X_feat_train_t).cpu().numpy()
        val_pred = model_fold(X_base_val_t, X_feat_val_t).cpu().numpy()
        
        train_label = (train_pred > 0.5).astype(int).flatten()
        val_label = (val_pred > 0.5).astype(int).flatten()
        
        # Calculate all f1 scores
        if f1_type == 'bad':
            train_f1 = metrics.f1_score(y_train, train_label, pos_label=1, zero_division=0)  # bad=1
            val_f1 = metrics.f1_score(y_val, val_label, pos_label=1, zero_division=0)       # bad=1
        elif f1_type == 'macro':
            train_f1 = metrics.f1_score(y_train, train_label, average='macro', zero_division=0)
            val_f1 = metrics.f1_score(y_val, val_label, average='macro', zero_division=0)
        elif f1_type == 'weighted':
            train_f1 = metrics.f1_score(y_train, train_label, average='weighted', zero_division=0)
            val_f1 = metrics.f1_score(y_val, val_label, average='weighted', zero_division=0)
        else:
            train_f1 = metrics.f1_score(y_train, train_label, pos_label=1, zero_division=0)  # bad=1
            val_f1 = metrics.f1_score(y_val, val_label, pos_label=1, zero_division=0)       # bad=1
            
        # Always calculate all types for output filename (ç»Ÿä¸€æ ‡ç­¾å®šä¹‰ï¼šgood=0, bad=1)
        val_f1_good = metrics.f1_score(y_val, val_label, pos_label=0, zero_division=0)  # good=0
        val_f1_bad = metrics.f1_score(y_val, val_label, pos_label=1, zero_division=0)   # bad=1
        val_f1_macro = metrics.f1_score(y_val, val_label, average='macro', zero_division=0)
        val_f1_weighted = metrics.f1_score(y_val, val_label, average='weighted', zero_division=0)
        
        train_acc = metrics.accuracy_score(y_train, train_label)
        val_acc = metrics.accuracy_score(y_val, val_label)
    
    return {
        'model': model_fold,
        'scaler': scaler_fold,
        'train_f1': train_f1,
        'val_f1': val_f1,
        'val_f1_good': val_f1_good,
        'val_f1_bad': val_f1_bad,
        'val_f1_macro': val_f1_macro,
        'val_f1_weighted': val_f1_weighted,
        'train_acc': train_acc,
        'val_acc': val_acc,
        'overfitting': train_f1 - val_f1,
        'fold_id': fold_id
    }

# =====================================================
# ä¸»ç¨‹åº - Enhanced PyTorch Version
# =====================================================

def main(f1_type='bad'):
    print(f"=== ULTRA Multi-Strategy Ensemble with PyTorch Meta-ANN (F1 Type: {f1_type}) ===")
    print(f"f1_type :{f1_type} (å¯é€‰ï¼š'bad', 'macro', 'weighted')")
    # æ•°æ®åŠ è½½
    print("\n=== Loading Data ===")
    features_path = '/Users/mannormal/4011/Qi Zihan/v2/feature_extraction/result/features_cleaned_no_leakage1.csv'
    all_features_df = pd.read_csv(features_path)

    pwd = '/Users/mannormal/4011/Qi Zihan/original_data/'
    ta = pd.read_csv(pwd + 'train_acc.csv')
    te = pd.read_csv(pwd + 'test_acc_predict.csv')
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    print(f"è®­ç»ƒæ•°æ®åˆ—: {list(ta.columns)}")
    print(f"ç‰¹å¾æ•°æ®åˆ—: {list(all_features_df.columns)}")
    print(f"è®­ç»ƒæ•°æ®æ ·æœ¬: {ta.head()}")
    
    # æ£€æŸ¥flagåˆ—æ˜¯å¦å­˜åœ¨
    if 'flag' not in ta.columns:
        print("âŒ é”™è¯¯ï¼štrain_acc.csvä¸­æ²¡æœ‰flagåˆ—ï¼")
        return
    
    ta.loc[ta['flag'] == 0, 'flag'] = -1

    strategy_data = load_strategy_categories()
    
    # å¦‚æœall_features_dfä¸­å·²æœ‰flagåˆ—ï¼Œå…ˆåˆ é™¤é¿å…å†²çª
    cols_to_drop = []
    if 'flag' in all_features_df.columns:
        cols_to_drop.append('flag')
    if 'data_type' in all_features_df.columns:  # åŒæ—¶åˆ é™¤data_typeåˆ—
        cols_to_drop.append('data_type')
    
    if cols_to_drop:
        print(f"âš ï¸  ç‰¹å¾æ•°æ®ä¸­çš„ä»¥ä¸‹åˆ—å°†è¢«åˆ é™¤: {cols_to_drop}")
        all_features_df = all_features_df.drop(cols_to_drop, axis=1)
    
    training_df = pd.merge(all_features_df, ta[['account', 'flag']], on='account', how='inner')
    
    # å†æ¬¡æ£€æŸ¥åˆå¹¶ç»“æœ
    print(f"åˆå¹¶åçš„åˆ—: {list(training_df.columns)}")
    print(f"åˆå¹¶åæ˜¯å¦æœ‰flagåˆ—: {'flag' in training_df.columns}")
    
    if 'flag' not in training_df.columns:
        print("âŒ é”™è¯¯ï¼šåˆå¹¶åæ•°æ®ä¸­æ²¡æœ‰flagåˆ—ï¼")
        return

    training_df['account_type'] = training_df.apply(classify_account_type_improved, axis=1)

    print(f"Training data: {training_df.shape}")
    print(f"Account type distribution: {dict(training_df['account_type'].value_counts())}")
    print(f"Flag distribution: {dict(training_df['flag'].value_counts())}")
    
    # å‡†å¤‡åŸå§‹ç‰¹å¾
    feature_cols = [col for col in training_df.columns 
                   if col not in ['account', 'flag', 'account_type']]
    original_features = training_df[feature_cols].values
    y_true = np.where(training_df['flag'].values == -1, 0, 1)
    
    print(f"Original features shape: {original_features.shape}")
    print(f"Class distribution: {dict(zip(*np.unique(y_true, return_counts=True)))}")

    # =====================================================
    # Phase 1: å¢å¼ºéšæœºæ£®æ—é›†æˆ
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 1: Enhanced Random Forest Ensemble")
    print(f"{'='*80}")
    
    # ğŸ¯ ä½¿ç”¨ç­–ç•¥ç‰¹å®šè®­ç»ƒ (200ä¸ªåˆ†å¸ƒå¼æ¨¡å‹) - åŒæ—¶å‡†å¤‡æµ‹è¯•æ•°æ®ä»¥ç”Ÿæˆæµ‹è¯•é¢„æµ‹
    print(f"ğŸ¯ Training Mode: Strategy-Specific (200 distributed models)")
    
    # é¢„å…ˆå‡†å¤‡æµ‹è¯•æ•°æ®ä»¥ä¾¿åœ¨è®­ç»ƒæ—¶åŒæ­¥ç”Ÿæˆæµ‹è¯•é¢„æµ‹
    test_df = pd.merge(all_features_df, te[['account']], on='account', how='inner')
    test_df['account_type'] = test_df.apply(classify_account_type_improved, axis=1)
    
    rf_predictions, test_rf_predictions, rf_cv_scores, _ = train_strategy_specific_rf_ensemble(
        training_df, strategy_data, test_data=test_df
    )
    
    # =====================================================
    # Phase 2: ç­–ç•¥é›†æˆ (å·²åŒ…å«åœ¨Phase 1ä¸­)
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 2: Skipped (strategy-specific training completed in Phase 1)")
    print(f"{'='*80}")
    
    strategy_results = {}
    print(f"   Strategy-specific models already trained: {rf_predictions.shape[1] if len(rf_predictions.shape) > 1 else 0}")
    print(f"   Average CV F1: {np.mean(rf_cv_scores):.4f}")
    
    # åˆå¹¶æ‰€æœ‰é¢„æµ‹ (ç­–ç•¥ç‰¹å®šè®­ç»ƒå·²åŒ…å«æ‰€æœ‰æ¨¡å‹)
    print(f"\nğŸ“Š Combining Predictions:")
    print(f"   Strategy-specific predictions: {rf_predictions.shape}")
    
    combined_base_predictions = rf_predictions
    print(f"   ğŸ“Š Total models: {combined_base_predictions.shape[1]} (distributed across 6 strategies)")
    
    # =====================================================
    # Phase 3: PyTorch Meta-ANN Training (for reference only)
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 3: PyTorch Meta-ANN Training (Reference)")
    print(f"{'='*80}")
    
    # Train reference model (results not used for final predictions)
    _, _, _, _ = train_pytorch_meta_ann(
        base_predictions=combined_base_predictions,
        original_features=original_features,
        y_true=y_true,
        n_epochs=500,
        patience=30
    )
    
    # =====================================================
    # Phase 4: CVè®­ç»ƒå¹¶é€‰æ‹©æœ€ä½³æ¨¡å‹ - ä¿®æ”¹ç‚¹3ï¼šä¸»è¦ä¿®æ”¹
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 4: Cross-Validation with Early Stopping & Best Model Selection")
    print(f"{'='*80}")
    
    # 5æŠ˜äº¤å‰éªŒè¯è¯„ä¼°
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_results = []
    fold_models = []
    
    print("\nFold | Train F1 | Val F1   | Good F1  | Bad F1   | Train Acc| Val Acc  | Overfitting")
    print("-" * 80)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(combined_base_predictions, y_true)):
        # åˆ†å‰²æ•°æ®
        X_base_train = combined_base_predictions[train_idx]
        X_base_val = combined_base_predictions[val_idx]
        X_feat_train = original_features[train_idx]
        X_feat_val = original_features[val_idx]
        y_train_fold = y_true[train_idx]
        y_val_fold = y_true[val_idx]
        
        # è®­ç»ƒMeta-ANN with early stopping
        fold_result = train_single_fold_meta_ann(
            X_base_train, X_feat_train, y_train_fold, 
            X_base_val, X_feat_val, y_val_fold, 
            fold_id=fold, f1_type=f1_type, n_epochs=500, patience=30
        )
        
        fold_models.append(fold_result)
        cv_results.append(fold_result)
        
        overfitting = fold_result['overfitting']
        overfit_status = "ğŸ”´ High" if overfitting > 0.1 else "ğŸŸ¡ Med" if overfitting > 0.05 else "ğŸŸ¢ Low"
        
        print(f"{fold+1:4d} | {fold_result['train_f1']:8.4f} | {fold_result['val_f1']:8.4f} | {fold_result['val_f1_good']:8.4f} | {fold_result['val_f1_bad']:8.4f} | {fold_result['train_acc']:8.4f} | {fold_result['val_acc']:8.4f} | {overfit_status}")
    
    # é€‰æ‹©æœ€ä½³foldæ¨¡å‹ - ä¿®æ”¹ç‚¹4ï¼šé€‰æ‹©é€»è¾‘
    best_fold_idx = np.argmax([result['val_f1'] for result in cv_results])
    best_fold_model = fold_models[best_fold_idx]
    
    print(f"\nğŸ† Best Fold: {best_fold_idx + 1} (Val F1: {best_fold_model['val_f1']:.4f})")
    
    # CVç»Ÿè®¡
    avg_train_f1 = np.mean([r['train_f1'] for r in cv_results])
    avg_val_f1 = np.mean([r['val_f1'] for r in cv_results])
    avg_val_f1_good = np.mean([r['val_f1_good'] for r in cv_results])
    avg_val_f1_bad = np.mean([r['val_f1_bad'] for r in cv_results])
    # Remove unused variables
    # avg_val_f1_macro = np.mean([r['val_f1_macro'] for r in cv_results])  
    # avg_val_f1_weighted = np.mean([r['val_f1_weighted'] for r in cv_results])
    avg_train_acc = np.mean([r['train_acc'] for r in cv_results])
    avg_val_acc = np.mean([r['val_acc'] for r in cv_results])
    avg_overfitting = avg_train_f1 - avg_val_f1
    
    print("-" * 80)
    print(f"Avg  | {avg_train_f1:8.4f} | {avg_val_f1:8.4f} | {avg_val_f1_good:8.4f} | {avg_val_f1_bad:8.4f} | {avg_train_acc:8.4f} | {avg_val_acc:8.4f} | {avg_overfitting:+7.4f}")
    
    print(f"\nğŸ¤– Meta-ANN Performance (Using Best CV Fold):")
    print(f"   Best Fold Val F1: {best_fold_model['val_f1']:.4f}")
    print(f"   Average CV F1: {avg_val_f1:.4f}")
    print(f"   CV F1 std: {np.std([r['val_f1'] for r in cv_results]):.4f}")
    print(f"   Generalization Gap: {avg_overfitting:+.4f}")
    
    # =====================================================
    # æœ€ç»ˆç»“æœæ±‡æ€»
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ† FINAL RESULTS SUMMARY")
    print(f"{'='*80}")
    
    print(f"\nğŸ“Š Base Models Performance:")
    print(f"   Enhanced RF Ensemble: {np.mean(rf_cv_scores):.4f} F1")
    # Strategy results are integrated into the RF ensemble above
    if strategy_results:  # Only print if there are separate strategy results
        for strategy_name, results in strategy_results.items():
            print(f"   {strategy_name.capitalize()} Strategy: {results['avg_cv']:.4f} F1")
    
    print(f"\nğŸ¤– Meta-ANN Performance:")
    print(f"   Best CV Fold F1: {best_fold_model['val_f1']:.4f}")
    print(f"   Average CV F1: {avg_val_f1:.4f}")
    print(f"   Generalization Gap: {avg_overfitting:+.4f}")
    
    if avg_overfitting > 0.1:
        print("   ğŸ”´ HIGH overfitting - consider regularization")
    elif avg_overfitting > 0.05:
        print("   ğŸŸ¡ MEDIUM overfitting - monitor closely")
    else:
        print("   ğŸŸ¢ LOW overfitting - good generalization")
    
    print(f"\nğŸ¯ Model Architecture:")
    print(f"   Base models: {combined_base_predictions.shape[1]} (100 RF + {combined_base_predictions.shape[1]-100} Strategy)")
    print(f"   Original features: {original_features.shape[1]}")
    print(f"   Meta-ANN: ResNet-style with {combined_base_predictions.shape[1]+original_features.shape[1]} â†’ 128 (3 res blocks) â†’ 1")

    # =====================================================
    # Phase 5: æµ‹è¯•é›†é¢„æµ‹ & ç”Ÿæˆå¤šä¸ªæäº¤æ–‡ä»¶
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 5: Test Set Prediction & Multiple Submission Generation")
    print(f"{'='*80}")
    
    print(f"ğŸ“Š Test Data Info:")
    print(f"   Test accounts: {test_df.shape[0]}")
    print(f"   Test account type distribution: {dict(test_df['account_type'].value_counts())}")
    
    # =====================================================
    # æµ‹è¯•é›†RFé¢„æµ‹å·²åœ¨è®­ç»ƒé˜¶æ®µç”Ÿæˆ (æ— æ•°æ®æ³„æ¼)
    # =====================================================
    print(f"\nâœ… Test Set RF Predictions (generated during training, no data leakage)")
    
    # å‡†å¤‡æµ‹è¯•é›†ç‰¹å¾
    feature_cols = [col for col in test_df.columns if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]
    test_original_features = test_df[feature_cols].values
    
    # ä½¿ç”¨è®­ç»ƒæ—¶ç”Ÿæˆçš„æµ‹è¯•é¢„æµ‹
    test_combined_predictions = test_rf_predictions
    
    print(f"âœ… Test RF predictions shape: {test_combined_predictions.shape}")
    print(f"   ğŸ“Š Predictions generated during training phase - no redundant model training!")
    
    # =====================================================
    # ç”Ÿæˆå¤šä¸ªé¢„æµ‹æ–‡ä»¶ (ä½¿ç”¨ä¸åŒfoldæ¨¡å‹)
    # =====================================================
    print(f"\nğŸ”® Generating Multiple Test Predictions...")
    
    all_submissions = []
    
    # ä½¿ç”¨å‰3ä¸ªæœ€å¥½çš„foldæ¨¡å‹ç”Ÿæˆé¢„æµ‹
    sorted_folds = sorted(enumerate(cv_results), key=lambda x: x[1]['val_f1'], reverse=True)
    top_3_folds = sorted_folds[:3]
    
    print(f"   ğŸ† Using top 3 fold models:")
    for rank, (fold_idx, fold_result) in enumerate(top_3_folds, 1):
        print(f"   {rank}. Fold {fold_idx+1}: Val F1 = {fold_result['val_f1']:.4f}")
    
    for rank, (fold_idx, fold_result) in enumerate(top_3_folds, 1):
        print(f"\n   ğŸ”® Generating predictions with Fold {fold_idx+1} model (rank {rank})...")
        
        try:
            # ä½¿ç”¨è¯¥foldçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
            fold_model_data = fold_models[fold_idx]
            model = fold_model_data['model']
            scaler = fold_model_data['scaler']
            
            # å‡†å¤‡æµ‹è¯•æ•°æ® (åŒ¹é…Meta-ANNçš„åŒè¾“å…¥æ¶æ„)
            device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
            
            # 1. åŸºç¡€æ¨¡å‹é¢„æµ‹ (ä½¿ç”¨æ­£ç¡®ç”Ÿæˆçš„æµ‹è¯•é›†RFé¢„æµ‹)
            # ä¿®å¤æ•°æ®æ³„æ¼ï¼šä½¿ç”¨ä¸ºæµ‹è¯•é›†ä¸“é—¨ç”Ÿæˆçš„RFé¢„æµ‹
            test_base_features = test_combined_predictions
            
            # 2. åŸå§‹ç‰¹å¾ 
            test_original_features_scaled = scaler.transform(test_original_features)
            
            # 3. Meta-ANNé¢„æµ‹ (åŒè¾“å…¥)
            model.eval()
            with torch.no_grad():
                X_base_tensor = torch.FloatTensor(test_base_features).to(device)
                X_feat_tensor = torch.FloatTensor(test_original_features_scaled).to(device)
                
                # Meta-ANNéœ€è¦ä¸¤ä¸ªè¾“å…¥
                test_pred_proba = model(X_base_tensor, X_feat_tensor).cpu().numpy().flatten()
                test_pred_labels = (test_pred_proba > 0.5).astype(int)
            
            # åˆ›å»ºæäº¤æ–‡ä»¶
            submission_df = pd.DataFrame({
                'account': test_df['account'].values,
                'flag': test_pred_labels
            })
            
            # ç»Ÿè®¡ç»“æœ
            pred_counts = submission_df['flag'].value_counts()
            print(f"      Good (1): {pred_counts.get(1, 0)} ({pred_counts.get(1, 0)/len(submission_df)*100:.1f}%)")
            print(f"      Bad (0): {pred_counts.get(0, 0)} ({pred_counts.get(0, 0)/len(submission_df)*100:.1f}%)")
            
            # ç”Ÿæˆæ–‡ä»¶å
            fold_f1 = fold_result['val_f1']
            fold_f1_good = fold_result['val_f1_good'] 
            fold_f1_bad = fold_result['val_f1_bad']
            fold_f1_macro = fold_result['val_f1_macro']
            fold_f1_weighted = fold_result['val_f1_weighted']
            
            # ä¿æŒä¸ä¹‹å‰ä¸€è‡´çš„æ–‡ä»¶åæ ¼å¼ï¼Œæ·»åŠ rankå’Œfoldä¿¡æ¯
            random_seed = random.randint(100, 999)  # ç”Ÿæˆéšæœºæ•°
            filename = f"ultra_resnet_meta_ann_rank{rank}_fold{fold_idx+1}_{f1_type}_f1_{fold_f1:.4f}_good_{fold_f1_good:.4f}_bad_{fold_f1_bad:.4f}_macro_{fold_f1_macro:.4f}_weighted_{fold_f1_weighted:.4f}_seed_{random_seed}.csv"
            filepath = f"/Users/mannormal/4011/Qi Zihan/v2/results/{filename}"
            
            # ä¿å­˜æ–‡ä»¶
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            submission_df.to_csv(filepath, index=False)
            
            all_submissions.append({
                'rank': rank,
                'fold': fold_idx + 1,
                'val_f1': fold_f1,
                'filename': filename,
                'filepath': filepath,
                'submission_df': submission_df
            })
            
            print(f"      âœ… Saved: {filename}")
            
        except Exception as e:
            print(f"      âŒ Error generating prediction for Fold {fold_idx+1}: {str(e)}")
            print(f"      âš ï¸  Skipping this fold and continuing...")
            continue
    
    if not all_submissions:
        raise RuntimeError("âŒ Failed to generate any submission files!")
    
    print(f"\nğŸ¯ Generated {len(all_submissions)} submission files:")
    for sub in all_submissions:
        print(f"   Rank {sub['rank']}: {sub['filename']} (Val F1: {sub['val_f1']:.4f})")
    
    # è®¾ç½®æœ€ä½³æäº¤ä½œä¸ºä¸»è¦ç»“æœ
    best_submission = all_submissions[0]
    submission_df = best_submission['submission_df']
    filepath = best_submission['filepath']
    
    # =====================================================
    # è¿”å›ç»“æœæ±‡æ€»
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ TRAINING COMPLETED - RESULTS SUMMARY")
    print(f"{'='*80}")
    
    print(f"ğŸ“Š Final Results:")
    print(f"   Best Validation F1: {best_submission['val_f1']:.4f}")
    print(f"   Total Fold Models: {len(cv_results)}")
    print(f"   Generated Submissions: {len(all_submissions)}")
    print(f"   Primary Submission: {best_submission['filename']}")
    
    return {
        'cv_results': cv_results,
        'best_submission': best_submission,
        'all_submissions': all_submissions,
        'rf_cv_scores': rf_cv_scores,
        'rf_predictions': rf_predictions
    }
    
if __name__ == "__main__":
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='Ultra ResNet Meta-ANN Training')
    parser.add_argument('--f1_type', type=str, default='bad', 
                        choices=['bad', 'macro', 'weighted'],
                        help='F1 score type for model selection (default: bad)')
    
    args = parser.parse_args()
    
    results = main(f1_type="weighted")
    
    print(f"\n{'='*80}")
    print("âœ… Strategy-Specific Meta-ANN Training Complete!")
    print(f"ğŸ¯ RF Models Trained: {len(results['rf_cv_scores'])}")
    print(f"ğŸ¯ Average RF CV F1: {np.mean(results['rf_cv_scores']):.4f}")
    print(f"ğŸ¯ Best Submission: {results['best_submission']['filename']}")
    print(f"ğŸ“Š Best Val F1: {results['best_submission']['val_f1']:.4f}")
    print(f"ğŸ“Š Generated {len(results['all_submissions'])} submission files")
    print(f"{'='*80}")
    
    # æ˜¾ç¤ºæ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶
    print(f"\nğŸ‰ SUBMISSIONS READY!")
    for i, sub in enumerate(results['all_submissions'], 1):
        print(f"ğŸ“„ Rank {i}: {sub['filename']} (Val F1: {sub['val_f1']:.4f})")
    print(f"ğŸŒ± Seed: {seed_num}")