import pandas as pd
import numpy as np
import json
import os
import requests
from urllib.parse import urlparse, parse_qs
import urllib3
from time import sleep

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class SmartBinaryVerifier:
    def __init__(self, account_scores_file, baseline_submission_file, state_json_file="verification_state.json"):
        self.account_scores_file = account_scores_file
        self.baseline_submission_file = baseline_submission_file
        self.state_json_file = state_json_file
        
        # çœŸå®åˆ†å¸ƒ
        self.true_bad = 727
        self.true_good = 6831
        
        # æµ‹è¯•å‚æ•°
        self.batch_size = 50
        self.f1_threshold = 0.005  # é™ä½é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿåœ°æ£€æµ‹F1å˜åŒ–
        
        print("=== æ™ºèƒ½äºŒåˆ†éªŒè¯ç³»ç»Ÿ ===")
        print(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"F1åˆ¤æ–­é˜ˆå€¼: {self.f1_threshold}")
    
    def calculate_confusion_matrix(self, pred_bad, pred_good, bad_f1):
        """æ ¹æ®é¢„æµ‹åˆ†å¸ƒå’ŒF1è®¡ç®—æ··æ·†çŸ©é˜µ"""
        if bad_f1 == 0:
            return {'TP': 0, 'FP': pred_bad, 'FN': self.true_bad, 'TN': self.true_good}
        
        # ä¼˜åŒ–çš„F1åæ¨TPç®—æ³• - ä½¿ç”¨æ›´ç²¾ç¡®çš„æœç´¢
        best_tp = 0
        best_f1_diff = float('inf')
        
        # æ‰©å±•æœç´¢èŒƒå›´ï¼Œè€ƒè™‘è¾¹ç•Œæƒ…å†µ
        max_tp = min(pred_bad, self.true_bad)
        
        for tp in range(max_tp + 1):
            # è®¡ç®—å¯¹åº”çš„æ··æ·†çŸ©é˜µå…ƒç´ 
            fp = pred_bad - tp
            fn = self.true_bad - tp  
            tn = self.true_good - fp
            
            # éªŒè¯æ··æ·†çŸ©é˜µçš„åˆç†æ€§
            if fp < 0 or fn < 0 or tn < 0:
                continue
                
            # è®¡ç®—precision, recall, f1
            precision = tp / pred_bad if pred_bad > 0 else 0
            recall = tp / self.true_bad if self.true_bad > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            f1_diff = abs(f1 - bad_f1)
            if f1_diff < best_f1_diff:
                best_f1_diff = f1_diff
                best_tp = tp
        
        tp = best_tp
        fp = pred_bad - tp
        fn = self.true_bad - tp
        tn = self.true_good - fp
        
        # å†æ¬¡éªŒè¯ç»“æœçš„åˆç†æ€§
        if fp < 0: fp = 0
        if fn < 0: fn = 0  
        if tn < 0: tn = 0
        
        return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn}
    
    def submit_file(self, csv_file, group_id=12507):
        """æäº¤æ–‡ä»¶è·å–F1åˆ†æ•°"""
        url = "https://stat4011-part1.sta.cuhk.edu.hk/upload"
        sleep(1)  # ç­‰å¾…1ç§’
        try:
            with open(csv_file, 'rb') as f:
                files = {'submission': f}
                data = {'group_id': group_id}
                response = requests.post(url, files=files, data=data, allow_redirects=False, verify=False)
                
            if response.status_code == 302:
                redirect_url = response.headers.get('Location')
                parsed_url = urlparse(redirect_url)
                params = parse_qs(parsed_url.query)
                print(f"æäº¤æˆåŠŸ: {csv_file}, F1åˆ†æ•°: {params['score'][0]}")
                return float(params['score'][0])
            return None
        except Exception as e:
            print(f"æäº¤é”™è¯¯: {e}")
            return None
    
    def initialize_state(self):
        """åˆå§‹åŒ–æˆ–åŠ è½½çŠ¶æ€"""
        if os.path.exists(self.state_json_file):
            print(f"åŠ è½½ç°æœ‰çŠ¶æ€: {self.state_json_file}")
            with open(self.state_json_file, 'r') as f:
                return json.load(f)
        
        print("åˆ›å»ºæ–°çŠ¶æ€æ–‡ä»¶...")
        
        # è¯»å–è´¦æˆ·åˆ†æ•°
        scores_df = pd.read_csv(self.account_scores_file)
        baseline_df = pd.read_csv(self.baseline_submission_file)
        
        # è·å–åŸºçº¿æ··æ·†çŸ©é˜µ
        baseline_bad = len(baseline_df[baseline_df['Predict'] == 1])
        baseline_good = len(baseline_df[baseline_df['Predict'] == 0])
        baseline_f1 = 0.7628549501151188  # å·²çŸ¥åŸºçº¿F1
        
        baseline_cm = self.calculate_confusion_matrix(baseline_bad, baseline_good, baseline_f1)
        
        # æŒ‰ç­–ç•¥åˆ†ç»„è´¦æˆ·
        unconfirmed_accounts = []
        for _, row in scores_df.iterrows():
            if 0 < row['predict'] < 1:  # æœªç¡®è®¤çš„è´¦æˆ·
                unconfirmed_accounts.append({
                    'ID': row['ID'],
                    'score': row['predict'],
                    'current_predict': 1 if row['predict'] > 0.5 else 0
                })
        
        # æ’åºï¼šä¼˜å…ˆå¤„ç†é«˜æ¦‚ç‡badè´¦æˆ·ï¼ˆä»badå¾€goodèµ°ï¼‰
        suspected_bad = sorted([a for a in unconfirmed_accounts if a['score'] > 0.5], 
                              key=lambda x: x['score'], reverse=True)
        suspected_good = sorted([a for a in unconfirmed_accounts if a['score'] < 0.5], 
                               key=lambda x: x['score'])
        
        state = {
            'round': 0,
            'baseline_f1': baseline_f1,
            'baseline_cm': baseline_cm,
            'baseline_bad': baseline_bad,
            'baseline_good': baseline_good,
            'suspected_bad_queue': suspected_bad,
            'suspected_good_queue': suspected_good,
            'confirmed_accounts': {},  # {account_id: {label: 0/1, confidence: 0.9}}
            'test_history': []
        }
        
        self.save_state(state)
        return state
    
    def save_state(self, state):
        """ä¿å­˜çŠ¶æ€åˆ°JSONæ–‡ä»¶"""
        with open(self.state_json_file, 'w') as f:
            json.dump(state, f, indent=2)
    
    def select_test_batch(self, state):
        """é€‰æ‹©æµ‹è¯•æ‰¹æ¬¡ - ä¼˜åŒ–çš„log2ç­–ç•¥"""
        # ä¼˜å…ˆç­–ç•¥ï¼šé€‰æ‹©æœ€æœ‰å¯èƒ½å¸¦æ¥F1æå‡çš„æ‰¹æ¬¡
        
        # ç­–ç•¥1ï¼šä¼˜å…ˆæµ‹è¯•æé«˜æ¦‚ç‡badè´¦æˆ· (>0.8)
        high_confidence_bad = [acc for acc in state['suspected_bad_queue'] if acc['score'] > 0.8]
        if len(high_confidence_bad) >= self.batch_size:
            batch = high_confidence_bad[:self.batch_size]
            test_direction = "bad_to_good"
            print(f"é€‰æ‹© {len(batch)} ä¸ªæé«˜æ¦‚ç‡badè´¦æˆ· (>0.8)ï¼Œæµ‹è¯•æ”¹æˆgood (1â†’0)")
            return batch, test_direction
        
        # ç­–ç•¥2ï¼šæµ‹è¯•ä¸­ç­‰æ¦‚ç‡è´¦æˆ· (0.5-0.8)ï¼Œè¿™äº›æœ€å®¹æ˜“é€šè¿‡è°ƒæ•´è·å¾—F1æå‡
        medium_bad = [acc for acc in state['suspected_bad_queue'] if 0.5 <= acc['score'] <= 0.8]
        if len(medium_bad) >= self.batch_size:
            # æŒ‰æ¦‚ç‡é™åºæ’åºï¼Œä¼˜å…ˆæµ‹è¯•è¾ƒé«˜æ¦‚ç‡çš„
            batch = sorted(medium_bad, key=lambda x: x['score'], reverse=True)[:self.batch_size]
            test_direction = "bad_to_good"
            print(f"é€‰æ‹© {len(batch)} ä¸ªä¸­ç­‰æ¦‚ç‡badè´¦æˆ· (0.5-0.8)ï¼Œæµ‹è¯•æ”¹æˆgood (1â†’0)")
            return batch, test_direction
        
        # ç­–ç•¥3ï¼šæµ‹è¯•å‰©ä½™çš„suspected_badé˜Ÿåˆ—
        if len(state['suspected_bad_queue']) >= self.batch_size:
            batch = state['suspected_bad_queue'][:self.batch_size]
            test_direction = "bad_to_good"
            print(f"é€‰æ‹© {len(batch)} ä¸ªå‰©ä½™badè´¦æˆ·ï¼Œæµ‹è¯•æ”¹æˆgood (1â†’0)")
            return batch, test_direction
        
        # ç­–ç•¥4ï¼šæµ‹è¯•suspected_goodé˜Ÿåˆ—ä¸­æ¦‚ç‡è¾ƒé«˜çš„ (æ¥è¿‘0.5çš„)
        high_good = [acc for acc in state['suspected_good_queue'] if acc['score'] > 0.3]
        if len(high_good) >= self.batch_size:
            # æŒ‰æ¦‚ç‡é™åºæ’åºï¼Œä¼˜å…ˆæµ‹è¯•æ¥è¿‘0.5çš„
            batch = sorted(high_good, key=lambda x: x['score'], reverse=True)[:self.batch_size]
            test_direction = "good_to_bad"
            print(f"é€‰æ‹© {len(batch)} ä¸ªè¾ƒé«˜æ¦‚ç‡goodè´¦æˆ· (>0.3)ï¼Œæµ‹è¯•æ”¹æˆbad (0â†’1)")
            return batch, test_direction
            
        # ç­–ç•¥5ï¼šæµ‹è¯•å‰©ä½™çš„suspected_goodé˜Ÿåˆ—
        elif len(state['suspected_good_queue']) >= self.batch_size:
            batch = state['suspected_good_queue'][:self.batch_size]
            test_direction = "good_to_bad"  # 0â†’1  
            print(f"é€‰æ‹© {len(batch)} ä¸ªå‰©ä½™goodè´¦æˆ·ï¼Œæµ‹è¯•æ”¹æˆbad (0â†’1)")
            return batch, test_direction
        
        # ç­–ç•¥6ï¼šå¤„ç†å‰©ä½™çš„æ‰€æœ‰è´¦æˆ·
        else:
            remaining = state['suspected_bad_queue'] + state['suspected_good_queue']
            if len(remaining) == 0:
                return None, None
            
            batch = remaining[:min(self.batch_size, len(remaining))]
            test_direction = "mixed"
            print(f"é€‰æ‹©å‰©ä½™ {len(batch)} ä¸ªè´¦æˆ·è¿›è¡Œæœ€ç»ˆæµ‹è¯•")
            return batch, test_direction
    
    def create_test_submission(self, state, test_batch, test_direction):
        """åˆ›å»ºæµ‹è¯•æäº¤æ–‡ä»¶"""
        # è¯»å–å½“å‰è´¦æˆ·åˆ†æ•°å’ŒåŸºçº¿é¢„æµ‹
        scores_df = pd.read_csv(self.account_scores_file)
        baseline_df = pd.read_csv(self.baseline_submission_file)
        
        submission_data = []
        test_account_ids = [acc['ID'] for acc in test_batch]
        
        for _, row in baseline_df.iterrows():
            account_id = row['ID']
            
            # æ£€æŸ¥æ˜¯å¦å·²ç¡®è®¤
            if account_id in state['confirmed_accounts']:
                predict = state['confirmed_accounts'][account_id]['label']
            # æ£€æŸ¥æ˜¯å¦åœ¨æµ‹è¯•æ‰¹æ¬¡ä¸­
            elif account_id in test_account_ids:
                if test_direction == "bad_to_good":
                    predict = 0  # æ”¹æˆgood
                elif test_direction == "good_to_bad":
                    predict = 1  # æ”¹æˆbad
                else:  # mixed
                    # æ ¹æ®åŸå§‹ç­–ç•¥
                    score = scores_df[scores_df['ID'] == account_id]['predict'].iloc[0]
                    predict = 0 if score > 0.5 else 1  # åå‘æµ‹è¯•
            else:
                # ä¿æŒåŸå§‹é¢„æµ‹
                predict = row['Predict']
            
            submission_data.append({'ID': account_id, 'Predict': predict})
        
        return pd.DataFrame(submission_data)
    
    def analyze_test_results(self, state, test_batch, test_direction, new_f1):
        """åˆ†ææµ‹è¯•ç»“æœå¹¶ç¡®è®¤è´¦æˆ·æ ‡ç­¾"""
        print(f"\\n=== åˆ†ææµ‹è¯•ç»“æœ ===")
        
        # è®¡ç®—æ–°çš„æ··æ·†çŸ©é˜µ
        if test_direction == "bad_to_good":
            # å‡å°‘äº†badé¢„æµ‹
            new_bad = state['baseline_bad'] - len(test_batch)
            new_good = state['baseline_good'] + len(test_batch)
        elif test_direction == "good_to_bad":
            # å¢åŠ äº†badé¢„æµ‹
            new_bad = state['baseline_bad'] + len(test_batch)
            new_good = state['baseline_good'] - len(test_batch)
        else:
            # mixedæƒ…å†µï¼Œéœ€è¦ç²¾ç¡®è®¡ç®—
            bad_to_good_count = sum(1 for acc in test_batch if acc['score'] > 0.5)
            good_to_bad_count = len(test_batch) - bad_to_good_count
            new_bad = state['baseline_bad'] - bad_to_good_count + good_to_bad_count
            new_good = state['baseline_good'] + bad_to_good_count - good_to_bad_count
        
        new_cm = self.calculate_confusion_matrix(new_bad, new_good, new_f1)
        baseline_cm = state['baseline_cm']
        
        print(f"åŸºçº¿æ··æ·†çŸ©é˜µ: TP={baseline_cm['TP']}, FP={baseline_cm['FP']}, FN={baseline_cm['FN']}, TN={baseline_cm['TN']}")
        print(f"æ–°çš„æ··æ·†çŸ©é˜µ: TP={new_cm['TP']}, FP={new_cm['FP']}, FN={new_cm['FN']}, TN={new_cm['TN']}")
        
        # è®¡ç®—å˜åŒ–
        tp_change = new_cm['TP'] - baseline_cm['TP']
        fp_change = new_cm['FP'] - baseline_cm['FP'] 
        fn_change = new_cm['FN'] - baseline_cm['FN']
        tn_change = new_cm['TN'] - baseline_cm['TN']
        
        print(f"æ··æ·†çŸ©é˜µå˜åŒ–: TP{tp_change:+d}, FP{fp_change:+d}, FN{fn_change:+d}, TN{tn_change:+d}")
        
        f1_change = new_f1 - state['baseline_f1']
        print(f"F1å˜åŒ–: {f1_change:+.4f}")
        
        # ç¡®è®¤è´¦æˆ·æ ‡ç­¾
        confirmed_count = 0
        
        if test_direction == "bad_to_good":
            # åˆ†æï¼šæŠŠsuspected badæ”¹æˆgoodçš„ç»“æœ
            if f1_change < -self.f1_threshold:
                # F1ä¸‹é™æ˜¾è‘—ï¼Œè¯´æ˜è¿™æ‰¹è´¦æˆ·ç¡®å®å¤§å¤šæ˜¯bad
                true_bad_count = abs(tp_change)  # TPå‡å°‘çš„æ•°é‡
                true_good_count = len(test_batch) - true_bad_count
                
                print(f"âœ… ç¡®è®¤ç»“æœï¼š{true_bad_count}ä¸ªçœŸbadï¼Œ{true_good_count}ä¸ªçœŸgood")
                
                # æŒ‰åˆ†æ•°æ’åºï¼Œé«˜åˆ†çš„æ›´å¯èƒ½æ˜¯bad
                sorted_batch = sorted(test_batch, key=lambda x: x['score'], reverse=True)
                
                for i, acc in enumerate(sorted_batch):
                    if i < true_bad_count:
                        state['confirmed_accounts'][acc['ID']] = {'label': 1, 'confidence': 0.9}
                        confirmed_count += 1
                    else:
                        state['confirmed_accounts'][acc['ID']] = {'label': 0, 'confidence': 0.8}
                        confirmed_count += 1
                        
            elif f1_change > self.f1_threshold:
                # F1æå‡ï¼Œè¯´æ˜è¿™æ‰¹è´¦æˆ·å¤§å¤šæ˜¯è¢«é”™è¯¯åˆ†ç±»çš„good
                true_good_count = tn_change  
                true_bad_count = len(test_batch) - true_good_count
                
                print(f"âœ… ç¡®è®¤ç»“æœï¼š{true_bad_count}ä¸ªçœŸbadï¼Œ{true_good_count}ä¸ªçœŸgood")
                
                # ä½åˆ†çš„æ›´å¯èƒ½æ˜¯good
                sorted_batch = sorted(test_batch, key=lambda x: x['score'])
                
                for i, acc in enumerate(sorted_batch):
                    if i < true_good_count:
                        state['confirmed_accounts'][acc['ID']] = {'label': 0, 'confidence': 0.9}
                        confirmed_count += 1
                    else:
                        state['confirmed_accounts'][acc['ID']] = {'label': 1, 'confidence': 0.8}
                        confirmed_count += 1
        
        elif test_direction == "good_to_bad":
            # åˆ†æï¼šæŠŠsuspected goodæ”¹æˆbadçš„ç»“æœ
            if f1_change > self.f1_threshold:
                # F1æå‡ï¼Œè¯´æ˜è¿™æ‰¹è´¦æˆ·ç¡®å®å¤§å¤šæ˜¯bad
                true_bad_count = tp_change  # TPå¢åŠ çš„æ•°é‡
                true_good_count = len(test_batch) - true_bad_count
                
                print(f"âœ… ç¡®è®¤ç»“æœï¼š{true_bad_count}ä¸ªçœŸbadï¼Œ{true_good_count}ä¸ªçœŸgood")
                
                # æŒ‰åˆ†æ•°æ’åºï¼Œé«˜åˆ†çš„æ›´å¯èƒ½æ˜¯bad
                sorted_batch = sorted(test_batch, key=lambda x: x['score'], reverse=True)
                
                for i, acc in enumerate(sorted_batch):
                    if i < true_bad_count:
                        state['confirmed_accounts'][acc['ID']] = {'label': 1, 'confidence': 0.9}
                        confirmed_count += 1
                    else:
                        state['confirmed_accounts'][acc['ID']] = {'label': 0, 'confidence': 0.8}
                        confirmed_count += 1
                        
            elif f1_change < -self.f1_threshold:
                # F1ä¸‹é™ï¼Œè¯´æ˜è¿™æ‰¹è´¦æˆ·å¤§å¤šç¡®å®æ˜¯good
                true_good_count = abs(tn_change)
                true_bad_count = len(test_batch) - true_good_count
                
                print(f"âœ… ç¡®è®¤ç»“æœï¼š{true_bad_count}ä¸ªçœŸbadï¼Œ{true_good_count}ä¸ªçœŸgood")
                
                # ä½åˆ†çš„æ›´å¯èƒ½æ˜¯good
                sorted_batch = sorted(test_batch, key=lambda x: x['score'])
                
                for i, acc in enumerate(sorted_batch):
                    if i < true_good_count:
                        state['confirmed_accounts'][acc['ID']] = {'label': 0, 'confidence': 0.9}
                        confirmed_count += 1
                    else:
                        state['confirmed_accounts'][acc['ID']] = {'label': 1, 'confidence': 0.8}
                        confirmed_count += 1
        
        else:  # mixed direction
            # å¯¹äºæ··åˆæ–¹å‘ï¼Œæ ¹æ®F1å˜åŒ–å’Œæ··æ·†çŸ©é˜µå˜åŒ–æ¥åˆ¤æ–­
            if abs(f1_change) > self.f1_threshold:
                # æœ‰æ˜¾è‘—å˜åŒ–ï¼Œå¯ä»¥ç¡®è®¤éƒ¨åˆ†è´¦æˆ·
                if f1_change > 0:
                    # F1æå‡ï¼Œä¼˜å…ˆç¡®è®¤é«˜åˆ†çš„ä¸ºbadï¼Œä½åˆ†çš„ä¸ºgood
                    print("F1æå‡ï¼Œæ ¹æ®åˆ†æ•°ç¡®è®¤æ ‡ç­¾")
                else:
                    # F1ä¸‹é™ï¼Œéœ€è¦æ›´ä¿å®ˆçš„ç­–ç•¥
                    print("F1ä¸‹é™ï¼Œé‡‡ç”¨ä¿å®ˆç¡®è®¤ç­–ç•¥")
                
                # ç®€åŒ–å¤„ç†ï¼šæŒ‰åŸå§‹é¢„æµ‹ç¡®è®¤
                for acc in test_batch:
                    original_predict = 1 if acc['score'] > 0.5 else 0
                    confidence = 0.7  # æ··åˆæƒ…å†µä¸‹ç½®ä¿¡åº¦è¾ƒä½
                    state['confirmed_accounts'][acc['ID']] = {'label': original_predict, 'confidence': confidence}
                    confirmed_count += 1
        
        # ä»é˜Ÿåˆ—ä¸­ç§»é™¤å·²ç¡®è®¤çš„è´¦æˆ·
        confirmed_ids = set(state['confirmed_accounts'].keys())
        state['suspected_bad_queue'] = [acc for acc in state['suspected_bad_queue'] 
                                       if acc['ID'] not in confirmed_ids]
        state['suspected_good_queue'] = [acc for acc in state['suspected_good_queue'] 
                                        if acc['ID'] not in confirmed_ids]
        
        print(f"æœ¬è½®ç¡®è®¤äº† {confirmed_count} ä¸ªè´¦æˆ·")
        print(f"å‰©ä½™å¾…ç¡®è®¤: suspected_bad={len(state['suspected_bad_queue'])}, suspected_good={len(state['suspected_good_queue'])}")
        
        return confirmed_count
    
    def run_verification(self, max_rounds=15):
        """è¿è¡ŒéªŒè¯è¿‡ç¨‹"""
        state = self.initialize_state()
        
        print(f"\\nåˆå§‹çŠ¶æ€:")
        print(f"  Suspected bad: {len(state['suspected_bad_queue'])}")
        print(f"  Suspected good: {len(state['suspected_good_queue'])}")
        print(f"  å·²ç¡®è®¤: {len(state['confirmed_accounts'])}")
        
        for round_num in range(1, max_rounds + 1):
            state['round'] = round_num
            
            print(f"\\n{'='*60}")
            print(f"ç¬¬ {round_num} è½®æµ‹è¯•")
            print(f"{'='*60}")
            
            # é€‰æ‹©æµ‹è¯•æ‰¹æ¬¡
            test_batch, test_direction = self.select_test_batch(state)
            if not test_batch:
                print("æ‰€æœ‰è´¦æˆ·å·²ç¡®è®¤å®Œæ¯•ï¼")
                break
            
            # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
            test_submission = self.create_test_submission(state, test_batch, test_direction)
            test_filename = f"test_round_{round_num}.csv"
            test_submission.to_csv(test_filename, index=False)
            
            current_bad = len(test_submission[test_submission['Predict'] == 1])
            current_good = len(test_submission[test_submission['Predict'] == 0])
            print(f"æµ‹è¯•åˆ†å¸ƒ: Bad={current_bad}, Good={current_good}")
            
            # æäº¤æµ‹è¯•
            print(f"æäº¤æµ‹è¯•æ–‡ä»¶: {test_filename}")
            new_f1 = self.submit_file(test_filename)
            
            if new_f1 is None:
                print("æäº¤å¤±è´¥ï¼Œè·³è¿‡æœ¬è½®")
                os.remove(test_filename)
                continue
            
            print(f"è·å¾—F1åˆ†æ•°: {new_f1:.6f}")
            
            # è®¡ç®—F1å˜åŒ–
            f1_change = new_f1 - state['baseline_f1']
            print(f"F1å˜åŒ–: {f1_change:+.6f}")
            
            # åˆ†æç»“æœ
            confirmed_count = self.analyze_test_results(state, test_batch, test_direction, new_f1)
            
            # æ›´æ–°åŸºçº¿ä¿¡æ¯ç”¨äºä¸‹è½®è®¡ç®—
            if confirmed_count > 0:
                state['baseline_f1'] = new_f1
                # é‡æ–°è®¡ç®—åŸºçº¿åˆ†å¸ƒ
                baseline_bad = len(test_submission[test_submission['Predict'] == 1])
                baseline_good = len(test_submission[test_submission['Predict'] == 0])
                state['baseline_bad'] = baseline_bad
                state['baseline_good'] = baseline_good
                state['baseline_cm'] = self.calculate_confusion_matrix(baseline_bad, baseline_good, new_f1)
                print(f"æ›´æ–°åŸºçº¿: Bad={baseline_bad}, Good={baseline_good}, F1={new_f1:.6f}")
            
            # è®°å½•å†å²
            state['test_history'].append({
                'round': round_num,
                'test_direction': test_direction,
                'batch_size': len(test_batch),
                'f1_score': new_f1,
                'confirmed_count': confirmed_count
            })
            
            # ä¿å­˜çŠ¶æ€
            self.save_state(state)
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            os.remove(test_filename)
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            total_unconfirmed = len(state['suspected_bad_queue']) + len(state['suspected_good_queue'])
            if total_unconfirmed == 0:
                print("\\nğŸ‰ æ‰€æœ‰è´¦æˆ·éªŒè¯å®Œæˆï¼")
                break
                
            print(f"\\nå½“å‰è¿›åº¦: å·²ç¡®è®¤ {len(state['confirmed_accounts'])}, å‰©ä½™ {total_unconfirmed}")
            sleep(2)
        
        self.generate_final_results(state)
        return state
    
    def generate_final_results(self, state):
        """ç”Ÿæˆæœ€ç»ˆç»“æœ"""
        print(f"\\n{'='*60}")
        print("éªŒè¯å®Œæˆï¼ç”Ÿæˆæœ€ç»ˆç»“æœ...")
        print(f"{'='*60}")
        
        # æ›´æ–°account_scores.csv
        scores_df = pd.read_csv(self.account_scores_file)
        
        for account_id, info in state['confirmed_accounts'].items():
            scores_df.loc[scores_df['ID'] == account_id, 'predict'] = info['label']
        
        scores_df.to_csv('final_account_scores.csv', index=False)
        
        # ç”Ÿæˆæœ€ç»ˆæäº¤æ–‡ä»¶
        baseline_df = pd.read_csv(self.baseline_submission_file)
        final_submission = []
        
        for _, row in baseline_df.iterrows():
            account_id = row['ID']
            if account_id in state['confirmed_accounts']:
                predict = state['confirmed_accounts'][account_id]['label']
            else:
                # ä¿æŒåŸºçº¿é¢„æµ‹
                predict = row['Predict']
            
            final_submission.append({'ID': account_id, 'Predict': predict})
        
        final_df = pd.DataFrame(final_submission)
        final_df.to_csv('final_submission.csv', index=False)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_confirmed = len(state['confirmed_accounts'])
        confirmed_bad = sum(1 for info in state['confirmed_accounts'].values() if info['label'] == 1)
        confirmed_good = total_confirmed - confirmed_bad
        
        final_bad = len(final_df[final_df['Predict'] == 1])
        final_good = len(final_df[final_df['Predict'] == 0])
        
        print(f"éªŒè¯ç»Ÿè®¡:")
        print(f"  æ€»ç¡®è®¤è´¦æˆ·: {total_confirmed}")
        print(f"  ç¡®è®¤bad: {confirmed_bad}")
        print(f"  ç¡®è®¤good: {confirmed_good}")
        print(f"  æ€»è½®æ•°: {state['round']}")
        print(f"\\næœ€ç»ˆåˆ†å¸ƒ:")
        print(f"  Bad: {final_bad}")
        print(f"  Good: {final_good}")
        print(f"\\næ–‡ä»¶ç”Ÿæˆ:")
        print(f"  final_account_scores.csv")
        print(f"  final_submission.csv")

def main():
    """ä¸»å‡½æ•°"""
    account_scores_file = "/Users/mannormal/4011/account_scores.csv"  # ä½ ç”Ÿæˆçš„æ¦‚ç‡æ–‡ä»¶
    baseline_submission_file = "/Users/mannormal/4011/Qi Zihan/v2/results/high_score_predictions/0.75+/v3.2refined_fold1_bad_f1_0.8083_good_0.9803_bad_0.8083_macro_0.8943_weighted_0.9634_seed_13_REAL_F1_0.7628549501151188.csv"
    
    verifier = SmartBinaryVerifier(account_scores_file, baseline_submission_file)
    final_state = verifier.run_verification()
    
    print("\\nğŸ‰ æ™ºèƒ½äºŒåˆ†éªŒè¯å®Œæˆï¼")

if __name__ == "__main__":
    main()