import pandas as pd
import json
import os
import requests
from urllib.parse import urlparse, parse_qs
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class CorrectBinarySearchOptimizer:
    def __init__(self, account_scores_file, state_json_file="correct_binary_state.json"):
        self.account_scores_file = account_scores_file
        self.state_json_file = state_json_file
        
        # çœŸå®åˆ†å¸ƒï¼ˆä¼°è®¡å€¼ï¼‰
        self.true_bad = 727
        self.true_good = 6831
        
        print("=== æ­£ç¡®äºŒåˆ†æ³•F1ä¼˜åŒ–ç³»ç»Ÿ ===")
        print("ç­–ç•¥: åŸºäºTP/FP/FN/TNå˜åŒ–è®¡ç®—è½¬æ¢æˆåŠŸç‡çš„çœŸæ­£äºŒåˆ†æ³•")
    
    def submit_file(self, csv_file, group_id=12507):
        """æäº¤æ–‡ä»¶è·å–F1åˆ†æ•°"""
        url = "https://stat4011-part1.sta.cuhk.edu.hk/upload"
        try:
            with open(csv_file, 'rb') as f:
                files = {'submission': f}
                data = {'group_id': group_id}
                response = requests.post(url, files=files, data=data, allow_redirects=False, verify=False)
                
            if response.status_code == 302:
                redirect_url = response.headers.get('Location')
                parsed_url = urlparse(redirect_url)
                params = parse_qs(parsed_url.query)
                return float(params['score'][0])
            return None
        except Exception as e:
            print(f"æäº¤é”™è¯¯: {e}")
            return None
    
    def calculate_confusion_matrix(self, pred_bad_count, pred_good_count, f1_score):
        """æ ¹æ®é¢„æµ‹åˆ†å¸ƒå’ŒF1åˆ†æ•°è®¡ç®—æ··æ·†çŸ©é˜µ"""
        if f1_score == 0:
            return {'TP': 0, 'FP': pred_bad_count, 'FN': self.true_bad, 'TN': self.true_good}
        
        # é€šè¿‡F1åæ¨TP
        best_tp = 0
        best_f1_diff = float('inf')
        
        for tp in range(min(pred_bad_count, self.true_bad) + 1):
            fp = pred_bad_count - tp
            fn = self.true_bad - tp
            tn = self.true_good - fp
            
            # éªŒè¯åˆç†æ€§
            if fp < 0 or fn < 0 or tn < 0:
                continue
                
            precision = tp / pred_bad_count if pred_bad_count > 0 else 0
            recall = tp / self.true_bad if self.true_bad > 0 else 0
            calculated_f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            f1_diff = abs(calculated_f1 - f1_score)
            if f1_diff < best_f1_diff:
                best_f1_diff = f1_diff
                best_tp = tp
        
        tp = best_tp
        fp = pred_bad_count - tp
        fn = self.true_bad - tp
        tn = self.true_good - fp
        
        return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn}
    
    def initialize_state(self):
        """åˆå§‹åŒ–çŠ¶æ€"""
        if os.path.exists(self.state_json_file):
            print(f"åŠ è½½ç°æœ‰çŠ¶æ€: {self.state_json_file}")
            with open(self.state_json_file, 'r') as f:
                return json.load(f)
        
        print("åˆ›å»ºæ–°çŠ¶æ€ï¼Œä»å…¨0å¼€å§‹...")
        
        # è¯»å–è´¦æˆ·åˆ†æ•°
        scores_df = pd.read_csv(self.account_scores_file)
        all_accounts = []
        for _, row in scores_df.iterrows():
            all_accounts.append({
                'ID': row['ID'],
                'score': row['predict'],
                'current_predict': 0  # åˆå§‹å…¨éƒ¨ä¸º0 (good)
            })
        
        # æŒ‰æ¦‚ç‡é™åºæ’åº
        all_accounts.sort(key=lambda x: x['score'], reverse=True)
        
        # æäº¤åˆå§‹å…¨0æ–‡ä»¶è·å–åŸºçº¿
        baseline_df = pd.DataFrame([{'ID': acc['ID'], 'Predict': 0} for acc in all_accounts])
        baseline_df.to_csv('baseline_all_zero.csv', index=False)
        baseline_f1 = self.submit_file('baseline_all_zero.csv')
        os.remove('baseline_all_zero.csv')
        
        if baseline_f1 is None:
            baseline_f1 = 0.0
        
        baseline_cm = self.calculate_confusion_matrix(0, len(all_accounts), baseline_f1)
        
        state = {
            'round': 0,
            'all_accounts': all_accounts,
            'current_f1': baseline_f1,
            'best_f1': baseline_f1,
            'current_cm': baseline_cm,
            'test_queue': [],  # å½“å‰è¦æµ‹è¯•çš„è´¦æˆ·é˜Ÿåˆ—
            'confirmed_predictions': {},  # {account_id: 0/1} å·²ç¡®è®¤çš„é¢„æµ‹
            'test_history': []
        }
        
        print(f"åˆå§‹åŸºçº¿F1: {baseline_f1:.6f}")
        print(f"åŸºçº¿æ··æ·†çŸ©é˜µ: TP={baseline_cm['TP']}, FP={baseline_cm['FP']}, FN={baseline_cm['FN']}, TN={baseline_cm['TN']}")
        
        self.save_state(state)
        return state
    
    def save_state(self, state):
        """ä¿å­˜çŠ¶æ€"""
        with open(self.state_json_file, 'w') as f:
            json.dump(state, f, indent=2)
    
    def save_best_submission(self, state, f1_score):
        """ä¿å­˜F1æ–°é«˜æ—¶çš„æœ€ä½³æäº¤æ–‡ä»¶"""
        submission_data = []
        for acc in state['all_accounts']:
            if acc['ID'] in state['confirmed_predictions']:
                predict = state['confirmed_predictions'][acc['ID']]
            else:
                predict = acc['current_predict']
            submission_data.append({'ID': acc['ID'], 'Predict': predict})
        
        df = pd.DataFrame(submission_data)
        filename = f"best_correct_f1_{f1_score:.6f}_round_{state['round']}.csv"
        df.to_csv(filename, index=False)
        print(f"ğŸ¯ F1æ–°é«˜ï¼ä¿å­˜æ–‡ä»¶: {filename}")
    
    def select_next_test_batch(self, state):
        """é€‰æ‹©ä¸‹ä¸€æ‰¹è¦æµ‹è¯•çš„è´¦æˆ·"""
        if len(state['test_queue']) == 0:
            # åˆå§‹åŒ–ï¼šé€‰æ‹©æ‰€æœ‰é«˜æ¦‚ç‡è´¦æˆ·ä½œä¸ºå€™é€‰
            candidates = []
            for acc in state['all_accounts']:
                if acc['ID'] not in state['confirmed_predictions'] and acc['score'] > 0.5:
                    candidates.append(acc)
            
            if len(candidates) == 0:
                # å¦‚æœæ²¡æœ‰é«˜æ¦‚ç‡å€™é€‰ï¼Œé€‰æ‹©æ‰€æœ‰æœªç¡®è®¤è´¦æˆ·
                candidates = [acc for acc in state['all_accounts'] 
                            if acc['ID'] not in state['confirmed_predictions']]
            
            if len(candidates) == 0:
                return None, None
                
            state['test_queue'] = candidates
            print(f"åˆå§‹åŒ–æµ‹è¯•é˜Ÿåˆ—: {len(candidates)}ä¸ªè´¦æˆ·")
        
        # é€‰æ‹©æµ‹è¯•æ‰¹æ¬¡ï¼ˆäºŒåˆ†ï¼‰
        batch_size = max(1, len(state['test_queue']) // 2)
        test_batch = state['test_queue'][:batch_size]
        
        # å†³å®šæµ‹è¯•æ–¹å‘ï¼š0â†’1 è¿˜æ˜¯ 1â†’0
        if test_batch[0]['current_predict'] == 0:
            test_direction = "0_to_1"  # good â†’ bad
            print(f"æµ‹è¯• {len(test_batch)} ä¸ªè´¦æˆ·ï¼š0â†’1 (goodæ”¹ä¸ºbad)")
        else:
            test_direction = "1_to_0"  # bad â†’ good  
            print(f"æµ‹è¯• {len(test_batch)} ä¸ªè´¦æˆ·ï¼š1â†’0 (badæ”¹ä¸ºgood)")
        
        return test_batch, test_direction
    
    def create_test_submission(self, state, test_batch, test_direction):
        """åˆ›å»ºæµ‹è¯•æäº¤æ–‡ä»¶"""
        submission_data = []
        test_account_ids = [acc['ID'] for acc in test_batch]
        
        for acc in state['all_accounts']:
            if acc['ID'] in state['confirmed_predictions']:
                # å·²ç¡®è®¤çš„é¢„æµ‹
                predict = state['confirmed_predictions'][acc['ID']]
            elif acc['ID'] in test_account_ids:
                # æµ‹è¯•æ‰¹æ¬¡ï¼šè¿›è¡Œè½¬æ¢
                if test_direction == "0_to_1":
                    predict = 1  # good â†’ bad
                else:
                    predict = 0  # bad â†’ good
            else:
                # å…¶ä»–è´¦æˆ·ä¿æŒå½“å‰é¢„æµ‹
                predict = acc['current_predict']
            
            submission_data.append({'ID': acc['ID'], 'Predict': predict})
        
        return pd.DataFrame(submission_data)
    
    def analyze_test_results(self, state, test_batch, test_direction, new_f1):
        """åˆ†ææµ‹è¯•ç»“æœå¹¶è®¡ç®—è½¬æ¢æˆåŠŸç‡"""
        print(f"\n=== åˆ†ææµ‹è¯•ç»“æœ ===")
        
        # è®¡ç®—æ–°æ—§æ··æ·†çŸ©é˜µ
        test_submission = self.create_test_submission(state, test_batch, test_direction)
        new_bad_count = len(test_submission[test_submission['Predict'] == 1])
        new_good_count = len(test_submission[test_submission['Predict'] == 0])
        new_cm = self.calculate_confusion_matrix(new_bad_count, new_good_count, new_f1)
        
        old_cm = state['current_cm']
        
        print(f"è½¬æ¢æ–¹å‘: {test_direction}")
        print(f"æµ‹è¯•è´¦æˆ·æ•°: {len(test_batch)}")
        print(f"F1å˜åŒ–: {state['current_f1']:.6f} â†’ {new_f1:.6f} ({new_f1 - state['current_f1']:+.6f})")
        print(f"æ—§æ··æ·†çŸ©é˜µ: TP={old_cm['TP']}, FP={old_cm['FP']}, FN={old_cm['FN']}, TN={old_cm['TN']}")
        print(f"æ–°æ··æ·†çŸ©é˜µ: TP={new_cm['TP']}, FP={new_cm['FP']}, FN={new_cm['FN']}, TN={new_cm['TN']}")
        
        # è®¡ç®—è½¬æ¢æˆåŠŸç‡
        if test_direction == "0_to_1":
            # good â†’ badçš„è½¬æ¢
            # å¯¹äº0â†’1è½¬æ¢ï¼Œæˆ‘ä»¬æµ‹è¯•äº† len(test_batch) ä¸ªè´¦æˆ·
            # è¿™äº›è½¬æ¢å¯èƒ½å¢åŠ TPï¼ˆæ­£ç¡®è¯†åˆ«çœŸbadï¼‰æˆ–å¢åŠ FPï¼ˆé”™è¯¯è¯†åˆ«goodä¸ºbadï¼‰
            tp_change = new_cm['TP'] - old_cm['TP']
            fp_change = new_cm['FP'] - old_cm['FP']
            
            # æ€»è½¬æ¢æ•°åº”è¯¥ç­‰äºæµ‹è¯•æ‰¹æ¬¡å¤§å°
            test_batch_size = len(test_batch)
            
            # æˆåŠŸè½¬æ¢ = TPå¢åŠ çš„æ•°é‡
            # å¤±è´¥è½¬æ¢ = FPå¢åŠ çš„æ•°é‡  
            # ä½†è¦ç¡®ä¿æ€»æ•°ç­‰äºæµ‹è¯•æ‰¹æ¬¡å¤§å°
            if tp_change + fp_change == test_batch_size:
                success_count = tp_change
                failure_count = fp_change
                success_rate = success_count / test_batch_size if test_batch_size > 0 else 0
            else:
                # å¦‚æœæ•°é‡ä¸åŒ¹é…ï¼Œè¯´æ˜æœ‰å…¶ä»–å˜åŒ–ï¼ŒæŒ‰æ¯”ä¾‹è®¡ç®—
                success_count = tp_change
                failure_count = test_batch_size - tp_change
                success_rate = success_count / test_batch_size if test_batch_size > 0 else 0
            
            print(f"è½¬æ¢åˆ†æ: æµ‹è¯•{test_batch_size}ä¸ª, TPå¢åŠ {tp_change}, FPå¢åŠ {fp_change}, æˆåŠŸç‡{success_rate:.2%}")
            
        else:  # "1_to_0"
            # bad â†’ goodçš„è½¬æ¢
            tn_change = new_cm['TN'] - old_cm['TN']
            fn_change = new_cm['FN'] - old_cm['FN']
            
            test_batch_size = len(test_batch)
            
            # å¯¹äº1â†’0è½¬æ¢ï¼ŒæˆåŠŸçš„è½¬æ¢ä¼šå¢åŠ TNï¼Œå¤±è´¥çš„è½¬æ¢ä¼šå¢åŠ FN
            if tn_change + fn_change == test_batch_size:
                success_count = tn_change
                failure_count = fn_change
                success_rate = success_count / test_batch_size if test_batch_size > 0 else 0
            else:
                success_count = tn_change
                failure_count = test_batch_size - tn_change
                success_rate = success_count / test_batch_size if test_batch_size > 0 else 0
            
            print(f"è½¬æ¢åˆ†æ: æµ‹è¯•{test_batch_size}ä¸ª, TNå¢åŠ {tn_change}, FNå¢åŠ {fn_change}, æˆåŠŸç‡{success_rate:.2%}")
        
        # äºŒåˆ†å†³ç­–
        decision = self.make_binary_decision(state, test_batch, success_rate, new_f1)
        
        # æ›´æ–°çŠ¶æ€
        if new_f1 > state['best_f1']:
            state['best_f1'] = new_f1
            self.save_best_submission(state, new_f1)
        
        state['current_f1'] = new_f1
        state['current_cm'] = new_cm
        
        return decision
    
    def make_binary_decision(self, state, test_batch, success_rate, new_f1):
        """åŸºäºè½¬æ¢æˆåŠŸç‡åšå‡ºäºŒåˆ†å†³ç­–"""
        print(f"\n=== äºŒåˆ†å†³ç­– ===")
        
        if success_rate == 1.0:
            # 100%æˆåŠŸï¼Œç¡®è®¤æ•´æ‰¹è½¬æ¢
            print(f"âœ… æˆåŠŸç‡100%ï¼Œç¡®è®¤è¿™{len(test_batch)}ä¸ªè´¦æˆ·çš„è½¬æ¢")
            
            # ç¡®è®¤è¿™æ‰¹è´¦æˆ·çš„è½¬æ¢
            for acc in test_batch:
                if acc['current_predict'] == 0:
                    state['confirmed_predictions'][acc['ID']] = 1  # ç¡®è®¤ä¸ºbad
                    acc['current_predict'] = 1
                else:
                    state['confirmed_predictions'][acc['ID']] = 0  # ç¡®è®¤ä¸ºgood
                    acc['current_predict'] = 0
            
            # ä»æµ‹è¯•é˜Ÿåˆ—ç§»é™¤å·²ç¡®è®¤çš„è´¦æˆ·
            confirmed_ids = set(acc['ID'] for acc in test_batch)
            state['test_queue'] = [acc for acc in state['test_queue'] 
                                 if acc['ID'] not in confirmed_ids]
            
            return f"confirmed_{len(test_batch)}"
            
        elif success_rate == 0.0:
            # 0%æˆåŠŸï¼Œç¡®è®¤æ•´æ‰¹ä¸è½¬æ¢ï¼ˆä¿æŒåŸæ ‡ç­¾ï¼‰
            print(f"âœ… æˆåŠŸç‡0%ï¼Œç¡®è®¤è¿™{len(test_batch)}ä¸ªè´¦æˆ·ä¿æŒåŸæ ‡ç­¾")
            
            # ç¡®è®¤è¿™æ‰¹è´¦æˆ·ä¿æŒåŸæ ‡ç­¾
            for acc in test_batch:
                state['confirmed_predictions'][acc['ID']] = acc['current_predict']
            
            # ä»æµ‹è¯•é˜Ÿåˆ—ç§»é™¤å·²ç¡®è®¤çš„è´¦æˆ·
            confirmed_ids = set(acc['ID'] for acc in test_batch)
            state['test_queue'] = [acc for acc in state['test_queue'] 
                                 if acc['ID'] not in confirmed_ids]
            
            return f"confirmed_original_{len(test_batch)}"
            
        else:
            # éƒ¨åˆ†æˆåŠŸï¼Œéœ€è¦äºŒåˆ†åˆ°æ›´å°æ‰¹æ¬¡
            print(f"âŒ æˆåŠŸç‡{success_rate:.2%}ï¼Œæ— æ³•ç¡®å®šå…·ä½“å“ªäº›è´¦æˆ·æˆåŠŸï¼Œéœ€è¦äºŒåˆ†")
            
            if len(test_batch) == 1:
                # å•ä¸ªè´¦æˆ·ä½†æˆåŠŸç‡ä¸æ˜¯0æˆ–100%ï¼Œè¿™æ˜¯å¼‚å¸¸æƒ…å†µ
                # ä¿å®ˆå¤„ç†ï¼šä¿æŒåŸæ ‡ç­¾
                acc = test_batch[0]
                state['confirmed_predictions'][acc['ID']] = acc['current_predict']
                print(f"âš ï¸ å•ä¸ªè´¦æˆ·å¼‚å¸¸æˆåŠŸç‡ï¼Œä¿å®ˆç¡®è®¤: {acc['ID']} ä¿æŒä¸º {acc['current_predict']}")
                
                # ä»æµ‹è¯•é˜Ÿåˆ—ç§»é™¤
                state['test_queue'] = [a for a in state['test_queue'] if a['ID'] != acc['ID']]
                return "single_conservative"
            else:
                # äºŒåˆ†ï¼šé‡æ–°æ’åˆ—æµ‹è¯•é˜Ÿåˆ—ï¼Œä¼˜å…ˆæµ‹è¯•æ¦‚ç‡æ›´é«˜çš„ä¸€åŠ
                test_batch.sort(key=lambda x: x['score'], reverse=True)
                
                # å°†æµ‹è¯•æ‰¹æ¬¡é‡æ–°æ”¾å…¥é˜Ÿåˆ—å‰ç«¯ï¼Œå‡†å¤‡ä¸‹æ¬¡äºŒåˆ†
                other_accounts = [acc for acc in state['test_queue'] 
                                if acc['ID'] not in [b['ID'] for b in test_batch]]
                state['test_queue'] = test_batch + other_accounts
                
                print(f"äºŒåˆ†ç­–ç•¥: å°†{len(test_batch)}ä¸ªè´¦æˆ·é‡æ–°æ’åºï¼Œä¸‹è½®æµ‹è¯•å‰{len(test_batch)//2}ä¸ª")
                return f"binary_split_{len(test_batch)}"
    
    def run_optimization(self, max_rounds=50, target_f1=1.0):
        """è¿è¡Œä¼˜åŒ–è¿‡ç¨‹"""
        state = self.initialize_state()
        
        print(f"\nå¼€å§‹æ­£ç¡®äºŒåˆ†æ³•ä¼˜åŒ–ï¼Œç›®æ ‡F1: {target_f1}")
        
        for round_num in range(1, max_rounds + 1):
            state['round'] = round_num
            
            print(f"\n{'='*60}")
            print(f"ç¬¬ {round_num} è½®ä¼˜åŒ–")
            print(f"{'='*60}")
            
            # é€‰æ‹©æµ‹è¯•æ‰¹æ¬¡
            test_batch, test_direction = self.select_next_test_batch(state)
            if not test_batch:
                print("æ‰€æœ‰è´¦æˆ·å·²å¤„ç†å®Œæ¯•ï¼")
                break
            
            print(f"æ¦‚ç‡èŒƒå›´: {test_batch[0]['score']:.6f} - {test_batch[-1]['score']:.6f}")
            
            # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
            test_submission = self.create_test_submission(state, test_batch, test_direction)
            test_filename = f"correct_test_round_{round_num}.csv"
            test_submission.to_csv(test_filename, index=False)
            
            current_bad = len(test_submission[test_submission['Predict'] == 1])
            current_good = len(test_submission[test_submission['Predict'] == 0])
            print(f"æµ‹è¯•åˆ†å¸ƒ: Bad={current_bad}, Good={current_good}")
            
            # æäº¤æµ‹è¯•
            print(f"æäº¤æµ‹è¯•æ–‡ä»¶: {test_filename}")
            new_f1 = self.submit_file(test_filename)
            
            if new_f1 is None:
                print("æäº¤å¤±è´¥ï¼Œè·³è¿‡æœ¬è½®")
                os.remove(test_filename)
                continue
            
            # åˆ†æç»“æœå¹¶åšå‡ºäºŒåˆ†å†³ç­–
            decision = self.analyze_test_results(state, test_batch, test_direction, new_f1)
            
            # è®°å½•å†å²
            state['test_history'].append({
                'round': round_num,
                'test_direction': test_direction,
                'batch_size': len(test_batch),
                'f1_score': new_f1,
                'decision': decision
            })
            
            # ä¿å­˜çŠ¶æ€
            self.save_state(state)
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            os.remove(test_filename)
            
            # æ£€æŸ¥å®Œæˆæ¡ä»¶
            if new_f1 >= target_f1:
                print(f"\nğŸ‰ è¾¾åˆ°ç›®æ ‡F1={target_f1}ï¼")
                break
            
            # æ˜¾ç¤ºè¿›åº¦
            confirmed_count = len(state['confirmed_predictions'])
            remaining_queue = len(state['test_queue'])
            print(f"\nå½“å‰è¿›åº¦:")
            print(f"  å½“å‰F1: {state['current_f1']:.6f}")
            print(f"  æœ€ä½³F1: {state['best_f1']:.6f}")
            print(f"  å·²ç¡®è®¤è´¦æˆ·: {confirmed_count}")
            print(f"  å¾…æµ‹è¯•é˜Ÿåˆ—: {remaining_queue}")
            
            if remaining_queue == 0:
                print("æµ‹è¯•é˜Ÿåˆ—å·²æ¸…ç©ºï¼")
                break
        
        self.generate_final_results(state)
        return state
    
    def generate_final_results(self, state):
        """ç”Ÿæˆæœ€ç»ˆç»“æœ"""
        print(f"\n{'='*60}")
        print("æ­£ç¡®äºŒåˆ†æ³•ä¼˜åŒ–å®Œæˆï¼")
        print(f"{'='*60}")
        
        # ç”Ÿæˆæœ€ç»ˆæäº¤æ–‡ä»¶
        submission_data = []
        for acc in state['all_accounts']:
            if acc['ID'] in state['confirmed_predictions']:
                predict = state['confirmed_predictions'][acc['ID']]
            else:
                predict = acc['current_predict']
            submission_data.append({'ID': acc['ID'], 'Predict': predict})
        
        final_df = pd.DataFrame(submission_data)
        final_df.to_csv('final_correct_binary_submission.csv', index=False)
        
        # ç»Ÿè®¡
        final_bad = len(final_df[final_df['Predict'] == 1])
        final_good = len(final_df[final_df['Predict'] == 0])
        confirmed_count = len(state['confirmed_predictions'])
        
        print(f"æœ€ç»ˆç»Ÿè®¡:")
        print(f"  æœ€ä½³F1åˆ†æ•°: {state['best_f1']:.6f}")
        print(f"  æœ€ç»ˆé¢„æµ‹bad: {final_bad}")
        print(f"  æœ€ç»ˆé¢„æµ‹good: {final_good}")
        print(f"  ç¡®è®¤è´¦æˆ·æ•°: {confirmed_count}")
        print(f"  æ€»è½®æ•°: {state['round']}")
        print(f"  ç”Ÿæˆæ–‡ä»¶: final_correct_binary_submission.csv")

def main():
    """ä¸»å‡½æ•°"""
    account_scores_file = "/Users/mannormal/4011/account_scores.csv"
    
    optimizer = CorrectBinarySearchOptimizer(account_scores_file)
    final_state = optimizer.run_optimization(max_rounds=50, target_f1=1.0)
    
    print("\nğŸ‰ æ­£ç¡®äºŒåˆ†æ³•ä¼˜åŒ–å®Œæˆï¼")

if __name__ == "__main__":
    main()