import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix
import os
from datetime import datetime

def create_bad_model_predictions():
    """åˆ›å»ºæ•…æ„è¡¨ç°å¾ˆå·®çš„æ¨¡å‹æ¥è¯Šæ–­F1è¯„ä»·æŒ‡æ ‡"""
    
    print("="*60)
    print("ğŸ” æ•…æ„è®­ç»ƒå·®æ¨¡å‹ - F1æŒ‡æ ‡è¯Šæ–­å®éªŒ")
    print("="*60)
    
    # 1. æ•°æ®åŠ è½½
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    data_path = "/Users/mannormal/4011/Qi Zihan/feature_extraction/generated_features/all_features_with_time.csv"
    df = pd.read_csv(data_path)
    
    train_path = "/Users/mannormal/4011/Qi Zihan/original_data/train_acc.csv"
    train_df = pd.read_csv(train_path)
    
    test_path = "/Users/mannormal/4011/Qi Zihan/original_data/test_acc_predict.csv"
    test_df = pd.read_csv(test_path)
    
    # å¤„ç†è®­ç»ƒæ•°æ®
    train_accounts = set(train_df['account'])
    df_train = df[df['account'].isin(train_accounts)].copy()
    df_train = df_train.merge(train_df[['account', 'flag']], on='account', how='inner')
    df_train['label'] = df_train['flag']
    
    # å¤„ç†æµ‹è¯•æ•°æ®
    test_accounts = set(test_df['account'])
    df_test = df[df['account'].isin(test_accounts)].copy()
    
    print(f"è®­ç»ƒæ•°æ®: {df_train.shape}")
    print(f"æµ‹è¯•æ•°æ®: {df_test.shape}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(df_train['label'])}")
    
    # 2. ç‰¹å¾é€‰æ‹© - æ•…æ„é€‰æ‹©å¾ˆå°‘ä¸”å¯èƒ½ä¸é‡è¦çš„ç‰¹å¾
    print(f"\nğŸ¯ æ•…æ„é€‰æ‹©å°‘é‡ç‰¹å¾æ¥é™ä½æ¨¡å‹æ€§èƒ½...")
    
    time_cols = ['first_transaction_time', 'last_transaction_time']
    all_features = [col for col in df.columns if col not in ['account'] + time_cols]
    
    # éšæœºé€‰æ‹©4ä¸ªç‰¹å¾ï¼Œæ•…æ„åˆ¶é€ ä¿¡æ¯ä¸è¶³
    np.random.seed(42)
    bad_features = np.random.choice(all_features, size=min(4, len(all_features)), replace=False).tolist()
    
    print(f"é€‰æ‹©çš„'å·®'ç‰¹å¾: {bad_features}")
    
    # 3. æ•°æ®é¢„å¤„ç† - æ•…æ„ç®€åŒ–
    X_train = df_train[bad_features].copy()
    X_test = df_test[bad_features].copy()
    y_train = df_train['label'].values
    
    # ç®€å•å¤„ç†ç¼ºå¤±å€¼
    X_train = X_train.fillna(X_train.mean())
    X_test = X_test.fillna(X_train.mean())
    
    # ç®€å•æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"ç‰¹å¾å¤„ç†åå½¢çŠ¶: è®­ç»ƒ{X_train_scaled.shape}, æµ‹è¯•{X_test_scaled.shape}")
    
    # 4. åˆ›å»º8ä¸ªå€™é€‰å·®æ¨¡å‹
    print(f"\nğŸ¤– åˆ›å»º8ä¸ªå€™é€‰å·®æ¨¡å‹...")
    
    # å®šä¹‰æ¨¡å‹ç±»
    class RandomModel:
        def __init__(self, bad_ratio=0.05, random_seed=456):
            self.bad_ratio = bad_ratio
            self.random_seed = random_seed
            
        def fit(self, X, y):
            pass
            
        def predict(self, X):
            n_samples = len(X)
            np.random.seed(self.random_seed)
            predictions = np.random.choice([0, 1], size=n_samples, p=[1-self.bad_ratio, self.bad_ratio])
            return predictions
    
    class AlwaysGoodModel:
        def fit(self, X, y): pass
        def predict(self, X): return np.zeros(len(X), dtype=int)
    
    class AlwaysBadModel:
        def fit(self, X, y): pass
        def predict(self, X): return np.ones(len(X), dtype=int)
    
    # åˆ›å»ºå€™é€‰æ¨¡å‹
    candidate_models = {}
    candidate_models['over_regularized'] = LogisticRegression(C=0.0001, max_iter=5, solver='liblinear', random_state=42)
    candidate_models['underfit'] = LogisticRegression(C=100, max_iter=3, solver='liblinear', random_state=123)
    candidate_models['random_conservative'] = RandomModel(bad_ratio=0.05, random_seed=456)
    candidate_models['random_aggressive'] = RandomModel(bad_ratio=0.25, random_seed=789)
    candidate_models['minimal_features'] = LogisticRegression(C=1.0, max_iter=20, solver='liblinear', random_state=999)
    candidate_models['extreme_overfit'] = LogisticRegression(C=10000, max_iter=2, solver='liblinear', random_state=555)
    candidate_models['always_good'] = AlwaysGoodModel()
    candidate_models['always_bad'] = AlwaysBadModel()
    
    # 5. 5æŠ˜äº¤å‰éªŒè¯è¯„ä¼°æ‰€æœ‰å€™é€‰æ¨¡å‹
    print(f"\nğŸ“Š 5æŠ˜äº¤å‰éªŒè¯è¯„ä¼°å€™é€‰æ¨¡å‹...")
    
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    candidate_results = {}
    
    for model_name, model in candidate_models.items():
        print(f"\nè¯„ä¼°å€™é€‰æ¨¡å‹: {model_name}")
        
        cv_weighted_f1 = []
        cv_macro_f1 = []
        cv_bad_f1 = []
        cv_accuracy = []
        cv_bad_recall = []
        cv_bad_precision = []
        
        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled, y_train)):
            X_fold_train, X_fold_val = X_train_scaled[train_idx], X_train_scaled[val_idx]
            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_fold_train, y_fold_train)
            val_pred = model.predict(X_fold_val)
            
            # è®¡ç®—å„ç§F1
            weighted_f1 = f1_score(y_fold_val, val_pred, average='weighted')
            macro_f1 = f1_score(y_fold_val, val_pred, average='macro') if len(np.unique(val_pred)) > 1 else 0
            bad_f1 = f1_score(y_fold_val, val_pred, pos_label=1) if len(np.unique(val_pred)) > 1 and 1 in val_pred else 0
            accuracy = accuracy_score(y_fold_val, val_pred)
            
            # åå®¢æˆ·æŒ‡æ ‡
            bad_recall = recall_score(y_fold_val, val_pred, pos_label=1, zero_division=0)
            bad_precision = precision_score(y_fold_val, val_pred, pos_label=1, zero_division=0)
            
            cv_weighted_f1.append(weighted_f1)
            cv_macro_f1.append(macro_f1)
            cv_bad_f1.append(bad_f1)
            cv_accuracy.append(accuracy)
            cv_bad_recall.append(bad_recall)
            cv_bad_precision.append(bad_precision)
        
        candidate_results[model_name] = {
            'cv_weighted_f1': np.mean(cv_weighted_f1),
            'cv_macro_f1': np.mean(cv_macro_f1),
            'cv_bad_f1': np.mean(cv_bad_f1),
            'cv_accuracy': np.mean(cv_accuracy),
            'cv_bad_recall': np.mean(cv_bad_recall),
            'cv_bad_precision': np.mean(cv_bad_precision),
            'f1_scores': [np.mean(cv_weighted_f1), np.mean(cv_macro_f1), np.mean(cv_bad_f1)]
        }
        
        print(f"  åŠ æƒF1: {np.mean(cv_weighted_f1):.4f}")
        print(f"  å®F1: {np.mean(cv_macro_f1):.4f}")  
        print(f"  åå®¢æˆ·F1: {np.mean(cv_bad_f1):.4f}")
    
    # 6. é€‰æ‹©ä¸‰ä¸ªF1å·®è·æœ€å¤§çš„æ¨¡å‹
    print(f"\nğŸ¯ é€‰æ‹©ä¸‰ä¸ªF1å·®è·æœ€å¤§çš„æ¨¡å‹...")
    
    # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„ä¸‰ä¸ªF1ä¹‹é—´çš„æ–¹å·®ï¼ˆå·®è·ï¼‰
    model_f1_variance = {}
    for model_name, result in candidate_results.items():
        f1_scores = result['f1_scores']
        variance = np.var(f1_scores)  # æ–¹å·®è¶Šå¤§ï¼Œå·®è·è¶Šå¤§
        range_span = max(f1_scores) - min(f1_scores)  # èŒƒå›´è·¨åº¦
        model_f1_variance[model_name] = {
            'variance': variance,
            'range': range_span,
            'scores': f1_scores,
            'weighted_f1': result['cv_weighted_f1'],
            'macro_f1': result['cv_macro_f1'],
            'bad_f1': result['cv_bad_f1']
        }
    
    # æŒ‰F1å·®è·æ’åºï¼Œé€‰æ‹©å‰3ä¸ª
    sorted_models = sorted(model_f1_variance.items(), 
                          key=lambda x: x[1]['variance'], reverse=True)
    
    top3_models = sorted_models[:3]
    
    print(f"é€‰æ‹©çš„3ä¸ªF1å·®è·æœ€å¤§çš„æ¨¡å‹:")
    for i, (model_name, info) in enumerate(top3_models, 1):
        print(f"{i}. {model_name}:")
        print(f"   åŠ æƒF1: {info['weighted_f1']:.4f}")
        print(f"   å®F1: {info['macro_f1']:.4f}")
        print(f"   åå®¢æˆ·F1: {info['bad_f1']:.4f}")
        print(f"   F1æ–¹å·®: {info['variance']:.4f}")
        print(f"   F1è·¨åº¦: {info['range']:.4f}")
    
    # 7. ç”¨é€‰å®šçš„3ä¸ªæ¨¡å‹ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹
    print(f"\nğŸ”® ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...")
    
    selected_models = {}
    
    for model_name, _ in top3_models:
        model = candidate_models[model_name]
        
        # ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®è®­ç»ƒ
        model.fit(X_train_scaled, y_train)
        
        # é¢„æµ‹æµ‹è¯•é›†
        test_pred = model.predict(X_test_scaled)
        
        selected_models[model_name] = {
            'model': model,
            'cv_results': candidate_results[model_name],
            'test_predictions': test_pred,
            'test_bad_ratio': np.mean(test_pred)
        }
        
        print(f"{model_name}: æµ‹è¯•é›†é¢„æµ‹Badå®¢æˆ· {np.sum(test_pred)} ({np.mean(test_pred)*100:.1f}%)")
    
    # 8. è¯¦ç»†å¯¹æ¯”åˆ†æ
    print(f"\n" + "="*80)
    print("ğŸ“Š æœ€ç»ˆé€‰å®šçš„3ä¸ªæ¨¡å‹F1å¯¹æ¯”")
    print("="*80)
    
    print(f"{'æ¨¡å‹':<20} {'åŠ æƒF1':<12} {'å®F1':<12} {'åF1':<12} {'F1æ–¹å·®':<12} {'æµ‹è¯•Bad%':<10}")
    print("-" * 90)
    
    for model_name, result in selected_models.items():
        cv_results = result['cv_results']
        variance = model_f1_variance[model_name]['variance']
        print(f"{model_name:<20} "
              f"{cv_results['cv_weighted_f1']:<12.4f} "
              f"{cv_results['cv_macro_f1']:<12.4f} "
              f"{cv_results['cv_bad_f1']:<12.4f} "
              f"{variance:<12.4f} "
              f"{result['test_bad_ratio']*100:<10.1f}%")
    
    # 9. ä¿å­˜é¢„æµ‹æ–‡ä»¶
    result_dir = "/Users/mannormal/4011/Qi Zihan/result_analysis/prediction_results"
    os.makedirs(result_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    saved_files = []
    
    for i, (model_name, result) in enumerate(selected_models.items(), 1):
        # ä¿å­˜æµ‹è¯•é›†é¢„æµ‹
        pred_df = pd.DataFrame({
            'account': df_test['account'].values,
            'prediction': result['test_predictions']
        })
        
        cv_results = result['cv_results']
        filename = f"DIAGNOSTIC_MODEL_{i}_{model_name}_{timestamp}_wF1_{cv_results['cv_weighted_f1']:.4f}_mF1_{cv_results['cv_macro_f1']:.4f}_bF1_{cv_results['cv_bad_f1']:.4f}.csv"
        pred_df.to_csv(os.path.join(result_dir, filename), index=False)
        saved_files.append(filename)
        print(f"å·²ä¿å­˜æ¨¡å‹{i}: {filename}")
    
    # 10. F1æŒ‡æ ‡è¯Šæ–­ç»“è®º
    print(f"\n" + "="*80)
    print("ğŸ” F1æŒ‡æ ‡è¯Šæ–­å®éªŒç»“è®º")
    print("="*80)
    
    print(f"\né€šè¿‡5æŠ˜äº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬é€‰æ‹©äº†3ä¸ªF1å·®è·æœ€å¤§çš„æ¨¡å‹:")
    
    for i, (model_name, result) in enumerate(selected_models.items(), 1):
        cv_results = result['cv_results']
        variance_info = model_f1_variance[model_name]
        
        print(f"\nğŸ·ï¸  æ¨¡å‹{i}: {model_name}")
        print(f"   åŠ æƒF1: {cv_results['cv_weighted_f1']:.4f}")
        print(f"   å®å¹³å‡F1: {cv_results['cv_macro_f1']:.4f}")
        print(f"   åå®¢æˆ·F1: {cv_results['cv_bad_f1']:.4f}")
        print(f"   F1æœ€å¤§å·®è·: {variance_info['range']:.4f}")
        print(f"   æµ‹è¯•é›†é¢„æµ‹Badå®¢æˆ·: {np.sum(result['test_predictions'])} ({result['test_bad_ratio']*100:.1f}%)")
    
    print(f"\nğŸ“‹ è¯Šæ–­ä½¿ç”¨æ–¹æ³•:")
    print(f"1. ç”¨è¿™3ä¸ªé¢„æµ‹æ–‡ä»¶æµ‹è¯•ä½ çš„è¯„ä»·ç³»ç»Ÿ")
    print(f"2. çœ‹ç³»ç»Ÿç»™å‡ºçš„åˆ†æ•°æœ€æ¥è¿‘å“ªä¸ªF1æŒ‡æ ‡")
    print(f"3. å¦‚æœç³»ç»Ÿåˆ†æ•°:")
    print(f"   - æ¥è¿‘åŠ æƒF1 â†’ ç³»ç»Ÿç”¨weighted average F1")
    print(f"   - æ¥è¿‘å®å¹³å‡F1 â†’ ç³»ç»Ÿç”¨macro average F1")
    print(f"   - æ¥è¿‘åå®¢æˆ·F1 â†’ ç³»ç»Ÿä¸“æ³¨åå®¢æˆ·æ£€æµ‹")
    
    return selected_models, saved_files

if __name__ == "__main__":
    results, files = create_bad_model_predictions()
    print(f"\nâœ… F1è¯Šæ–­å®éªŒå®Œæˆ!")
    print(f"ç”Ÿæˆäº†3ä¸ªF1å·®è·æœ€å¤§çš„æ¨¡å‹é¢„æµ‹æ–‡ä»¶:")
    for i, file in enumerate(files, 1):
        print(f"  {i}. {file}")
    print(f"\nğŸ§ª ç°åœ¨å¯ä»¥ç”¨è¿™äº›æ–‡ä»¶æµ‹è¯•ä½ çš„è¯„ä»·ç³»ç»Ÿï¼Œè¯Šæ–­ä½¿ç”¨çš„æ˜¯å“ªç§F1æŒ‡æ ‡!")