import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import os
from datetime import datetime
import math

# è®¾ç½®è®¾å¤‡
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

# ========== å›å½’åŸºç¡€çš„MLPæ¶æ„ ==========
class RefinedMLP(nn.Module):
    """ç²¾ç®€ç‰ˆï¼šå›å½’åŸå§‹æ¶æ„ï¼Œä½†é€‚é…æ–°çš„ç‰¹å¾æ•°é‡å¹¶åŠ å…¥æœ‰æ•ˆæ”¹è¿›"""
    
    def __init__(self, input_dim=53, dropout_rates=[0.3, 0.2, 0.2, 0.1, 0.1]):  # æ›´æ–°é»˜è®¤è¾“å…¥ç»´åº¦
        super(RefinedMLP, self).__init__()
        
        self.network = nn.Sequential(
            # input_dim -> 128
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_rates[0]),
            
            # 128 -> 64
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rates[1]),
            
            # 64 -> 64
            nn.Linear(64, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rates[2]),
            
            # 64 -> 32
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(dropout_rates[3]),
            
            # 32 -> 16
            nn.Linear(32, 16),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(dropout_rates[4]),
            
            # 16 -> 2
            nn.Linear(16, 2)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)

# ========== è½»é‡çº§Label Smoothing ==========
class LightLabelSmoothingCE(nn.Module):
    """è½»é‡çº§Label Smoothing - åªä¿ç•™æ ¸å¿ƒåŠŸèƒ½"""
    def __init__(self, smoothing=0.05, class_weights=None):
        super(LightLabelSmoothingCE, self).__init__()
        self.smoothing = smoothing
        self.class_weights = class_weights
        
    def forward(self, pred, target):
        log_prob = F.log_softmax(pred, dim=1)
        
        # ç®€åŒ–çš„Label smoothing
        with torch.no_grad():
            smooth_target = torch.zeros_like(pred)
            smooth_target.fill_(self.smoothing / (pred.size(1) - 1))  # å¹³æ»‘åˆ†å¸ƒ
            smooth_target.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)  # çœŸå®æ ‡ç­¾
        
        loss = -torch.sum(smooth_target * log_prob, dim=1)
        
        # åº”ç”¨ç±»åˆ«æƒé‡
        if self.class_weights is not None:
            weights = self.class_weights[target]
            loss = loss * weights
        
        return loss.mean()

# ========== ç®€åŒ–çš„æ—©åœæ³• ==========
class SimpleEarlyStopping:
    """ç®€åŒ–çš„æ—©åœæ³• - ä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½"""
    def __init__(self, patience=12, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_score = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, score, model):
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model)
            return False
        
        if score > self.best_score + self.min_delta:
            self.best_score = score
            self.counter = 0
            self.save_checkpoint(model)
            return False
        else:
            self.counter += 1
            if self.counter >= self.patience:
                # æ¢å¤æœ€ä½³æƒé‡
                if self.best_weights is not None:
                    model.load_state_dict(self.best_weights)
                return True
        
        return False
    
    def save_checkpoint(self, model):
        self.best_weights = {k: v.clone() for k, v in model.state_dict().items()}

# ========== ç²¾ç®€çš„è®­ç»ƒå‡½æ•° ==========
def train_refined_model(model, train_loader, val_loader, config, class_weights, epochs=100):
    """ç²¾ç®€çš„è®­ç»ƒå‡½æ•° - åªä¿ç•™æœ‰æ•ˆçš„ç»„ä»¶"""
    
    print(f"ğŸ¯ ç²¾ç®€è®­ç»ƒé…ç½®:")
    print(f"   Weight Decay: {config['weight_decay']}")
    print(f"   Learning Rate: {config['lr']}")
    print(f"   Label Smoothing: {config['smoothing']}")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = LightLabelSmoothingCE(
        smoothing=config['smoothing'], 
        class_weights=class_weights.to(device)
    )
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=config['lr'], 
        weight_decay=config['weight_decay']
    )
    
    # ç®€å•çš„å­¦ä¹ ç‡è°ƒåº¦ - éªŒè¯åœæ»æ—¶å‡åŠ
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=6, min_lr=config['lr']*0.01
    )
    
    # æ—©åœæ³•
    early_stopping = SimpleEarlyStopping(patience=12, min_delta=0.001)
    
    train_losses = []
    val_f1_scores = []
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # è½»å¾®çš„æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                _, predicted = torch.max(outputs, 1)
                
                val_predictions.extend(predicted.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        val_f1 = f1_score(val_targets, val_predictions, average='weighted')
        avg_train_loss = train_loss / len(train_loader)
        
        train_losses.append(avg_train_loss)
        val_f1_scores.append(val_f1)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_f1)
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_f1, model):
            print(f"Early stopping at epoch {epoch+1}")
            break
        
        if epoch % 10 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f'Epoch [{epoch+1}/{epochs}], '
                  f'Train Loss: {avg_train_loss:.4f}, '
                  f'Val F1: {val_f1:.4f}, '
                  f'LR: {current_lr:.2e}')
    
    return train_losses, val_f1_scores

# ========== èšç„¦çš„é…ç½®æµ‹è¯• ==========
def focused_config_search(X, y):
    """èšç„¦çš„é…ç½®æœç´¢ - åŸºäºä½ çš„å‘ç°"""
    
    print("\nğŸ¯ èšç„¦é…ç½®æœç´¢ - åŸºäºå‘ç°çš„æœ‰æ•ˆæ¨¡å¼")
    print("=" * 60)
    
    # åŸºäºä½ çš„åˆ†æï¼Œé‡ç‚¹æµ‹è¯•é«˜æ­£åˆ™åŒ–é…ç½®
    focused_configs = [
        # é…ç½®1: ä½ å‘ç°çš„æœ€ä½³é…ç½®
        {'weight_decay': 0.001, 'lr': 0.0008, 'smoothing': 0.05, 'name': 'æœ€ä½³å‘ç°'},
        
        # é…ç½®2: ç¨å¾®é™ä½æ­£åˆ™åŒ–
        {'weight_decay': 0.0008, 'lr': 0.001, 'smoothing': 0.05, 'name': 'å¹³è¡¡ç‰ˆæœ¬'},
        
        # é…ç½®3: åŸå§‹åŸºç¡€ + è½»å¾®æ”¹è¿›
        {'weight_decay': 0.0005, 'lr': 0.001, 'smoothing': 0.03, 'name': 'ä¿å®ˆæ”¹è¿›'},
        
        # é…ç½®4: æœ€å°æ”¹åŠ¨
        {'weight_decay': 0.0001, 'lr': 0.001, 'smoothing': 0.02, 'name': 'æœ€å°æ”¹åŠ¨'},
    ]
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = np.bincount(y)
    class_weights = torch.FloatTensor(len(y) / (len(class_counts) * class_counts))
    print(f"ç±»åˆ«æƒé‡: {class_weights}")
    
    results = []
    
    # 5æŠ˜å¿«é€ŸéªŒè¯
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    for config_idx, config in enumerate(focused_configs):
        print(f"\nğŸ“Š æµ‹è¯•é…ç½® {config_idx+1}: {config['name']}")
        print(f"   {config}")
        
        fold_scores = []
        
        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # æ•°æ®å‡†å¤‡
            train_dataset = TensorDataset(
                torch.FloatTensor(X_train.values),
                torch.LongTensor(y_train)
            )
            val_dataset = TensorDataset(
                torch.FloatTensor(X_val.values),
                torch.LongTensor(y_val)
            )
            
            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
            
            # æ¨¡å‹è®­ç»ƒ
            model = RefinedMLP(input_dim=X.shape[1]).to(device)
            train_losses, val_f1_scores = train_refined_model(
                model, train_loader, val_loader, config, class_weights, epochs=80
            )
            
            fold_f1 = max(val_f1_scores) if val_f1_scores else 0
            fold_scores.append(fold_f1)
            
            print(f"    Fold {fold_idx+1}: F1 = {fold_f1:.4f}")
        
        # ç»Ÿè®¡ç»“æœ
        mean_f1 = np.mean(fold_scores)
        std_f1 = np.std(fold_scores)
        
        results.append({
            'config': config,
            'f1_mean': mean_f1,
            'f1_std': std_f1,
            'fold_scores': fold_scores
        })
        
        print(f"âœ… {config['name']}: F1 = {mean_f1:.4f} Â± {std_f1:.4f}")
    
    # é€‰æ‹©æœ€ä½³é…ç½®
    best_result = max(results, key=lambda x: x['f1_mean'])
    
    print(f"\nğŸ† æœ€ä½³é…ç½®: {best_result['config']['name']}")
    print(f"   F1åˆ†æ•°: {best_result['f1_mean']:.4f} Â± {best_result['f1_std']:.4f}")
    print(f"   è¯¦ç»†é…ç½®: {best_result['config']}")
    
    return best_result, results

# ========== æœ€ç»ˆå®Œæ•´éªŒè¯ ==========
def final_validation(X, y, best_config, cv_folds=10):
    """æœ€ç»ˆ10æŠ˜äº¤å‰éªŒè¯"""
    
    print(f"\nğŸ”„ æœ€ç»ˆ{cv_folds}æŠ˜äº¤å‰éªŒè¯")
    print("=" * 60)
    print(f"ä½¿ç”¨é…ç½®: {best_config['name']}")
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = np.bincount(y)
    class_weights = torch.FloatTensor(len(y) / (len(class_counts) * class_counts))
    
    # 10æŠ˜äº¤å‰éªŒè¯
    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    fold_results = []
    
    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        print(f"\n--- Fold {fold_idx+1}/{cv_folds} ---")
        
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # æ•°æ®å‡†å¤‡
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train.values),
            torch.LongTensor(y_train)
        )
        val_dataset = TensorDataset(
            torch.FloatTensor(X_val.values),
            torch.LongTensor(y_val)
        )
        
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        
        # æ¨¡å‹è®­ç»ƒ
        model = RefinedMLP(input_dim=X.shape[1]).to(device)
        train_losses, val_f1_scores = train_refined_model(
            model, train_loader, val_loader, best_config, class_weights, epochs=100
        )
        
        best_f1 = max(val_f1_scores)
        fold_results.append({
            'fold': fold_idx + 1,
            'f1': best_f1,
            'epochs': len(train_losses)
        })
        
        print(f"Fold {fold_idx+1} æœ€ä½³F1: {best_f1:.4f}")
    
    # ç»Ÿè®¡æœ€ç»ˆç»“æœ
    final_f1_scores = [r['f1'] for r in fold_results]
    mean_f1 = np.mean(final_f1_scores)
    std_f1 = np.std(final_f1_scores)
    avg_epochs = np.mean([r['epochs'] for r in fold_results])
    
    print(f"\nğŸ‰ æœ€ç»ˆç»“æœ:")
    print(f"   å¹³å‡F1: {mean_f1:.4f} Â± {std_f1:.4f}")
    print(f"   å„æŠ˜F1: {[f'{f:.4f}' for f in final_f1_scores]}")
    print(f"   å¹³å‡è®­ç»ƒè½®æ•°: {avg_epochs:.1f}")
    
    return mean_f1, std_f1, fold_results

# ========== ä¸»å‡½æ•° ==========
def main_refined():
    """ç²¾ç®€ä¸»å‡½æ•° - å›å½’åŸºç¡€ä½†æœ‰é’ˆå¯¹æ€§æ”¹è¿›"""
    
    print("="*80)
    print("ğŸ¯ MLPç²¾ç®€ä¼˜åŒ– - å›å½’åŸºç¡€ + æœ‰æ•ˆæ”¹è¿›")
    print("="*80)
    
    # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    print("\nğŸ“‚ æ•°æ®åŠ è½½...")
    data_path = "/Users/mannormal/4011/Qi Zihan/feature_extraction/generated_features/all_features_with_time.csv"
    df = pd.read_csv(data_path)
    
    train_path = "/Users/mannormal/4011/Qi Zihan/original_data/train_acc.csv"
    train_df = pd.read_csv(train_path)
    
    test_path = "/Users/mannormal/4011/Qi Zihan/original_data/test_acc_predict.csv"
    test_df = pd.read_csv(test_path)
    
    # å¤„ç†è®­ç»ƒæ•°æ®
    train_accounts = set(train_df['account'])
    df_train = df[df['account'].isin(train_accounts)].copy()
    df_train = df_train.merge(train_df[['account', 'flag']], on='account', how='inner')
    df_train['label'] = df_train['flag']
    
    print(f"è®­ç»ƒæ•°æ®: {df_train.shape}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(df_train['label'])}")
    
    # ç‰¹å¾é¢„å¤„ç† - æ’é™¤æ—¶é—´å­—ç¬¦ä¸²ç‰¹å¾ï¼Œä¿ç•™æ•°å€¼ç‰¹å¾
    # æ’é™¤accountå’Œæ—¶é—´å­—ç¬¦ä¸²åˆ—
    time_cols = ['first_transaction_time', 'last_transaction_time'] 
    feature_cols = [col for col in df.columns if col not in ['account'] + time_cols]
    
    print(f"ğŸ“Š åŸå§‹ç‰¹å¾æ•°: {len(df.columns)-1}")
    print(f"ğŸ“Š æ’é™¤æ—¶é—´å­—ç¬¦ä¸²å: {len(feature_cols)} ä¸ªæ•°å€¼ç‰¹å¾")
    print(f"ğŸ“Š æ’é™¤çš„æ—¶é—´åˆ—: {time_cols}")
    
    # ä½¿ç”¨ç®€åŒ–çš„é¢„å¤„ç†ï¼ˆé¿å…å¯¼å…¥mlpæ¨¡å—çš„ä¾èµ–é—®é¢˜ï¼‰
    X_train = df_train[feature_cols].copy()
    
    # åŸºç¡€é¢„å¤„ç†
    for col in X_train.columns:
        if 'profit' in col.lower():
            # å¯¹profitç±»ç‰¹å¾è¿›è¡Œå¯¹æ•°å˜æ¢å’Œæˆªæ–­
            X_train[col] = np.sign(X_train[col]) * np.log1p(np.abs(X_train[col]))
            Q01, Q99 = X_train[col].quantile([0.01, 0.99])
            X_train[col] = np.clip(X_train[col], Q01, Q99)
        elif 'ratio' in col.lower():
            # å¯¹ratioç±»ç‰¹å¾è¿›è¡Œæˆªæ–­
            X_train[col] = np.clip(X_train[col], 0, 50)
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train = pd.DataFrame(
        scaler.fit_transform(X_train),
        columns=X_train.columns,
        index=X_train.index
    )
    
    y_train = df_train['label'].values
    
    print(f"ğŸ”§ ä½¿ç”¨ {X_train.shape[1]} ä¸ªæ•°å€¼ç‰¹å¾ (æ’é™¤æ—¶é—´å­—ç¬¦ä¸²)")
    
    # 2. èšç„¦é…ç½®æœç´¢
    best_result, all_results = focused_config_search(X_train, y_train)
    
    # 3. ä¸åŸå§‹åŸºçº¿å¯¹æ¯”
    print(f"\nğŸ“Š ä¸åŸå§‹æ€§èƒ½å¯¹æ¯”:")
    print(f"   åŸå§‹MLPåŸºçº¿: ~0.8900")
    print(f"   å½“å‰æœ€ä½³é…ç½®: {best_result['f1_mean']:.4f}")
    
    if best_result['f1_mean'] >= 0.888:  # è®¾å®šä¸€ä¸ªåˆç†çš„é˜ˆå€¼
        print("âœ… é…ç½®æœ‰æ•ˆï¼Œè¿›è¡Œæœ€ç»ˆéªŒè¯")
        
        # 4. æœ€ç»ˆ10æŠ˜éªŒè¯
        final_mean_f1, final_std_f1, fold_results = final_validation(
            X_train, y_train, best_result['config'], cv_folds=10
        )
        
        # 5. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        print(f"\nğŸš€ è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
        full_dataset = TensorDataset(
            torch.FloatTensor(X_train.values),
            torch.LongTensor(y_train)
        )
        full_loader = DataLoader(full_dataset, batch_size=64, shuffle=True)
        
        # è®­ç»ƒæœ€ä¼˜è½®æ•°
        optimal_epochs = int(np.mean([r['epochs'] for r in fold_results]))
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_counts = np.bincount(y_train)
        class_weights = torch.FloatTensor(len(y_train) / (len(class_counts) * class_counts))
        
        # æœ€ç»ˆæ¨¡å‹
        final_model = RefinedMLP(input_dim=X_train.shape[1]).to(device)
        
        criterion = LightLabelSmoothingCE(
            smoothing=best_result['config']['smoothing'], 
            class_weights=class_weights.to(device)
        )
        optimizer = optim.AdamW(
            final_model.parameters(), 
            lr=best_result['config']['lr'], 
            weight_decay=best_result['config']['weight_decay']
        )
        
        print(f"è®­ç»ƒè½®æ•°: {optimal_epochs}")
        final_model.train()
        
        for epoch in range(optimal_epochs):
            epoch_loss = 0.0
            for batch_X, batch_y in full_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                
                optimizer.zero_grad()
                outputs = final_model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)
                optimizer.step()
                
                epoch_loss += loss.item()
            
            if epoch % 20 == 0:
                print(f'Epoch [{epoch+1}/{optimal_epochs}], Loss: {epoch_loss/len(full_loader):.4f}')
        
        print("âœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ!")
        
        # 6. é¢„æµ‹æµ‹è¯•é›†
        print(f"\nğŸ”® é¢„æµ‹æµ‹è¯•é›†...")
        
        # å¤„ç†æµ‹è¯•é›†
        df_test = df[df['account'].isin(set(test_df['account']))].copy()
        X_test = df_test[feature_cols].copy()  # ä½¿ç”¨ç›¸åŒçš„æ•°å€¼ç‰¹å¾åˆ—
        
        # é¢„å¤„ç† - ä¸è®­ç»ƒé›†ä¿æŒä¸€è‡´
        for col in X_test.columns:
            if 'profit' in col.lower():
                X_test[col] = np.sign(X_test[col]) * np.log1p(np.abs(X_test[col]))
                Q01 = X_train[col].quantile(0.01)
                Q99 = X_train[col].quantile(0.99)
                X_test[col] = np.clip(X_test[col], Q01, Q99)
            elif 'ratio' in col.lower():
                X_test[col] = np.clip(X_test[col], 0, 50)
        
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test),
            columns=X_test.columns,
            index=X_test.index
        )
        
        # é¢„æµ‹
        final_model.eval()
        X_test_tensor = torch.FloatTensor(X_test_scaled.values).to(device)
        
        with torch.no_grad():
            outputs = final_model(X_test_tensor)
            _, predictions = torch.max(outputs, 1)
        
        predictions = predictions.cpu().numpy()
        
        # ä¿å­˜ç»“æœ
        result_dir = "/Users/mannormal/4011/Qi Zihan/result_analysis/prediction_results/"
        os.makedirs(result_dir, exist_ok=True)
        
        submission_df = pd.DataFrame({
            'account': df_test['account'].values,
            'Predict': predictions
        })
        
        config_name = best_result['config']['name'].replace(' ', '_')
        filename = f"MLP_REFINED_{config_name}_f1_{final_mean_f1:.4f}.csv"
        submission_df.to_csv(os.path.join(result_dir, filename), index=False)
        
        print(f"\nğŸ‰ ç²¾ç®€ç‰ˆMLPå®Œæˆ!")
        print(f"ğŸ“Š æœ€ä½³é…ç½®: {best_result['config']['name']}")
        print(f"ğŸ“ˆ æœ€ç»ˆF1: {final_mean_f1:.4f} Â± {final_std_f1:.4f}")
        print(f"ğŸ“ æ–‡ä»¶: {filename}")
        print(f"ğŸ“Š é¢„æµ‹åˆ†å¸ƒ: {np.bincount(predictions)}")
        
        return {
            'config': best_result['config'],
            'cv_f1': final_mean_f1,
            'cv_std': final_std_f1,
            'submission': submission_df,
            'all_configs': all_results
        }
        
    else:
        print("âŒ é…ç½®æ•ˆæœä¸ä½³ï¼Œå»ºè®®ä¿æŒåŸå§‹ç‰ˆæœ¬")
        return None

if __name__ == "__main__":
    results = main_refined()