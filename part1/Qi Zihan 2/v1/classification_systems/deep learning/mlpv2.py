import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import os
from datetime import datetime
import math

# è®¾ç½®è®¾å¤‡
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

# ========== æ¸è¿›å¼å¤æ‚åº¦çš„æ¨¡å‹è®¾è®¡ ==========
class BaselineMLP(nn.Module):
    """ç®€å•ç‰ˆ: 30â†’128â†’64â†’32â†’2 (çº¦4kå‚æ•°)"""
    
    def __init__(self, input_dim=30, dropout_rates=[0.3, 0.2, 0.1]):
        super(BaselineMLP, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_rates[0]),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rates[1]),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(dropout_rates[2]),
            
            nn.Linear(32, 2)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)

class ModerateMLP(nn.Module):
    """ä¸­ç­‰ç‰ˆ: 30â†’192â†’96â†’48â†’2 (çº¦12kå‚æ•°)"""
    
    def __init__(self, input_dim=30, dropout_rates=[0.3, 0.25, 0.2, 0.1]):
        super(ModerateMLP, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(input_dim, 192),
            nn.BatchNorm1d(192),
            nn.ReLU(),
            nn.Dropout(dropout_rates[0]),
            
            nn.Linear(192, 96),
            nn.BatchNorm1d(96),
            nn.ReLU(),
            nn.Dropout(dropout_rates[1]),
            
            nn.Linear(96, 48),
            nn.BatchNorm1d(48),
            nn.ReLU(),
            nn.Dropout(dropout_rates[2]),
            
            nn.Linear(48, 24),
            nn.BatchNorm1d(24),
            nn.ReLU(),
            nn.Dropout(dropout_rates[3]),
            
            nn.Linear(24, 2)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)

class DeepMLP(nn.Module):
    """å¤æ‚ç‰ˆ: 30â†’256â†’128â†’64â†’32â†’2 (çº¦25kå‚æ•°)"""
    
    def __init__(self, input_dim=30, dropout_rates=[0.35, 0.3, 0.25, 0.2, 0.1]):
        super(DeepMLP, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(dropout_rates[0]),
            
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_rates[1]),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rates[2]),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(dropout_rates[3]),
            
            nn.Linear(32, 16),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(dropout_rates[4]),
            
            nn.Linear(16, 2)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)

# ========== ä¿®æ­£çš„Label SmoothingæŸå¤±å‡½æ•° ==========
class LabelSmoothingCrossEntropy(nn.Module):
    """Label Smoothingäº¤å‰ç†µæŸå¤±"""
    def __init__(self, num_classes=2, smoothing=0.1, class_weights=None):
        super(LabelSmoothingCrossEntropy, self).__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.class_weights = class_weights
        
    def forward(self, pred, target):
        log_prob = F.log_softmax(pred, dim=1)
        
        # Label smoothing
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.num_classes - 1))
            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)
        
        loss = torch.mean(torch.sum(-true_dist * log_prob, dim=1))
        
        # ç±»åˆ«æƒé‡
        if self.class_weights is not None:
            weights = self.class_weights[target]
            loss = loss * weights.mean()
        
        return loss

# ========== ä¿®æ­£çš„Dropoutè°ƒåº¦å™¨ ==========
class ImprovedDropoutScheduler:
    """æ”¹è¿›çš„Dropoutè°ƒåº¦å™¨ - åªåœ¨éªŒè¯æŒ‡æ ‡æ”¹å–„æ—¶é™ä½dropout"""
    def __init__(self, model, initial_rates, min_rates=None, decay_factor=0.9):
        self.model = model
        self.initial_rates = initial_rates.copy()
        self.min_rates = min_rates or [rate * 0.6 for rate in initial_rates]
        self.decay_factor = decay_factor
        self.current_rates = initial_rates.copy()
        self.last_improvement_epoch = 0
        
    def step(self, epoch, val_improving=True):
        """åªåœ¨éªŒè¯æŒ‡æ ‡æŒç»­æ”¹å–„ä¸”ç»è¿‡è¶³å¤Ÿè½®æ•°æ—¶æ‰é™ä½dropout"""
        if val_improving:
            self.last_improvement_epoch = epoch
        
        # åªæœ‰åœ¨æŒç»­æ”¹å–„ä¸”ç»è¿‡çƒ­èº«æœŸåæ‰é™ä½dropout
        epochs_since_improvement = epoch - self.last_improvement_epoch
        if val_improving and epoch > 20 and epochs_since_improvement < 5:
            for i, (current, minimum) in enumerate(zip(self.current_rates, self.min_rates)):
                self.current_rates[i] = max(current * self.decay_factor, minimum)
            self._apply_dropout_rates()
    
    def _apply_dropout_rates(self):
        """åº”ç”¨æ–°çš„dropoutç‡åˆ°æ¨¡å‹"""
        dropout_idx = 0
        for module in self.model.modules():
            if isinstance(module, nn.Dropout):
                if dropout_idx < len(self.current_rates):
                    module.p = self.current_rates[dropout_idx]
                    dropout_idx += 1

# ========== ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨ ==========
class CosineAnnealingWarmupScheduler:
    """å¸¦çƒ­èº«çš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    def __init__(self, optimizer, warmup_epochs, max_epochs, eta_min=0, warmup_start_lr=1e-6):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.eta_min = eta_min
        self.warmup_start_lr = warmup_start_lr
        self.base_lr = optimizer.param_groups[0]['lr']
        
    def step(self, epoch):
        if epoch < self.warmup_epochs:
            lr = self.warmup_start_lr + (self.base_lr - self.warmup_start_lr) * epoch / self.warmup_epochs
        else:
            progress = (epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)
            lr = self.eta_min + 0.5 * (self.base_lr - self.eta_min) * (1 + math.cos(math.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        return lr

# ========== æ—©åœæ³• ==========
class EarlyStopping:
    """ç®€åŒ–çš„æ—©åœæ³•"""
    def __init__(self, patience=15, min_delta=0.001, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_score = None
        self.counter = 0
        self.best_weights = None
        self.best_epoch = 0
        
    def __call__(self, epoch, score, model):
        is_better = False
        
        if self.best_score is None:
            self.best_score = score
            self.best_epoch = epoch
            is_better = True
        elif score > self.best_score + self.min_delta:
            self.best_score = score
            self.best_epoch = epoch
            self.counter = 0
            is_better = True
        else:
            self.counter += 1
            
        if is_better and self.restore_best_weights:
            self.save_checkpoint(model)
            
        should_stop = self.counter >= self.patience
        if should_stop and self.restore_best_weights and self.best_weights is not None:
            model.load_state_dict(self.best_weights)
            
        return should_stop, is_better
    
    def save_checkpoint(self, model):
        self.best_weights = {k: v.clone() for k, v in model.state_dict().items()}

# ========== ç‰¹å¾é€‰æ‹© ==========
def feature_selection_analysis(X, y, feature_names, top_k=20):
    """ç‰¹å¾é‡è¦æ€§åˆ†æå’Œé€‰æ‹©"""
    print(f"\nğŸ” ç‰¹å¾é€‰æ‹©åˆ†æ (é€‰æ‹©å‰{top_k}ä¸ªç‰¹å¾)...")
    
    # ä½¿ç”¨Fç»Ÿè®¡é‡å’Œäº’ä¿¡æ¯ä¸¤ç§æ–¹æ³•
    print("è®¡ç®—Fç»Ÿè®¡é‡é‡è¦æ€§...")
    f_selector = SelectKBest(f_classif, k=top_k)
    X_f_selected = f_selector.fit_transform(X, y)
    f_scores = f_selector.scores_
    f_selected_features = [feature_names[i] for i in f_selector.get_support(indices=True)]
    
    print("è®¡ç®—äº’ä¿¡æ¯é‡è¦æ€§...")
    mi_selector = SelectKBest(mutual_info_classif, k=top_k)
    X_mi_selected = mi_selector.fit_transform(X, y)
    mi_scores = mi_selector.scores_
    mi_selected_features = [feature_names[i] for i in mi_selector.get_support(indices=True)]
    
    # æ‰¾åˆ°ä¸¤ç§æ–¹æ³•éƒ½é€‰ä¸­çš„ç‰¹å¾
    common_features = list(set(f_selected_features) & set(mi_selected_features))
    
    print(f"\nğŸ“Š ç‰¹å¾é€‰æ‹©ç»“æœ:")
    print(f"Fç»Ÿè®¡é‡é€‰æ‹©: {len(f_selected_features)} ä¸ªç‰¹å¾")
    print(f"äº’ä¿¡æ¯é€‰æ‹©: {len(mi_selected_features)} ä¸ªç‰¹å¾")
    print(f"å…±åŒé€‰æ‹©: {len(common_features)} ä¸ªç‰¹å¾")
    
    # æ˜¾ç¤ºå‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾
    f_importance = list(zip(feature_names, f_scores))
    f_importance.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\nğŸ† Fç»Ÿè®¡é‡ Top 10 ç‰¹å¾:")
    for i, (name, score) in enumerate(f_importance[:10]):
        print(f"  {i+1:2d}. {name:30} | F-Score: {score:8.2f}")
    
    # è¿”å›å…±åŒç‰¹å¾çš„ç´¢å¼•ï¼Œå¦‚æœä¸å¤Ÿåˆ™è¡¥å……Fç»Ÿè®¡é‡é«˜çš„
    if len(common_features) >= top_k // 2:
        selected_indices = [i for i, name in enumerate(feature_names) if name in common_features]
    else:
        selected_indices = f_selector.get_support(indices=True)
    
    return selected_indices[:top_k], [feature_names[i] for i in selected_indices[:top_k]]

# ========== å¿«é€Ÿè¶…å‚æ•°æœç´¢ ==========
def quick_hyperparameter_search(X, y, feature_names, model_configs):
    """ç”¨3æŠ˜CVå¿«é€Ÿç­›é€‰æœ€ä½³é…ç½®"""
    print(f"\nâš¡ å¿«é€Ÿè¶…å‚æ•°æœç´¢ (3æŠ˜CVé¢„ç­›é€‰)...")
    
    # é…ç½®å€™é€‰
    search_configs = [
        {'weight_decay': 1e-4, 'lr': 0.001, 'smoothing': 0.05},
        {'weight_decay': 5e-4, 'lr': 0.001, 'smoothing': 0.1},
        {'weight_decay': 1e-3, 'lr': 0.0008, 'smoothing': 0.1},
        {'weight_decay': 1e-4, 'lr': 0.0012, 'smoothing': 0.15},
    ]
    
    best_results = {}
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = np.bincount(y)
    class_weights = torch.FloatTensor(len(y) / (len(class_counts) * class_counts))
    
    for model_name, model_class in model_configs.items():
        print(f"\nğŸ¯ æµ‹è¯•æ¨¡å‹: {model_name}")
        model_results = []
        
        for i, config in enumerate(search_configs):
            print(f"  é…ç½® {i+1}/{len(search_configs)}: {config}")
            
            # 3æŠ˜äº¤å‰éªŒè¯
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
            fold_scores = []
            
            for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                
                # è½¬æ¢ä¸ºtensor
                X_train_tensor = torch.FloatTensor(X_train.values)
                y_train_tensor = torch.LongTensor(y_train)
                X_val_tensor = torch.FloatTensor(X_val.values)
                y_val_tensor = torch.LongTensor(y_val)
                
                # æ•°æ®åŠ è½½å™¨
                train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
                val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
                train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
                val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
                
                # æ¨¡å‹è®­ç»ƒ
                model = model_class(input_dim=X.shape[1]).to(device)
                fold_f1 = train_quick_model(model, train_loader, val_loader, config, class_weights)
                fold_scores.append(fold_f1)
            
            avg_f1 = np.mean(fold_scores)
            model_results.append({
                'config': config,
                'f1_mean': avg_f1,
                'f1_std': np.std(fold_scores)
            })
            
            print(f"    å¹³å‡F1: {avg_f1:.4f} Â± {np.std(fold_scores):.4f}")
        
        # æ‰¾åˆ°è¯¥æ¨¡å‹çš„æœ€ä½³é…ç½®
        best_config = max(model_results, key=lambda x: x['f1_mean'])
        best_results[model_name] = best_config
        
        print(f"\nâœ… {model_name} æœ€ä½³é…ç½®:")
        print(f"   F1: {best_config['f1_mean']:.4f} Â± {best_config['f1_std']:.4f}")
        print(f"   é…ç½®: {best_config['config']}")
    
    return best_results

def train_quick_model(model, train_loader, val_loader, config, class_weights, max_epochs=50):
    """å¿«é€Ÿè®­ç»ƒæ¨¡å‹ç”¨äºè¶…å‚æ•°æœç´¢"""
    
    criterion = LabelSmoothingCrossEntropy(
        num_classes=2, smoothing=config['smoothing'], class_weights=class_weights.to(device)
    )
    optimizer = optim.AdamW(
        model.parameters(), lr=config['lr'], weight_decay=config['weight_decay']
    )
    scheduler = CosineAnnealingWarmupScheduler(
        optimizer, warmup_epochs=5, max_epochs=max_epochs, eta_min=config['lr']*0.01
    )
    
    early_stopping = EarlyStopping(patience=8, min_delta=0.001)
    
    best_f1 = 0
    
    for epoch in range(max_epochs):
        # è®­ç»ƒ
        model.train()
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        
        # éªŒè¯
        model.eval()
        val_predictions, val_targets = [], []
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                _, predicted = torch.max(outputs, 1)
                val_predictions.extend(predicted.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        val_f1 = f1_score(val_targets, val_predictions, average='weighted')
        best_f1 = max(best_f1, val_f1)
        
        scheduler.step(epoch)
        
        should_stop, _ = early_stopping(epoch, val_f1, model)
        if should_stop:
            break
    
    return best_f1

# ========== å®Œæ•´è®­ç»ƒå‡½æ•° ==========
def train_optimized_model(model, train_loader, val_loader, config, class_weights, epochs=100):
    """ä¼˜åŒ–çš„è®­ç»ƒå‡½æ•°"""
    
    print(f"ğŸš€ å¼€å§‹ä¼˜åŒ–è®­ç»ƒ...")
    print(f"   æ¨¡å‹: {model.__class__.__name__}")
    print(f"   é…ç½®: {config}")
    
    criterion = LabelSmoothingCrossEntropy(
        num_classes=2, smoothing=config['smoothing'], class_weights=class_weights.to(device)
    )
    optimizer = optim.AdamW(
        model.parameters(), lr=config['lr'], weight_decay=config['weight_decay']
    )
    scheduler = CosineAnnealingWarmupScheduler(
        optimizer, warmup_epochs=10, max_epochs=epochs, eta_min=config['lr']*0.01
    )
    
    # Dropoutè°ƒåº¦å™¨
    if hasattr(model, 'network'):
        initial_dropout = [0.3, 0.25, 0.2, 0.1]  # æ ¹æ®æ¨¡å‹è°ƒæ•´
        dropout_scheduler = ImprovedDropoutScheduler(model, initial_dropout)
    else:
        dropout_scheduler = None
    
    early_stopping = EarlyStopping(patience=15, min_delta=0.001)
    
    train_losses = []
    val_f1_scores = []
    learning_rates = []
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_predictions, val_targets = [], []
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                _, predicted = torch.max(outputs, 1)
                val_predictions.extend(predicted.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        val_f1 = f1_score(val_targets, val_predictions, average='weighted')
        avg_train_loss = train_loss / len(train_loader)
        
        train_losses.append(avg_train_loss)
        val_f1_scores.append(val_f1)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        current_lr = scheduler.step(epoch)
        learning_rates.append(current_lr)
        
        # Dropoutè°ƒåº¦
        if dropout_scheduler is not None:
            is_improving = len(val_f1_scores) < 2 or val_f1 > max(val_f1_scores[:-1])
            dropout_scheduler.step(epoch, is_improving)
        
        # æ—©åœæ£€æŸ¥
        should_stop, is_improving = early_stopping(epoch, val_f1, model)
        if should_stop:
            print(f"Early stopping at epoch {epoch+1}, best epoch: {early_stopping.best_epoch+1}")
            break
        
        if epoch % 15 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], '
                  f'Train Loss: {avg_train_loss:.4f}, '
                  f'Val F1: {val_f1:.4f}, '
                  f'LR: {current_lr:.2e}')
    
    return train_losses, val_f1_scores, learning_rates

# ========== ä¸»å‡½æ•° ==========
def main_optimized():
    """ä¼˜åŒ–ç‰ˆä¸»å‡½æ•° - æ¸è¿›å¼æ”¹è¿›"""
    
    print("="*80)
    print("ğŸ¯ MLPæ¸è¿›å¼ä¼˜åŒ–è®­ç»ƒ")
    print("="*80)
    
    # 1. æ•°æ®åŠ è½½
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    data_path = "/Users/mannormal/4011/Qi Zihan/feature_extraction/generated_features/all_features_complete.csv"
    df = pd.read_csv(data_path)
    
    train_path = "/Users/mannormal/4011/Qi Zihan/original_data/train_acc.csv"
    train_df = pd.read_csv(train_path)
    
    test_path = "/Users/mannormal/4011/Qi Zihan/original_data/test_acc_predict.csv"
    test_df = pd.read_csv(test_path)
    
    # å¤„ç†è®­ç»ƒæ•°æ®
    train_accounts = set(train_df['account'])
    df_train = df[df['account'].isin(train_accounts)].copy()
    df_train = df_train.merge(train_df[['account', 'flag']], on='account', how='inner')
    df_train['label'] = df_train['flag']
    
    print(f"è®­ç»ƒæ•°æ®: {df_train.shape}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(df_train['label'])}")
    
    # 2. ç‰¹å¾é¢„å¤„ç†
    feature_cols = [col for col in df.columns if col != 'account']
    from mlp import preprocess_features  # å¤ç”¨åŸæœ‰é¢„å¤„ç†
    X_train, scaler = preprocess_features(df_train, feature_cols)
    y_train = df_train['label'].values
    
    print(f"é¢„å¤„ç†åç‰¹å¾æ•°: {X_train.shape[1]}")
    
    # 3. ç‰¹å¾é€‰æ‹©åˆ†æ
    selected_indices, selected_features = feature_selection_analysis(
        X_train.values, y_train, X_train.columns.tolist(), top_k=25
    )
    
    # åˆ›å»ºç‰¹å¾é€‰æ‹©ç‰ˆæœ¬çš„æ•°æ®
    X_train_selected = X_train.iloc[:, selected_indices]
    print(f"ç‰¹å¾é€‰æ‹©å: {X_train_selected.shape[1]} ä¸ªç‰¹å¾")
    
    # 4. å®šä¹‰æ¨¡å‹é…ç½® - æ¸è¿›å¼å¤æ‚åº¦
    model_configs = {
        'baseline': BaselineMLP,     # ~4k å‚æ•°
        'moderate': ModerateMLP,     # ~12k å‚æ•°
        'deep': DeepMLP,            # ~25k å‚æ•°
    }
    
    # 5. Phase 1: å¿«é€Ÿè¶…å‚æ•°æœç´¢ (3æŠ˜CV)
    print(f"\n{'='*60}")
    print("ğŸ”¬ Phase 1: å¿«é€Ÿè¶…å‚æ•°æœç´¢")
    print(f"{'='*60}")
    
    # æµ‹è¯•å…¨ç‰¹å¾ç‰ˆæœ¬
    print("\nğŸ“Š æµ‹è¯•å…¨ç‰¹å¾ç‰ˆæœ¬...")
    best_configs_full = quick_hyperparameter_search(X_train, y_train, X_train.columns.tolist(), model_configs)
    
    # æµ‹è¯•ç‰¹å¾é€‰æ‹©ç‰ˆæœ¬
    print("\nğŸ“Š æµ‹è¯•ç‰¹å¾é€‰æ‹©ç‰ˆæœ¬...")
    best_configs_selected = quick_hyperparameter_search(X_train_selected, y_train, selected_features, model_configs)
    
    # 6. Phase 2: é€‰æ‹©æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´10æŠ˜CV
    print(f"\n{'='*60}")
    print("ğŸ¯ Phase 2: æœ€ä½³é…ç½®çš„å®Œæ•´è¯„ä¼°")
    print(f"{'='*60}")
    
    # æ¯”è¾ƒå…¨ç‰¹å¾å’Œç‰¹å¾é€‰æ‹©çš„æœ€ä½³ç»“æœ
    best_full = max(best_configs_full.values(), key=lambda x: x['f1_mean'])
    best_selected = max(best_configs_selected.values(), key=lambda x: x['f1_mean'])
    
    print(f"\nğŸ“Š 3æŠ˜CVé¢„ç­›é€‰ç»“æœå¯¹æ¯”:")
    print(f"å…¨ç‰¹å¾æœ€ä½³: F1={best_full['f1_mean']:.4f}")
    print(f"ç‰¹å¾é€‰æ‹©æœ€ä½³: F1={best_selected['f1_mean']:.4f}")
    
    # é€‰æ‹©æ›´å¥½çš„ç‰ˆæœ¬è¿›è¡Œæœ€ç»ˆè®­ç»ƒ
    if best_selected['f1_mean'] > best_full['f1_mean']:
        print("âœ… é€‰æ‹©ç‰¹å¾é€‰æ‹©ç‰ˆæœ¬è¿›è¡Œæœ€ç»ˆè®­ç»ƒ")
        X_final = X_train_selected
        final_configs = best_configs_selected
        final_features = selected_features
    else:
        print("âœ… é€‰æ‹©å…¨ç‰¹å¾ç‰ˆæœ¬è¿›è¡Œæœ€ç»ˆè®­ç»ƒ")
        X_final = X_train
        final_configs = best_configs_full
        final_features = X_train.columns.tolist()
    
    # æ‰¾åˆ°æœ€ä½³æ¨¡å‹å’Œé…ç½®
    best_model_name = max(final_configs.keys(), key=lambda k: final_configs[k]['f1_mean'])
    best_config = final_configs[best_model_name]['config']
    best_model_class = model_configs[best_model_name]
    
    print(f"\nğŸ† æœ€ç»ˆé€‰æ‹©:")
    print(f"   æ¨¡å‹: {best_model_name}")
    print(f"   ç‰¹å¾æ•°: {X_final.shape[1]}")
    print(f"   é…ç½®: {best_config}")
    print(f"   é¢„æœŸF1: {final_configs[best_model_name]['f1_mean']:.4f}")
    
    # 7. 10æŠ˜äº¤å‰éªŒè¯æœ€ç»ˆè¯„ä¼°
    print(f"\nğŸ”„ 10æŠ˜äº¤å‰éªŒè¯æœ€ç»ˆè¯„ä¼°...")
    
    class_counts = np.bincount(y_train)
    class_weights = torch.FloatTensor(len(y_train) / (len(class_counts) * class_counts))
    
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    fold_results = []
    
    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_final, y_train)):
        print(f"\n--- Fold {fold_idx+1}/10 ---")
        
        X_fold_train, X_fold_val = X_final.iloc[train_idx], X_final.iloc[val_idx]
        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]
        
        # æ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(
            torch.FloatTensor(X_fold_train.values),
            torch.LongTensor(y_fold_train)
        )
        val_dataset = TensorDataset(
            torch.FloatTensor(X_fold_val.values),
            torch.LongTensor(y_fold_val)
        )
        
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        
        # è®­ç»ƒæ¨¡å‹
        model = best_model_class(input_dim=X_final.shape[1]).to(device)
        train_losses, val_f1_scores, learning_rates = train_optimized_model(
            model, train_loader, val_loader, best_config, class_weights, epochs=120
        )
        
        best_f1 = max(val_f1_scores)
        fold_results.append({
            'fold': fold_idx + 1,
            'f1': best_f1,
            'epochs': len(train_losses)
        })
        
        print(f"Fold {fold_idx+1} æœ€ä½³F1: {best_f1:.4f}")
    
    # 8. æœ€ç»ˆç»“æœ
    final_f1_scores = [r['f1'] for r in fold_results]
    mean_f1 = np.mean(final_f1_scores)
    std_f1 = np.std(final_f1_scores)
    
    print(f"\nğŸ‰ æœ€ç»ˆ10æŠ˜CVç»“æœ:")
    print(f"   å¹³å‡F1: {mean_f1:.4f} Â± {std_f1:.4f}")
    print(f"   å„æŠ˜F1: {[f'{f:.4f}' for f in final_f1_scores]}")
    print(f"   å¹³å‡è®­ç»ƒè½®æ•°: {np.mean([r['epochs'] for r in fold_results]):.1f}")
    
    # 9. è®­ç»ƒæœ€ç»ˆæ¨¡å‹å¹¶é¢„æµ‹
    print(f"\nğŸš€ è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    
    final_model = best_model_class(input_dim=X_final.shape[1]).to(device)
    
    # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
    full_dataset = TensorDataset(
        torch.FloatTensor(X_final.values),
        torch.LongTensor(y_train)
    )
    full_loader = DataLoader(full_dataset, batch_size=64, shuffle=True)
    
    # è®­ç»ƒæœ€ä¼˜è½®æ•°
    optimal_epochs = int(np.mean([r['epochs'] for r in fold_results]))
    
    criterion = LabelSmoothingCrossEntropy(
        num_classes=2, smoothing=best_config['smoothing'], class_weights=class_weights.to(device)
    )
    optimizer = optim.AdamW(
        final_model.parameters(), lr=best_config['lr'], weight_decay=best_config['weight_decay']
    )
    scheduler = CosineAnnealingWarmupScheduler(
        optimizer, warmup_epochs=5, max_epochs=optimal_epochs, eta_min=best_config['lr']*0.01
    )
    
    print(f"è®­ç»ƒè½®æ•°: {optimal_epochs}")
    final_model.train()
    
    for epoch in range(optimal_epochs):
        epoch_loss = 0.0
        for batch_X, batch_y in full_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = final_model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
        
        scheduler.step(epoch)
        
        if epoch % 20 == 0:
            print(f'Epoch [{epoch+1}/{optimal_epochs}], Loss: {epoch_loss/len(full_loader):.4f}')
    
    print("âœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ!")
    
    # 10. é¢„æµ‹æµ‹è¯•é›†
    print(f"\nğŸ”® é¢„æµ‹æµ‹è¯•é›†...")
    
    # å¤„ç†æµ‹è¯•é›† - ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾é€‰æ‹©
    df_test = df[df['account'].isin(set(test_df['account']))].copy()
    X_test = df_test[feature_cols].copy()
    X_test = X_test[X_train.columns]  # ä¿æŒä¸€è‡´æ€§
    
    # é¢„å¤„ç†
    for col in X_test.columns:
        if 'profit' in col:
            X_test[col] = np.sign(X_test[col]) * np.log1p(np.abs(X_test[col]))
            Q01 = X_train[col].quantile(0.01)
            Q99 = X_train[col].quantile(0.99)
            X_test[col] = np.clip(X_test[col], Q01, Q99)
        elif 'ratio' in col:
            X_test[col] = np.clip(X_test[col], 0, 50)
    
    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test),
        columns=X_test.columns,
        index=X_test.index
    )
    
    # åº”ç”¨ç›¸åŒçš„ç‰¹å¾é€‰æ‹©
    if X_final.shape[1] != X_train.shape[1]:  # å¦‚æœä½¿ç”¨äº†ç‰¹å¾é€‰æ‹©
        X_test_final = X_test_scaled.iloc[:, selected_indices]
    else:
        X_test_final = X_test_scaled
    
    # é¢„æµ‹
    final_model.eval()
    X_test_tensor = torch.FloatTensor(X_test_final.values).to(device)
    
    with torch.no_grad():
        outputs = final_model(X_test_tensor)
        _, predictions = torch.max(outputs, 1)
    
    predictions = predictions.cpu().numpy()
    
    # ä¿å­˜ç»“æœ
    result_dir = "/Users/mannormal/4011/Qi Zihan/result_analysis/prediction_results/"
    os.makedirs(result_dir, exist_ok=True)
    
    submission_df = pd.DataFrame({
        'account': df_test['account'].values,
        'Predict': predictions
    })
    
    filename = f"MLP_OPTIMIZED_{best_model_name}_f1_{mean_f1:.4f}_features_{X_final.shape[1]}.csv"
    submission_df.to_csv(os.path.join(result_dir, filename), index=False)
    
    print(f"\nğŸ‰ ä¼˜åŒ–ç‰ˆMLPå®Œæˆ!")
    print(f"ğŸ“Š æœ€ä½³æ¨¡å‹: {best_model_name}")
    print(f"ğŸ”§ ç‰¹å¾æ•°: {X_final.shape[1]}")
    print(f"ğŸ“ˆ 10æŠ˜CV F1: {mean_f1:.4f} Â± {std_f1:.4f}")
    print(f"ğŸ“ æ–‡ä»¶: {filename}")
    print(f"ğŸ“Š é¢„æµ‹åˆ†å¸ƒ: {np.bincount(predictions)}")
    
    return {
        'model_name': best_model_name,
        'config': best_config,
        'cv_f1': mean_f1,
        'cv_std': std_f1,
        'feature_count': X_final.shape[1],
        'selected_features': final_features,
        'submission': submission_df
    }

if __name__ == "__main__":
    results = main_optimized()