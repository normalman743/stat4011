import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
import numpy as np
import math
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import os
from datetime import datetime

# è®¾ç½®è®¾å¤‡
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç  - ä¸ºæ¯ä¸ªç‰¹å¾ä½ç½®æ·»åŠ ç¼–ç """
    
    def __init__(self, d_model, max_len=50):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        # x shape: [seq_len, batch_size, d_model]
        return x + self.pe[:x.size(0), :]

class FeatureEmbedding(nn.Module):
    """ç‰¹å¾åµŒå…¥å±‚ - å°†æ¯ä¸ªç‰¹å¾æ˜ å°„åˆ°d_modelç»´åº¦"""
    
    def __init__(self, input_dim, d_model, dropout=0.1):
        super(FeatureEmbedding, self).__init__()
        self.d_model = d_model
        self.input_dim = input_dim
        
        # æ¯ä¸ªç‰¹å¾ä¸€ä¸ªçº¿æ€§å˜æ¢
        self.feature_projections = nn.ModuleList([
            nn.Linear(1, d_model) for _ in range(input_dim)
        ])
        
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)
        
    def forward(self, x):
        # x shape: [batch_size, input_dim]
        batch_size = x.size(0)
        
        # å°†æ¯ä¸ªç‰¹å¾æŠ•å½±åˆ°d_modelç»´åº¦
        feature_embeddings = []
        for i in range(self.input_dim):
            feature_val = x[:, i:i+1]  # [batch_size, 1]
            embedded = self.feature_projections[i](feature_val)  # [batch_size, d_model]
            feature_embeddings.append(embedded)
        
        # Stack to create sequence: [input_dim, batch_size, d_model]
        embeddings = torch.stack(feature_embeddings, dim=0)
        
        # Apply layer norm and dropout
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        
        return embeddings

class TransformerBlock(nn.Module):
    """æ ‡å‡†Transformerç¼–ç å™¨å—"""
    
    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):
        super(TransformerBlock, self).__init__()
        
        self.self_attention = nn.MultiheadAttention(
            embed_dim=d_model, 
            num_heads=nhead, 
            dropout=dropout,
            batch_first=False  # ä½¿ç”¨ [seq_len, batch, embed_dim] æ ¼å¼
        )
        
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, dim_feedforward),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward, d_model)
        )
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # x shape: [seq_len, batch_size, d_model]
        
        # Self-attention with residual connection
        attn_output, _ = self.self_attention(x, x, x)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed forward with residual connection
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class FinancialTransformer(nn.Module):
    """é‡‘èè´¦æˆ·åˆ†ç±»Transformeræ¨¡å‹"""
    
    def __init__(self, input_dim=30, d_model=128, nhead=8, num_layers=3, 
                 dim_feedforward=256, dropout=0.1, num_classes=2):
        super(FinancialTransformer, self).__init__()
        
        self.input_dim = input_dim
        self.d_model = d_model
        
        # ç‰¹å¾åµŒå…¥
        self.feature_embedding = FeatureEmbedding(input_dim, d_model, dropout)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_len=input_dim)
        
        # Transformerç¼–ç å™¨å±‚
        self.transformer_layers = nn.ModuleList([
            TransformerBlock(d_model, nhead, dim_feedforward, dropout)
            for _ in range(num_layers)
        ])
        
        # å…¨å±€æ± åŒ–ç­–ç•¥
        self.pooling_strategy = "mean"  # å¯é€‰: "mean", "max", "cls"
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, d_model // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 4, num_classes)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        # x shape: [batch_size, input_dim]
        
        # ç‰¹å¾åµŒå…¥: [input_dim, batch_size, d_model]
        embedded = self.feature_embedding(x)
        
        # ä½ç½®ç¼–ç 
        embedded = self.pos_encoding(embedded)
        
        # é€šè¿‡Transformerå±‚
        for transformer_layer in self.transformer_layers:
            embedded = transformer_layer(embedded)
        
        # å…¨å±€æ± åŒ–: [input_dim, batch_size, d_model] -> [batch_size, d_model]
        if self.pooling_strategy == "mean":
            pooled = embedded.mean(dim=0)  # å¯¹åºåˆ—ç»´åº¦æ±‚å¹³å‡
        elif self.pooling_strategy == "max":
            pooled, _ = embedded.max(dim=0)  # å¯¹åºåˆ—ç»´åº¦æ±‚æœ€å¤§å€¼
        else:  # "cls" - ä½¿ç”¨ç¬¬ä¸€ä¸ªä½ç½®
            pooled = embedded[0]
        
        # åˆ†ç±»
        output = self.classifier(pooled)
        
        return output
    
    def get_attention_weights(self, x):
        """è·å–æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§£é‡Šæ€§åˆ†æ"""
        
        embedded = self.feature_embedding(x)
        embedded = self.pos_encoding(embedded)
        
        attention_weights = []
        
        for transformer_layer in self.transformer_layers:
            # æ‰‹åŠ¨è®¡ç®—æ³¨æ„åŠ›æƒé‡
            attn_output, attn_weights = transformer_layer.self_attention(
                embedded, embedded, embedded
            )
            attention_weights.append(attn_weights.detach())
            
            # ç»§ç»­å‰å‘ä¼ æ’­
            embedded = transformer_layer.norm1(embedded + transformer_layer.dropout(attn_output))
            ff_output = transformer_layer.feed_forward(embedded)
            embedded = transformer_layer.norm2(embedded + transformer_layer.dropout(ff_output))
        
        return attention_weights

class AdvancedFinancialTransformer(nn.Module):
    """é«˜çº§ç‰ˆæœ¬ï¼šåŠ å…¥ç‰¹å¾ç±»å‹æ„ŸçŸ¥çš„Transformer"""
    
    def __init__(self, input_dim=30, d_model=128, nhead=8, num_layers=3,
                 dim_feedforward=256, dropout=0.1, num_classes=2):
        super(AdvancedFinancialTransformer, self).__init__()
        
        self.input_dim = input_dim
        self.d_model = d_model
        
        # ç‰¹å¾ç±»å‹åµŒå…¥ï¼ˆä¸ºä¸åŒç±»å‹çš„ç‰¹å¾å­¦ä¹ ä¸åŒçš„åµŒå…¥ï¼‰
        self.feature_type_embedding = nn.Embedding(4, d_model // 4)  # 4ç§ç‰¹å¾ç±»å‹
        
        # ç‰¹å¾å€¼åµŒå…¥
        self.feature_value_projection = nn.Linear(1, d_model - d_model // 4)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_len=input_dim)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='relu',
            batch_first=False
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=num_layers
        )
        
        # æ³¨æ„åŠ›æ± åŒ–
        self.attention_pooling = nn.Sequential(
            nn.Linear(d_model, 1),
            nn.Softmax(dim=0)
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, num_classes)
        )
        
        # é¢„å®šä¹‰ç‰¹å¾ç±»å‹æ˜ å°„ - æ³¨å†Œä¸ºbuffer
        feature_type_map = self._create_feature_type_map()
        self.register_buffer('feature_type_map', feature_type_map)
    
    def _create_feature_type_map(self):
        """åˆ›å»ºç‰¹å¾ç±»å‹æ˜ å°„"""
        # 0: degreeç±», 1: profitç±», 2: sizeç±», 3: length/countç±»
        feature_types = []
        
        # å‡è®¾30ä¸ªç‰¹å¾çš„ç±»å‹åˆ†å¸ƒï¼ˆéœ€è¦æ ¹æ®å®é™…ç‰¹å¾è°ƒæ•´ï¼‰
        for i in range(self.input_dim):
            if i < 3:  # degreeç›¸å…³
                feature_types.append(0)
            elif i < 18:  # profitç›¸å…³
                feature_types.append(1)
            elif i < 24:  # sizeç›¸å…³
                feature_types.append(2)
            else:  # length/countç›¸å…³
                feature_types.append(3)
        
        return torch.tensor(feature_types, dtype=torch.long)
    
    def forward(self, x):
        # x shape: [batch_size, input_dim]
        batch_size = x.size(0)
        
        # åˆ›å»ºåµŒå…¥
        embeddings = []
        for i in range(self.input_dim):
            # ç‰¹å¾å€¼åµŒå…¥
            feature_val = x[:, i:i+1]  # [batch_size, 1]
            value_embed = self.feature_value_projection(feature_val)  # [batch_size, d_model-d_model//4]
            
            # ç‰¹å¾ç±»å‹åµŒå…¥
            feature_type = self.feature_type_map[i]
            type_embed = self.feature_type_embedding(feature_type)  # [d_model//4]
            type_embed = type_embed.unsqueeze(0).expand(batch_size, -1)  # [batch_size, d_model//4]
            
            # è¿æ¥å€¼åµŒå…¥å’Œç±»å‹åµŒå…¥
            combined_embed = torch.cat([value_embed, type_embed], dim=1)  # [batch_size, d_model]
            embeddings.append(combined_embed)
        
        # Stackä¸ºåºåˆ—: [input_dim, batch_size, d_model]
        embedded = torch.stack(embeddings, dim=0)
        
        # ä½ç½®ç¼–ç 
        embedded = self.pos_encoding(embedded)
        
        # Transformerç¼–ç 
        encoded = self.transformer_encoder(embedded)
        
        # æ³¨æ„åŠ›æ± åŒ–
        attention_weights = self.attention_pooling(encoded)  # [input_dim, batch_size, 1]
        weighted_features = encoded * attention_weights  # å¹¿æ’­ä¹˜æ³•
        pooled = weighted_features.sum(dim=0)  # [batch_size, d_model]
        
        # åˆ†ç±»
        output = self.classifier(pooled)
        
        return output

class EarlyStopping:
    """æ—©åœæ³•ç±»"""
    def __init__(self, patience=10, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_score = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, score, model):
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                if self.restore_best_weights:
                    model.load_state_dict(self.best_weights)
                return True
        else:
            self.best_score = score
            self.counter = 0
            self.save_checkpoint(model)
        return False
    
    def save_checkpoint(self, model):
        self.best_weights = model.state_dict().copy()

def preprocess_features(df, feature_cols, show_details=True):
    """æ•°æ®é¢„å¤„ç† - å¢å¼ºç‰ˆå¸¦è¯¦ç»†æ‰“å°"""
    
    print("=" * 60)
    print("ğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    print(f"åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
    
    X = df[feature_cols].copy()
    
    # ===== 1. æ£€æŸ¥å’Œåˆ é™¤å¸¸æ•°ç‰¹å¾ =====
    print("\nğŸ“Š æ£€æŸ¥å¸¸æ•°ç‰¹å¾...")
    constant_features = []
    feature_stats = {}
    
    for col in X.columns:
        unique_count = X[col].nunique()
        unique_values = X[col].unique()
        
        feature_stats[col] = {
            'unique_count': unique_count,
            'unique_values': unique_values[:5] if len(unique_values) > 5 else unique_values  # åªæ˜¾ç¤ºå‰5ä¸ª
        }
        
        if unique_count <= 1:
            constant_features.append(col)
            print(f"  âŒ {col}: åªæœ‰ {unique_count} ä¸ªå”¯ä¸€å€¼ -> {unique_values}")
    
    if constant_features:
        print(f"\nğŸ—‘ï¸  åˆ é™¤ {len(constant_features)} ä¸ªå¸¸æ•°ç‰¹å¾: {constant_features}")
        X = X.drop(columns=constant_features)
    else:
        print("âœ… æ²¡æœ‰å‘ç°å¸¸æ•°ç‰¹å¾")
    
    # ===== 2. æ˜¾ç¤ºæ•°æ®èŒƒå›´æƒ…å†µ =====
    print(f"\nğŸ“ˆ åŸå§‹æ•°æ®èŒƒå›´åˆ†æ:")
    print("-" * 50)
    
    extreme_features = []
    for col in X.columns:
        min_val, max_val = X[col].min(), X[col].max()
        std_val = X[col].std()
        
        print(f"{col:25} | èŒƒå›´: [{min_val:>12.2e}, {max_val:>12.2e}] | æ ‡å‡†å·®: {std_val:>10.2e}")
        
        # æ£€æŸ¥æç«¯å€¼
        if max_val > 1e10 or min_val < -1e10:
            extreme_features.append(col)
    
    if extreme_features:
        print(f"\nâš ï¸  å‘ç°æç«¯æ•°å€¼ç‰¹å¾: {extreme_features}")
    
    # ===== 3. å¤„ç†æç«¯å¼‚å¸¸å€¼ =====
    print(f"\nğŸ› ï¸  å¤„ç†å¼‚å¸¸å€¼...")
    processing_log = {}
    
    for col in X.columns:
        original_min, original_max = X[col].min(), X[col].max()
        
        if 'profit' in col:
            print(f"\n  å¤„ç†é‡‘é¢ç‰¹å¾: {col}")
            
            # Step 1: logå˜æ¢
            print(f"    åŸå§‹èŒƒå›´: [{original_min:.2e}, {original_max:.2e}]")
            X[col] = np.sign(X[col]) * np.log1p(np.abs(X[col]))
            log_min, log_max = X[col].min(), X[col].max()
            print(f"    Logå˜æ¢å: [{log_min:.4f}, {log_max:.4f}]")
            
            # Step 2: clipå¼‚å¸¸å€¼
            Q01 = X[col].quantile(0.01)
            Q99 = X[col].quantile(0.99)
            print(f"    1%åˆ†ä½æ•°: {Q01:.4f}, 99%åˆ†ä½æ•°: {Q99:.4f}")
            
            # ç»Ÿè®¡ä¼šè¢«clipçš„æ•°æ®
            will_be_clipped = ((X[col] < Q01) | (X[col] > Q99)).sum()
            clip_percentage = will_be_clipped / len(X) * 100
            
            X[col] = np.clip(X[col], Q01, Q99)
            print(f"    ClipåèŒƒå›´: [{X[col].min():.4f}, {X[col].max():.4f}]")
            print(f"    è¢«è£å‰ªæ•°æ®: {will_be_clipped}/{len(X)} ({clip_percentage:.2f}%)")
            
            processing_log[col] = {
                'type': 'profit',
                'original_range': (original_min, original_max),
                'log_range': (log_min, log_max),
                'clip_range': (Q01, Q99),
                'clipped_count': will_be_clipped
            }
            
        elif 'ratio' in col:
            print(f"\n  å¤„ç†æ¯”ä¾‹ç‰¹å¾: {col}")
            print(f"    åŸå§‹èŒƒå›´: [{original_min:.4f}, {original_max:.4f}]")
            
            # ç»Ÿè®¡ä¼šè¢«clipçš„æ•°æ®
            will_be_clipped = ((X[col] < 0) | (X[col] > 50)).sum()
            clip_percentage = will_be_clipped / len(X) * 100
            
            X[col] = np.clip(X[col], 0, 50)
            print(f"    Clipåˆ°[0, 50]: [{X[col].min():.4f}, {X[col].max():.4f}]")
            print(f"    è¢«è£å‰ªæ•°æ®: {will_be_clipped}/{len(X)} ({clip_percentage:.2f}%)")
            
            processing_log[col] = {
                'type': 'ratio',
                'original_range': (original_min, original_max),
                'clip_range': (0, 50),
                'clipped_count': will_be_clipped
            }
        
        else:
            # å…¶ä»–ç‰¹å¾ä¸å¤„ç†ï¼Œåªè®°å½•
            processing_log[col] = {
                'type': 'other',
                'original_range': (original_min, original_max)
            }
    
    # ===== 4. æ ‡å‡†åŒ– =====
    print(f"\nğŸ“ åº”ç”¨RobustScaleræ ‡å‡†åŒ–...")
    scaler = RobustScaler()
    
    # æ˜¾ç¤ºæ ‡å‡†åŒ–å‰åçš„ç»Ÿè®¡
    print("æ ‡å‡†åŒ–å‰åå¯¹æ¯” (å‰5ä¸ªç‰¹å¾):")
    print("-" * 60)
    
    for i, col in enumerate(X.columns[:5]):
        before_mean, before_std = X[col].mean(), X[col].std()
        print(f"  {col:20} | å‡å€¼: {before_mean:>8.3f} | æ ‡å‡†å·®: {before_std:>8.3f}")
    
    X_scaled = pd.DataFrame(
        scaler.fit_transform(X), 
        columns=X.columns, 
        index=X.index
    )
    
    print("\næ ‡å‡†åŒ–å:")
    for i, col in enumerate(X_scaled.columns[:5]):
        after_mean, after_std = X_scaled[col].mean(), X_scaled[col].std()
        print(f"  {col:20} | å‡å€¼: {after_mean:>8.3f} | æ ‡å‡†å·®: {after_std:>8.3f}")
    
    # ===== 5. æœ€ç»ˆæ‘˜è¦ =====
    print("\n" + "=" * 60)
    print("ğŸ“‹ é¢„å¤„ç†å®Œæˆæ‘˜è¦:")
    print(f"  â€¢ åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
    print(f"  â€¢ åˆ é™¤å¸¸æ•°ç‰¹å¾: {len(constant_features)}")
    print(f"  â€¢ æœ€ç»ˆç‰¹å¾æ•°: {X_scaled.shape[1]}")
    print(f"  â€¢ æ ·æœ¬æ•°: {X_scaled.shape[0]}")
    
    # ç»Ÿè®¡å„ç±»å¤„ç†çš„ç‰¹å¾æ•°
    profit_features = [k for k, v in processing_log.items() if v.get('type') == 'profit']
    ratio_features = [k for k, v in processing_log.items() if v.get('type') == 'ratio']
    other_features = [k for k, v in processing_log.items() if v.get('type') == 'other']
    
    print(f"  â€¢ é‡‘é¢ç‰¹å¾(log+clip): {len(profit_features)}")
    print(f"  â€¢ æ¯”ä¾‹ç‰¹å¾(clip): {len(ratio_features)}")
    print(f"  â€¢ å…¶ä»–ç‰¹å¾: {len(other_features)}")
    
    print("=" * 60)
    
    return X_scaled, scaler

def train_transformer_model(model, train_loader, val_loader, epochs=100, learning_rate=0.001, 
                           weight_decay=1e-4, class_weights=None):
    """è®­ç»ƒTransformeræ¨¡å‹"""
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    if class_weights is not None:
        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))
    else:
        criterion = nn.CrossEntropyLoss()
    
    # Transformeré€šå¸¸éœ€è¦æ›´å°çš„å­¦ä¹ ç‡
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate*0.5, weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6
    )
    
    # æ—©åœæ³•
    early_stopping = EarlyStopping(patience=10, min_delta=0.001)
    
    train_losses = []
    val_f1_scores = []
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆTransformerè®­ç»ƒä¸­å¾ˆæœ‰ç”¨ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            train_loss += loss.item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                _, predicted = torch.max(outputs, 1)
                
                val_predictions.extend(predicted.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        # è®¡ç®—F1åˆ†æ•°
        val_f1 = f1_score(val_targets, val_predictions, average='weighted')
        
        train_losses.append(train_loss / len(train_loader))
        val_f1_scores.append(val_f1)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_f1)
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_f1, model):
            print(f"Early stopping at epoch {epoch+1}")
            break
        
        if epoch % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], '
                  f'Train Loss: {train_losses[-1]:.4f}, '
                  f'Val F1: {val_f1:.4f}, '
                  f'LR: {optimizer.param_groups[0]["lr"]:.2e}')
    
    return train_losses, val_f1_scores

def cross_validation_training(X, y, model_type="basic", cv_folds=10, epochs=100):
    """äº¤å‰éªŒè¯è®­ç»ƒ - æ”¯æŒTransformeræ¨¡å‹"""
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = np.bincount(y)
    class_weights = torch.FloatTensor(len(y) / (len(class_counts) * class_counts))
    print(f"ç±»åˆ«æƒé‡: {class_weights}")
    
    # åˆ†å±‚äº¤å‰éªŒè¯
    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    all_folds = list(skf.split(X, y))
    
    print(f"ğŸ“Š ç­–ç•¥: {cv_folds}æŠ˜äº¤å‰éªŒè¯ï¼Œæ¨¡å‹ç±»å‹: {model_type}")
    print(f"   æ¯æŠ˜éªŒè¯é›†å¤§å°: ~{len(y)//cv_folds} ({100/cv_folds:.1f}%)")
    print(f"   æ¯æŠ˜è®­ç»ƒé›†å¤§å°: ~{len(y)*(cv_folds-1)//cv_folds} ({100*(cv_folds-1)/cv_folds:.1f}%)")
    
    fold_results = []
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒå…¨éƒ¨ {cv_folds} æŠ˜...")
    
    for fold_idx in range(cv_folds):
        train_idx, val_idx = all_folds[fold_idx]
        print(f"\n=== Fold {fold_idx+1}/{cv_folds} ===")
        
        # åˆ†å‰²æ•°æ®
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        print(f"è®­ç»ƒé›†: {len(X_train)}, éªŒè¯é›†: {len(X_val)}")
        
        # è½¬æ¢ä¸ºtensor
        X_train_tensor = torch.FloatTensor(X_train.values)
        y_train_tensor = torch.LongTensor(y_train)
        X_val_tensor = torch.FloatTensor(X_val.values)
        y_val_tensor = torch.LongTensor(y_val)
        
        # åˆ›å»ºDataLoader
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        
        # åˆå§‹åŒ–æ¨¡å‹
        if model_type == "basic":
            model = FinancialTransformer(
                input_dim=X.shape[1], 
                d_model=128, 
                nhead=8, 
                num_layers=3,
                dropout=0.1
            ).to(device)
        elif model_type == "advanced":
            model = AdvancedFinancialTransformer(
                input_dim=X.shape[1], 
                d_model=128, 
                nhead=8, 
                num_layers=3,
                dropout=0.1
            ).to(device)
        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        # è®­ç»ƒæ¨¡å‹
        train_losses, val_f1_scores = train_transformer_model(
            model, train_loader, val_loader, 
            epochs=epochs, class_weights=class_weights
        )
        
        # è·å–æ—©åœè½®æ•°å’Œæœ€ä½³F1åˆ†æ•°
        actual_epochs = len(train_losses)
        fold_best_f1 = max(val_f1_scores) if val_f1_scores else 0
        
        # è·å–æœ€ç»ˆé¢„æµ‹
        model.eval()
        with torch.no_grad():
            outputs = model(X_val_tensor.to(device))
            _, predictions = torch.max(outputs, 1)
            predictions = predictions.cpu().numpy()
        
        # è®¡ç®—æœ€ç»ˆè¯„ä¼°F1
        final_eval_f1 = f1_score(y_val, predictions, average='weighted')
        
        print(f"ğŸ¯ æœ€ä½³éªŒè¯F1: {fold_best_f1:.4f}")
        print(f"æœ€ç»ˆè¯„ä¼°F1: {final_eval_f1:.4f}")
        
        # å­˜å‚¨ç»“æœ
        fold_results.append({
            'fold_idx': fold_idx,
            'f1_score': fold_best_f1,
            'epochs': actual_epochs,
            'predictions': predictions,
            'val_indices': val_idx,
            'train_size': len(X_train),
            'val_size': len(X_val),
            'final_eval_f1': final_eval_f1
        })
        
        print(f"Fold {fold_idx+1} F1 Score: {fold_best_f1:.4f}, Early stopped at epoch: {actual_epochs}")
    
    # é€‰æ‹©æœ€ä½³fold
    best_fold = max(fold_results, key=lambda x: x['f1_score'])
    best_fold_idx = best_fold['fold_idx']
    
    print(f"\nğŸ† æœ€ä½³è¡¨ç°: Fold {best_fold_idx+1}")
    print(f"   F1åˆ†æ•°: {best_fold['f1_score']:.4f}")
    print(f"   æ—©åœè½®æ•°: {best_fold['epochs']}")
    print(f"   è®­ç»ƒé›†å¤§å°: {best_fold['train_size']}")
    print(f"   éªŒè¯é›†å¤§å°: {best_fold['val_size']}")
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ
    fold_f1_scores = [r['f1_score'] for r in fold_results]
    fold_predictions = []
    fold_indices = []
    
    for result in fold_results:
        fold_predictions.extend(result['predictions'])
        fold_indices.extend(result['val_indices'])
    
    mean_f1 = np.mean(fold_f1_scores)
    std_f1 = np.std(fold_f1_scores)
    optimal_epochs = best_fold['epochs']
    
    print(f"\n=== äº¤å‰éªŒè¯ç»“æœæ‘˜è¦ ===")
    print(f"å…¨éƒ¨{cv_folds}æŠ˜F1åˆ†æ•°: {fold_f1_scores}")
    print(f"å„æŠ˜æ—©åœè½®æ•°: {[r['epochs'] for r in fold_results]}")
    print(f"å¹³å‡F1åˆ†æ•°: {mean_f1:.4f} Â± {std_f1:.4f}")
    print(f"ğŸ¯ é€‰ç”¨æœ€ä¼˜è®­ç»ƒè½®æ•°: {optimal_epochs} (æ¥è‡ªæœ€ä½³Fold {best_fold_idx+1})")
    
    return mean_f1, fold_f1_scores, fold_predictions, fold_indices, optimal_epochs, fold_results, best_fold

def train_final_model(X_train, y_train, optimal_epochs, model_type="basic"):
    """è®­ç»ƒæœ€ç»ˆTransformeræ¨¡å‹"""
    
    print(f"\nğŸš€ è®­ç»ƒæœ€ç»ˆ{model_type} Transformeræ¨¡å‹ (ä½¿ç”¨å…¨éƒ¨{len(y_train)}ä¸ªè®­ç»ƒæ ·æœ¬)...")
    print(f"ğŸ¯ æœ€ä¼˜è®­ç»ƒè½®æ•°: {optimal_epochs}")
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = np.bincount(y_train)
    class_weights = torch.FloatTensor(len(y_train) / (len(class_counts) * class_counts))
    print(f"ç±»åˆ«æƒé‡: {class_weights}")
    
    # è½¬æ¢ä¸ºtensor
    X_train_tensor = torch.FloatTensor(X_train.values)
    y_train_tensor = torch.LongTensor(y_train)
    
    # åˆ›å»ºDataLoader
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    
    # åˆå§‹åŒ–æ¨¡å‹
    if model_type == "basic":
        model = FinancialTransformer(
            input_dim=X_train.shape[1], 
            d_model=128, 
            nhead=8, 
            num_layers=3,
            dropout=0.1
        ).to(device)
    elif model_type == "advanced":
        model = AdvancedFinancialTransformer(
            input_dim=X_train.shape[1], 
            d_model=128, 
            nhead=8, 
            num_layers=3,
            dropout=0.1
        ).to(device)
    
    # è®­ç»ƒè®¾ç½®
    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))
    optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)
    
    print(f"å¼€å§‹è®­ç»ƒæœ€ç»ˆæ¨¡å‹ ({optimal_epochs} epochs, æ— éªŒè¯é›†)...")
    model.train()
    
    for epoch in range(optimal_epochs):
        epoch_loss = 0.0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / len(train_loader)
        
        if epoch % 5 == 0:
            print(f'Epoch [{epoch+1}/{optimal_epochs}], Training Loss: {avg_loss:.4f}')
    
    print("âœ… æœ€ç»ˆTransformeræ¨¡å‹è®­ç»ƒå®Œæˆ!")
    return model

def predict_test_set(model, X_test, test_accounts):
    """å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹"""
    
    print("\nğŸ”® å¼€å§‹é¢„æµ‹æµ‹è¯•é›†...")
    
    model.eval()
    X_test_tensor = torch.FloatTensor(X_test.values).to(device)
    
    with torch.no_grad():
        outputs = model(X_test_tensor)
        probabilities = torch.softmax(outputs, dim=1)
        _, predictions = torch.max(outputs, 1)
    
    predictions = predictions.cpu().numpy()
    probabilities = probabilities.cpu().numpy()
    
    # åˆ›å»ºé¢„æµ‹ç»“æœDataFrame
    test_results = pd.DataFrame({
        'account': test_accounts,
        'predicted_label': predictions,
        'probability_good': probabilities[:, 0],
        'probability_bad': probabilities[:, 1]
    })
    
    print(f"æµ‹è¯•é›†é¢„æµ‹å®Œæˆ: {len(test_results)} ä¸ªè´¦æˆ·")
    print(f"é¢„æµ‹ä¸ºBadçš„è´¦æˆ·: {(predictions == 1).sum()} ({(predictions == 1).mean()*100:.2f}%)")
    
    return test_results

def analyze_attention_patterns(model, X_sample, feature_names):
    """åˆ†ææ³¨æ„åŠ›æ¨¡å¼ï¼Œäº†è§£æ¨¡å‹å…³æ³¨å“ªäº›ç‰¹å¾"""
    
    model.eval()
    
    if hasattr(model, 'get_attention_weights'):
        try:
            attention_weights = model.get_attention_weights(X_sample.to(device))
            
            # åˆ†æç¬¬ä¸€å±‚çš„æ³¨æ„åŠ›æƒé‡
            first_layer_attention = attention_weights[0]  # [batch_size, num_heads, seq_len, seq_len]
            
            # å¹³å‡æ‰€æœ‰headå’Œbatch
            avg_attention = first_layer_attention.mean(dim=(0, 1)).cpu().numpy()  # [seq_len, seq_len]
            
            # å¯è§†åŒ–æ³¨æ„åŠ›çŸ©é˜µ
            plt.figure(figsize=(12, 10))
            sns.heatmap(avg_attention, 
                        xticklabels=feature_names[:len(avg_attention)], 
                        yticklabels=feature_names[:len(avg_attention)],
                        annot=False, cmap='Blues')
            plt.title('Feature-to-Feature Attention Weights (Transformer)')
            plt.tight_layout()
            plt.show()
            
            return avg_attention
        except Exception as e:
            print(f"æ³¨æ„åŠ›åˆ†æå‡ºé”™: {e}")
            return None
    else:
        print("æ¨¡å‹ä¸æ”¯æŒæ³¨æ„åŠ›æƒé‡æå–")
        return None

def main():
    """ä¸»å‡½æ•° - Transformerç‰ˆæœ¬"""
    
    # 1. åŠ è½½æ‰€æœ‰æ•°æ®
    print("ğŸ”„ åŠ è½½æ•°æ®...")
    data_path = "/Users/mannormal/4011/Qi Zihan/feature_extraction/generated_features/all_features_complete.csv"
    df = pd.read_csv(data_path)
    
    # åŠ è½½è®­ç»ƒæ ‡ç­¾
    train_path = "/Users/mannormal/4011/Qi Zihan/original_data/train_acc.csv"
    train_df = pd.read_csv(train_path)
    
    # åŠ è½½æµ‹è¯•è´¦æˆ·
    test_path = "/Users/mannormal/4011/Qi Zihan/original_data/test_acc_predict.csv"
    test_df = pd.read_csv(test_path)
    
    print(f"ç‰¹å¾æ•°æ®: {df.shape}")
    print(f"è®­ç»ƒæ ‡ç­¾: {train_df.shape}")
    print(f"æµ‹è¯•è´¦æˆ·: {test_df.shape}")
    
    # 2. åˆ†ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_accounts = set(train_df['account'])
    test_accounts = set(test_df['account'])
    
    # è®­ç»ƒé›†ï¼šæœ‰æ ‡ç­¾çš„è´¦æˆ·
    df_train = df[df['account'].isin(train_accounts)].copy()
    df_train = df_train.merge(train_df[['account', 'flag']], on='account', how='inner')
    df_train['label'] = df_train['flag']
    
    # æµ‹è¯•é›†ï¼šéœ€è¦é¢„æµ‹çš„è´¦æˆ·
    df_test = df[df['account'].isin(test_accounts)].copy()
    
    print(f"è®­ç»ƒé›†å¤§å°: {df_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {df_test.shape}")
    print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {df_train['label'].value_counts()}")
    
    # 3. ç‰¹å¾å·¥ç¨‹
    feature_cols = [col for col in df.columns if col != 'account']
    print(f"\nğŸ“Š å¼€å§‹å¤„ç†è®­ç»ƒé›†ç‰¹å¾...")
    X_train, scaler = preprocess_features(df_train, feature_cols)
    y_train = df_train['label'].values
    
    print(f"è®­ç»ƒé›†å¤„ç†åç‰¹å¾æ•°: {X_train.shape[1]}")
    
    # 4. é€‰æ‹©æ¨¡å‹ç±»å‹
    model_type = "advanced"  # å¯ä»¥æ”¹ä¸º "advanced"
    print(f"\nğŸ—ï¸ é€‰æ‹©æ¨¡å‹ç±»å‹: {model_type}")
    
    # 5. äº¤å‰éªŒè¯è¯„ä¼°
    print("\n" + "="*60)
    print("ğŸ“Š å¼€å§‹Transformeräº¤å‰éªŒè¯è¯„ä¼°...")
    mean_f1, fold_f1_scores, cv_predictions, cv_indices, optimal_epochs, fold_results, best_fold = cross_validation_training(
        X_train, y_train, model_type=model_type, cv_folds=10, epochs=100
    )
    
    # 6. å¤„ç†æµ‹è¯•é›†ç‰¹å¾
    print(f"\nğŸ“Š å¼€å§‹å¤„ç†æµ‹è¯•é›†ç‰¹å¾...")
    X_test = df_test[feature_cols].copy()
    X_test = X_test[X_train.columns]  # ä¿æŒç‰¹å¾ä¸€è‡´æ€§
    
    # åº”ç”¨ç›¸åŒçš„é¢„å¤„ç†æ­¥éª¤
    for col in X_test.columns:
        if 'profit' in col:
            X_test[col] = np.sign(X_test[col]) * np.log1p(np.abs(X_test[col]))
            Q01 = X_train[col].quantile(0.01) if col in X_train.columns else X_test[col].min()
            Q99 = X_train[col].quantile(0.99) if col in X_train.columns else X_test[col].max()
            X_test[col] = np.clip(X_test[col], Q01, Q99)
        elif 'ratio' in col:
            X_test[col] = np.clip(X_test[col], 0, 50)
    
    # åº”ç”¨è®­ç»ƒå¥½çš„scaler
    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test),
        columns=X_test.columns,
        index=X_test.index
    )
    
    print(f"æµ‹è¯•é›†å¤„ç†åå½¢çŠ¶: {X_test_scaled.shape}")
    
    # 7. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    final_model = train_final_model(X_train, y_train, optimal_epochs, model_type=model_type)
    
    # 8. é¢„æµ‹æµ‹è¯•é›†
    test_results = predict_test_set(final_model, X_test_scaled, df_test['account'].values)
    
    # 9. æ³¨æ„åŠ›åˆ†æï¼ˆå¦‚æœæ”¯æŒï¼‰
    if hasattr(final_model, 'get_attention_weights'):
        print("\nğŸ” è¿›è¡Œæ³¨æ„åŠ›åˆ†æ...")
        sample_data = torch.FloatTensor(X_train.iloc[:10].values)
        attention_matrix = analyze_attention_patterns(final_model, sample_data, X_train.columns.tolist())
    
    # 10. ä¿å­˜ç»“æœ
    result_dir = "/Users/mannormal/4011/Qi Zihan/result_analysis/prediction_results/"
    os.makedirs(result_dir, exist_ok=True)
    
    # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
    fold_f1_mapping = []
    for result in fold_results:
        fold_size = len(result['predictions'])
        fold_f1_mapping.extend([result['f1_score']] * fold_size)
    
    cv_results_df = pd.DataFrame({
        'account': df_train.iloc[cv_indices]['account'].values,
        'true_label': df_train.iloc[cv_indices]['label'].values,
        'predicted_label': cv_predictions,
        'fold_f1_score': fold_f1_mapping
    })
    
    cv_filename = f"Transformer_{model_type}_cv_analysis_f1_score_{mean_f1:.4f}.csv"
    cv_results_df.to_csv(os.path.join(result_dir, cv_filename), index=False)
    
    # ä¿å­˜å…¨éƒ¨æ•°æ®è®­ç»ƒçš„æäº¤æ–‡ä»¶
    submission_df_full = pd.DataFrame({
        'account': test_results['account'],
        'Predict': test_results['predicted_label']
    })
    
    test_filename_full = f"Transformer_{model_type}_submission_FULL_DATA_f1_{mean_f1:.4f}_epochs_{optimal_epochs}.csv"
    submission_df_full.to_csv(os.path.join(result_dir, test_filename_full), index=False)
    
    # ä½¿ç”¨æœ€ä½³Foldé‡æ–°è®­ç»ƒå’Œé¢„æµ‹
    print(f"\nğŸ† ä½¿ç”¨æœ€ä½³Fold {best_fold['fold_idx']+1} çš„è®¾ç½®é‡æ–°é¢„æµ‹æµ‹è¯•é›†...")
    
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    all_folds = list(skf.split(X_train, y_train))
    best_train_idx, best_val_idx = all_folds[best_fold['fold_idx']]
    
    X_best_train = X_train.iloc[best_train_idx]
    y_best_train = y_train[best_train_idx]
    
    best_fold_model = train_final_model(X_best_train, y_best_train, best_fold['epochs'], model_type=model_type)
    best_fold_test_results = predict_test_set(best_fold_model, X_test_scaled, df_test['account'].values)
    
    submission_df_best = pd.DataFrame({
        'account': best_fold_test_results['account'],
        'Predict': best_fold_test_results['predicted_label']
    })
    
    test_filename_best = f"Transformer_{model_type}_submission_BEST_FOLD_{best_fold['fold_idx']+1}_f1_{best_fold['f1_score']:.4f}_epochs_{best_fold['epochs']}.csv"
    submission_df_best.to_csv(os.path.join(result_dir, test_filename_best), index=False)
    
    print(f"\nğŸ‰ Transformerè®­ç»ƒå®Œæˆï¼ç”Ÿæˆäº†ä¸¤ä¸ªæäº¤æ–‡ä»¶ï¼š")
    print(f"ğŸ“Š äº¤å‰éªŒè¯å¹³å‡F1: {mean_f1:.4f}")
    print(f"ğŸ† æœ€ä½³Fold F1: {best_fold['f1_score']:.4f}")
    print(f"ğŸ¤– æ¨¡å‹ç±»å‹: {model_type} Transformer")
    print(f"ğŸ“ äº¤å‰éªŒè¯åˆ†ææ–‡ä»¶: {cv_filename}")
    print(f"ğŸ“„ å…¨éƒ¨æ•°æ®æäº¤æ–‡ä»¶: {test_filename_full}")
    print(f"ğŸ¥‡ æœ€ä½³Foldæäº¤æ–‡ä»¶: {test_filename_best}")
    
    # å¯¹æ¯”ä¸¤ä¸ªé¢„æµ‹ç»“æœ
    print(f"\nğŸ“Š ä¸¤ä¸ªæäº¤æ–‡ä»¶å¯¹æ¯”:")
    print(f"å…¨éƒ¨æ•°æ®æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ: {submission_df_full['Predict'].value_counts().to_dict()}")
    print(f"æœ€ä½³Foldæ¨¡å‹é¢„æµ‹åˆ†å¸ƒ: {submission_df_best['Predict'].value_counts().to_dict()}")
    
    # è®¡ç®—ä¸¤ä¸ªé¢„æµ‹çš„ä¸€è‡´æ€§
    agreement = (submission_df_full['Predict'] == submission_df_best['Predict']).mean()
    print(f"ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§: {agreement:.4f} ({agreement*100:.2f}%)")
    
    return mean_f1, cv_results_df, submission_df_full, submission_df_best, best_fold

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨Transformeré‡‘èè´¦æˆ·åˆ†ç±»ç³»ç»Ÿ...")
    mean_f1_score, cv_results, submission_full, submission_best, best_fold_info = main()