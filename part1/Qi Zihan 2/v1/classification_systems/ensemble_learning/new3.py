import pandas as pd
import numpy as np
import os
from tqdm import tqdm
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import warnings
warnings.filterwarnings('ignore')

print("=== ULTRA Multi-Strategy Ensemble System with Optimized Meta-ANN ===")

# =====================================================
# ä¼˜åŒ–çš„Meta-ANNç±»å®šä¹‰
# =====================================================
class RefinedMetaANN(nn.Module):
    """ä¼˜åŒ–çš„Meta-ANNï¼Œèåˆbase predictionså’Œoriginal features"""
    
    def __init__(self, n_base_models, n_original_features, dropout_rates=[0.3, 0.2, 0.2, 0.1, 0.1]):
        super(RefinedMetaANN, self).__init__()
        
        # è¾“å…¥ç»´åº¦ = base model predictions + original features
        input_dim = n_base_models + n_original_features
        
        # å¯è®­ç»ƒçš„ç‰¹å¾ç¼©æ”¾å‚æ•°
        self.feature_scaler = nn.Parameter(torch.ones(n_original_features))
        self.feature_bias = nn.Parameter(torch.zeros(n_original_features))
        
        # æ›´æ·±çš„ç½‘ç»œæ¶æ„
        self.network = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_rates[0]),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rates[1]),
            
            nn.Linear(64, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rates[2]),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(dropout_rates[3]),
            
            nn.Linear(32, 16),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(dropout_rates[4]),
            
            nn.Linear(16, 2)  # è¾“å‡º2ä¸ªç±»åˆ«çš„logits
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x_base, x_feat):
        # ç‰¹å¾ç¼©æ”¾
        x_feat_scaled = self.feature_scaler * x_feat + self.feature_bias
        
        # èåˆbase predictionså’Œscaled features
        x = torch.cat([x_base, x_feat_scaled], dim=1)
        
        # é€šè¿‡ç½‘ç»œ
        return self.network(x)

class LightLabelSmoothingCE(nn.Module):
    """è½»é‡çº§Label Smoothingäº¤å‰ç†µæŸå¤±"""
    def __init__(self, smoothing=0.03, class_weights=None):
        super(LightLabelSmoothingCE, self).__init__()
        self.smoothing = smoothing
        self.class_weights = class_weights
        
    def forward(self, pred, target):
        log_prob = F.log_softmax(pred, dim=1)
        
        with torch.no_grad():
            smooth_target = torch.zeros_like(pred)
            smooth_target.fill_(self.smoothing / (pred.size(1) - 1))
            smooth_target.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)
        
        loss = -torch.sum(smooth_target * log_prob, dim=1)
        
        if self.class_weights is not None:
            weights = self.class_weights[target]
            loss = loss * weights
        
        return loss.mean()

class SimpleEarlyStopping:
    """ç®€å•çš„æ—©åœæœºåˆ¶ï¼Œä¸“æ³¨äºBadå®¢æˆ·F1"""
    def __init__(self, patience=15, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_score = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, score, model):
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model)
            return False
        
        if score > self.best_score + self.min_delta:
            self.best_score = score
            self.counter = 0
            self.save_checkpoint(model)
            return False
        else:
            self.counter += 1
            if self.counter >= self.patience:
                if self.best_weights is not None:
                    model.load_state_dict(self.best_weights)
                return True
        return False
    
    def save_checkpoint(self, model):
        self.best_weights = {k: v.clone() for k, v in model.state_dict().items()}

def train_refined_meta_ann(base_predictions, original_features, y_true, n_epochs=500, patience=20):
    """
    è®­ç»ƒä¼˜åŒ–çš„Meta-ANNï¼Œé‡ç‚¹å…³æ³¨Badå®¢æˆ·F1åˆ†æ•°ï¼ŒåŒ…å«é˜ˆå€¼ä¼˜åŒ–
    
    Args:
        base_predictions: (n_samples, n_models) - åŸºç¡€æ¨¡å‹é¢„æµ‹æ¦‚ç‡
        original_features: (n_samples, n_features) - åŸå§‹ç‰¹å¾
        y_true: (n_samples,) - çœŸå®æ ‡ç­¾ (0: Good, 1: Bad)
    
    Returns:
        è®­ç»ƒå¥½çš„æ¨¡å‹ã€é¢„æµ‹ç»“æœå’Œè¯¦ç»†æŒ‡æ ‡
    """
    print(f"\nğŸ¤– è®­ç»ƒä¼˜åŒ–Meta-ANN (é‡ç‚¹: Badå®¢æˆ·F1)")
    print(f"Base predictions shape: {base_predictions.shape}")
    print(f"Original features shape: {original_features.shape}")
    print(f"Label distribution: {dict(zip(*np.unique(y_true, return_counts=True)))}")
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    original_features_scaled = scaler.fit_transform(original_features)
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_base_tensor = torch.tensor(base_predictions, dtype=torch.float32).to(device)
    X_feat_tensor = torch.tensor(original_features_scaled, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y_true, dtype=torch.long).to(device)  # æ³¨æ„è¿™é‡Œç”¨LongTensor
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = np.bincount(y_true)
    class_weights = torch.FloatTensor(len(y_true) / (len(class_counts) * class_counts)).to(device)
    print(f"Class weights: {class_weights}")
    
    # åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹
    model = RefinedMetaANN(
        n_base_models=base_predictions.shape[1], 
        n_original_features=original_features_scaled.shape[1],
        dropout_rates=[0.3, 0.2, 0.2, 0.1, 0.1]
    ).to(device)
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)
    criterion = LightLabelSmoothingCE(smoothing=0.03, class_weights=class_weights)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=8, factor=0.5, min_lr=1e-6)
    early_stopping = SimpleEarlyStopping(patience=patience, min_delta=0.001)
    
    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    train_idx, val_idx = list(skf.split(base_predictions, y_true))[0]
    
    Xb_train, Xb_val = X_base_tensor[train_idx], X_base_tensor[val_idx]
    Xf_train, Xf_val = X_feat_tensor[train_idx], X_feat_tensor[val_idx]
    y_train, y_val = y_tensor[train_idx], y_tensor[val_idx]
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(Xb_train, Xf_train, y_train)
    val_dataset = TensorDataset(Xb_val, Xf_val, y_val)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    
    # è®­ç»ƒå†å²è®°å½•
    train_history = []
    best_bad_f1 = 0
    
    print("\nEpoch | Train Loss | Bad F1 | Macro F1 | Accuracy | Good F1 | LR       | Status")
    print("-" * 80)
    
    for epoch in range(n_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        for batch_xb, batch_xf, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_xb, batch_xf)
            loss = criterion(outputs, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for batch_xb, batch_xf, batch_y in val_loader:
                outputs = model(batch_xb, batch_xf)
                _, predicted = torch.max(outputs, 1)
                val_predictions.extend(predicted.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        val_f1_macro = f1_score(val_targets, val_predictions, average='macro', zero_division=0)
        val_accuracy = accuracy_score(val_targets, val_predictions)
        
        # è®¡ç®—å„ç±»åˆ«F1
        val_f1_per_class = f1_score(val_targets, val_predictions, average=None, zero_division=0)
        good_f1 = val_f1_per_class[0]
        bad_f1 = val_f1_per_class[1] if len(val_f1_per_class) > 1 else 0  # ä¸»è¦æŒ‡æ ‡
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(bad_f1)
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®°å½•å†å²
        epoch_data = {
            'epoch': epoch + 1,
            'train_loss': avg_train_loss,
            'bad_f1': bad_f1,
            'macro_f1': val_f1_macro,
            'accuracy': val_accuracy,
            'good_f1': good_f1,
            'lr': current_lr
        }
        train_history.append(epoch_data)
        
        # æ—©åœæ£€æŸ¥
        if bad_f1 > best_bad_f1:
            best_bad_f1 = bad_f1
            status = "âœ… Best"
        else:
            status = f"â³ {early_stopping.counter+1}/{early_stopping.patience}"
        
        # æ‰“å°è¿›åº¦
        if epoch % 20 == 0 or epoch >= n_epochs - 5:
            print(f"{epoch+1:5d} | {avg_train_loss:10.4f} | {bad_f1:6.4f} | {val_f1_macro:8.4f} | "
                  f"{val_accuracy:8.4f} | {good_f1:7.4f} | {current_lr:.2e} | {status}")
        
        if early_stopping(bad_f1, model):
            print(f"\nğŸ›‘ Early stopping at epoch {epoch+1}")
            print(f"ğŸ† Best Bad F1: {best_bad_f1:.4f}")
            break
    
    # =====================================================
    # æœ€ç»ˆè¯„ä¼° - æ·»åŠ é˜ˆå€¼ä¼˜åŒ–
    # =====================================================
    print(f"\n{'='*60}")
    print("ğŸ¯ é˜ˆå€¼ä¼˜åŒ–ä¸æœ€ç»ˆè¯„ä¼°")
    print(f"{'='*60}")
    
    model.eval()
    with torch.no_grad():
        # è·å–æ¦‚ç‡é¢„æµ‹
        final_outputs = model(X_base_tensor, X_feat_tensor)
        final_probs = F.softmax(final_outputs, dim=1)
        final_probs_np = final_probs.cpu().numpy()
        bad_probabilities = final_probs_np[:, 1]
    
    # å¤šç§é˜ˆå€¼ä¼˜åŒ–ç­–ç•¥
    optimization_metrics = ['f1_bad', 'f1_macro', 'precision_recall_balance', 'youden']
    optimal_results = {}
    
    for opt_metric in optimization_metrics:
        print(f"\n--- ä¼˜åŒ–æŒ‡æ ‡: {opt_metric} ---")
        optimal_threshold, optimal_score, threshold_details = find_optimal_threshold(
            y_true, bad_probabilities, metric=opt_metric
        )
        
        # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼é¢„æµ‹
        optimal_predictions = (bad_probabilities >= optimal_threshold).astype(int)
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        final_accuracy = accuracy_score(y_true, optimal_predictions)
        final_f1_macro = f1_score(y_true, optimal_predictions, average='macro', zero_division=0)
        final_f1_weighted = f1_score(y_true, optimal_predictions, average='weighted', zero_division=0)
        
        final_f1_per_class = f1_score(y_true, optimal_predictions, average=None, zero_division=0)
        final_good_f1 = final_f1_per_class[0]
        final_bad_f1 = final_f1_per_class[1] if len(final_f1_per_class) > 1 else 0
        
        precision_per_class = precision_score(y_true, optimal_predictions, average=None, zero_division=0)
        recall_per_class = recall_score(y_true, optimal_predictions, average=None, zero_division=0)
        
        final_good_precision = precision_per_class[0]
        final_good_recall = recall_per_class[0]
        final_bad_precision = precision_per_class[1] if len(precision_per_class) > 1 else 0
        final_bad_recall = recall_per_class[1] if len(recall_per_class) > 1 else 0
        
        optimal_results[opt_metric] = {
            'threshold': optimal_threshold,
            'predictions': optimal_predictions,
            'final_bad_f1': final_bad_f1,
            'final_good_f1': final_good_f1,
            'final_macro_f1': final_f1_macro,
            'final_weighted_f1': final_f1_weighted,
            'final_accuracy': final_accuracy,
            'final_bad_precision': final_bad_precision,
            'final_bad_recall': final_bad_recall,
            'final_good_precision': final_good_precision,
            'final_good_recall': final_good_recall,
            'threshold_details': threshold_details
        }
    
    # é€‰æ‹©æœ€ä½³çš„é˜ˆå€¼ä¼˜åŒ–ç­–ç•¥ (åŸºäºBad F1)
    best_strategy = max(optimal_results.keys(), 
                       key=lambda k: optimal_results[k]['final_bad_f1'])
    best_result = optimal_results[best_strategy]
    
    print(f"\nğŸ† æ¨èçš„æœ€ä½³é˜ˆå€¼ç­–ç•¥: {best_strategy}")
    print(f"   æœ€ä¼˜é˜ˆå€¼: {best_result['threshold']:.3f}")
    print(f"   Badå®¢æˆ·F1: {best_result['final_bad_f1']:.4f}")
    print(f"   Goodå®¢æˆ·F1: {best_result['final_good_f1']:.4f}")
    print(f"   å®å¹³å‡F1: {best_result['final_macro_f1']:.4f}")
    print(f"   æ•´ä½“å‡†ç¡®ç‡: {best_result['final_accuracy']:.4f}")
    print(f"   Badå®¢æˆ·æ£€å‡ºç‡: {best_result['final_bad_recall']:.4f}")
    print(f"   Badå®¢æˆ·é¢„æµ‹å‡†ç¡®ç‡: {best_result['final_bad_precision']:.4f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, best_result['predictions'])
    print(f"\n   ğŸ“‹ æ··æ·†çŸ©é˜µ (æœ€ä¼˜é˜ˆå€¼ {best_result['threshold']:.3f}):")
    print(f"              é¢„æµ‹Good  é¢„æµ‹Bad")
    print(f"   å®é™…Good    {cm[0,0]:6d}   {cm[0,1]:6d}")
    if cm.shape[0] > 1:
        print(f"   å®é™…Bad     {cm[1,0]:6d}   {cm[1,1]:6d}")
    
    return bad_probabilities, model, scaler, {
        'optimal_results': optimal_results,
        'best_strategy': best_strategy,
        'best_threshold': best_result['threshold'],
        'final_bad_f1': best_result['final_bad_f1'],
        'final_good_f1': best_result['final_good_f1'],
        'final_macro_f1': best_result['final_macro_f1'],
        'final_weighted_f1': best_result['final_weighted_f1'],
        'final_accuracy': best_result['final_accuracy'],
        'final_bad_precision': best_result['final_bad_precision'],
        'final_bad_recall': best_result['final_bad_recall'],
        'best_val_bad_f1': best_bad_f1,
        'train_history': train_history,
        'confusion_matrix': cm,
        'best_predictions': best_result['predictions']
    }

# =====================================================
# æ•°æ®åŠ è½½å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
# =====================================================
def load_strategy_categories():
    strategy_paths = {
        'traditional': '/Users/mannormal/4011/Qi Zihan/classification_strategies/traditional_4types/traditional_category_mapping.csv',
        'volume': '/Users/mannormal/4011/Qi Zihan/classification_strategies/volume_based/volume_category_mapping.csv',
        'profit': '/Users/mannormal/4011/Qi Zihan/classification_strategies/profit_based/profit_category_mapping.csv',
        'interaction': '/Users/mannormal/4011/Qi Zihan/classification_strategies/interaction_based/interaction_category_mapping.csv',
        'behavior': '/Users/mannormal/4011/Qi Zihan/classification_strategies/behavior_based/behavior_category_mapping.csv'
    }
    
    strategy_data = {}
    print("\n=== Loading Classification Strategies ===")
    for strategy_name, path in strategy_paths.items():
        if os.path.exists(path):
            strategy_data[strategy_name] = pd.read_csv(path)
            print(f"âœ… {strategy_name}: {len(strategy_data[strategy_name])} accounts")
        else:
            print(f"âŒ {strategy_name}: File not found")
    
    return strategy_data

def classify_account_type_original(row):
    has_forward = (row['normal_fprofit'] > 0 or row['abnormal_fprofit'] > 0 or 
                   row['normal_fsize'] > 0 or row['abnormal_fsize'] > 0)
    has_backward = (row['normal_bprofit'] > 0 or row['abnormal_bprofit'] > 0 or
                    row['normal_bsize'] > 0 or row['abnormal_bsize'] > 0)
    
    if has_forward and has_backward:
        return 'type1'
    elif has_forward and not has_backward:
        return 'type2'
    elif not has_forward and has_backward:
        return 'type3'
    else:
        return 'type4'

# =====================================================
# æ¨¡å‹è®­ç»ƒå‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼Œè¿”å›é¢„æµ‹ï¼‰
# =====================================================
def train_universal_ensemble(data, n_models=50):
    print(f"\n=== Training Universal Ensemble ({n_models} models) ===")
    data_copy = data.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    feature_cols = [col for col in data_copy.columns 
                   if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    sample_size = min(good_accounts, bad_accounts)
    
    predictions = []
    cv_scores = []
    X_all = data_copy[feature_cols].values
    y_all = data_copy['flag'].values
    
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    for i in tqdm(range(n_models), desc="Universal Models"):
        good_sample = data_copy[data_copy['flag'] == 1].sample(n=sample_size, replace=True, random_state=i)
        bad_sample = data_copy[data_copy['flag'] == 0].sample(n=sample_size, replace=True, random_state=i+1000)
        train_data = pd.concat([good_sample, bad_sample], axis=0)
        
        X_train = train_data[feature_cols].values
        y_train = train_data['flag'].values
        
        clf = RandomForestClassifier(
            n_estimators=100, 
            max_depth=15, 
            min_samples_split=10,
            random_state=i
        )
        clf.fit(X_train, y_train)
        
        cv_score = np.mean([clf.score(X_all[train_idx], y_all[train_idx]) 
                           for train_idx, val_idx in skf.split(X_all, y_all)])
        cv_scores.append(cv_score)
        
        y_pred = clf.predict_proba(X_all)[:, 1]  # æ¦‚ç‡é¢„æµ‹æ›´é€‚åˆåš meta-feature
        predictions.append(y_pred)
    
    return np.array(predictions), cv_scores

def train_strategy_ensemble(data, strategy_name, strategy_categories, n_models=10):
    print(f"\n=== Training {strategy_name.upper()} Strategy Ensemble ({n_models} models) ===")
    data_with_strategy = data.merge(strategy_categories, on='account', how='left')
    strategy_col = f"{strategy_name}_category"
    data_with_strategy[strategy_col] = data_with_strategy[strategy_col].fillna('unknown')
    
    data_copy = data_with_strategy.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    
    feature_cols = [col for col in data_copy.columns if col not in ['account', 'flag', 'account_type']]
    strategy_dummies = pd.get_dummies(data_copy[strategy_col], prefix=strategy_name)
    feature_data = pd.concat([
        data_copy[[col for col in feature_cols if not col.endswith('_category')]],
        strategy_dummies
    ], axis=1)
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    sample_size = min(good_accounts, bad_accounts)
    
    print(f"   Balanced sampling: {sample_size} per class")
    print(f"   Features: {feature_data.shape[1]} (base: {len([col for col in feature_cols if not col.endswith('_category')])}, strategy: {len(strategy_dummies.columns)})")
    
    predictions = []
    cv_scores = []
    X_all = feature_data.values
    y_all = data_copy['flag'].values
    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    
    for i in tqdm(range(n_models), desc=f"{strategy_name} Models"):
        good_sample = data_copy[data_copy['flag'] == 1].sample(n=sample_size, replace=True, random_state=i*100)
        bad_sample = data_copy[data_copy['flag'] == 0].sample(n=sample_size, replace=True, random_state=i*100+50)
        sample_indices = list(good_sample.index) + list(bad_sample.index)
        
        X_train = feature_data.loc[sample_indices].values
        y_train = pd.concat([good_sample, bad_sample])['flag'].values
        
        clf = RandomForestClassifier(
            n_estimators=120,
            max_depth=18,
            min_samples_split=8,
            min_samples_leaf=4,
            random_state=i*10,
            class_weight='balanced'
        )
        clf.fit(X_train, y_train)
        
        # è®¡ç®—åˆ†ç±»åˆ«F1åˆ†æ•°
        cv_f1_overall = []
        cv_f1_good = []
        cv_f1_bad = []
        
        for train_idx, val_idx in skf.split(X_all, y_all):
            val_pred = clf.predict(X_all[val_idx])
            f1_overall = metrics.f1_score(y_all[val_idx], val_pred, zero_division=0)
            f1_good = metrics.f1_score(y_all[val_idx], val_pred, pos_label=1, zero_division=0)
            f1_bad = metrics.f1_score(y_all[val_idx], val_pred, pos_label=0, zero_division=0)
            
            cv_f1_overall.append(f1_overall)
            cv_f1_good.append(f1_good)
            cv_f1_bad.append(f1_bad)
        
        cv_score = np.mean(cv_f1_overall)
        cv_scores.append(cv_score)
        
        y_pred = clf.predict_proba(X_all)[:, 1]
        predictions.append(y_pred)
    
    predictions_array = np.array(predictions).T
    
    # è®¡ç®—å¹³å‡åˆ†ç±»åˆ«F1
    avg_f1_overall = np.mean(cv_scores)
    print(f"   Average CV F1 (Overall): {avg_f1_overall:.4f}")
    
    return predictions_array, cv_scores

# =====================================================
# Enhanced Random Forest Ensemble Training
# =====================================================
def train_enhanced_rf_ensemble(data, n_models=100):
    """è®­ç»ƒå¢å¼ºçš„éšæœºæ£®æ—é›†æˆ - ä¼˜åŒ–ç‰ˆæœ¬"""
    print(f"\nğŸŒ³ Training Enhanced RF Ensemble ({n_models} models)")
    
    data_copy = data.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    feature_cols = [col for col in data_copy.columns 
                   if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    sample_size = min(good_accounts, bad_accounts)
    
    print(f"   ğŸ’¡ Data Info:")
    print(f"      Good accounts: {good_accounts}")
    print(f"      Bad accounts: {bad_accounts}")
    print(f"      Balanced sampling: {sample_size} per class")
    print(f"      Features: {len(feature_cols)}")
    print(f"      Imbalance ratio: 1:{bad_accounts//good_accounts}")
    
    X_all = data_copy[feature_cols].values
    y_all = data_copy['flag'].values
    
    predictions = []
    cv_scores = []
    
    # ä¼˜åŒ–çš„éšæœºæ£®æ—é…ç½® - æ›´æ·±æ›´å¤æ‚
    rf_configs = [
        {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 8, 'min_samples_leaf': 3},
        {'n_estimators': 180, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 2},
        {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4},
        {'n_estimators': 220, 'max_depth': 35, 'min_samples_split': 12, 'min_samples_leaf': 5},
    ]
    
    for i in tqdm(range(n_models), desc="RF Models"):
        # æ›´æ¿€è¿›çš„é‡‡æ ·ç­–ç•¥ - å¢åŠ æ ·æœ¬å¤šæ ·æ€§
        bootstrap_ratio = 0.8 + 0.4 * np.random.random()  # 0.8-1.2å€é‡‡æ ·
        actual_sample_size = int(sample_size * bootstrap_ratio)
        
        good_sample = data_copy[data_copy['flag'] == 1].sample(
            n=actual_sample_size, replace=True, random_state=i
        )
        bad_sample = data_copy[data_copy['flag'] == 0].sample(
            n=actual_sample_size, replace=True, random_state=i+3000
        )
        train_data = pd.concat([good_sample, bad_sample], axis=0)
        
        X_train = train_data[feature_cols].values
        y_train = train_data['flag'].values
        
        # å¾ªç¯ä½¿ç”¨ä¸åŒé…ç½®
        config = rf_configs[i % len(rf_configs)]
        
        clf = RandomForestClassifier(
            **config,
            random_state=i,
            class_weight='balanced_subsample',  # æ›´å¥½çš„ç±»å¹³è¡¡
            max_features='sqrt',  # ç‰¹å¾é€‰æ‹©ç­–ç•¥
            bootstrap=True,
            oob_score=True,
            n_jobs=1
        )
        clf.fit(X_train, y_train)
        
        # ä½¿ç”¨Out-of-Bagè¯„ä¼° + äº¤å‰éªŒè¯
        oob_score = clf.oob_score_ if hasattr(clf, 'oob_score_') else 0
        
        # 5æŠ˜äº¤å‰éªŒè¯
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)
        cv_f1_scores = []
        for train_idx, val_idx in skf.split(X_all, y_all):
            val_pred = clf.predict(X_all[val_idx])
            f1 = metrics.f1_score(y_all[val_idx], val_pred, zero_division=0)
            cv_f1_scores.append(f1)
        
        cv_score = np.mean(cv_f1_scores)
        cv_scores.append(cv_score)
        
        # æ¦‚ç‡é¢„æµ‹
        y_pred_proba = clf.predict_proba(X_all)[:, 1]
        predictions.append(y_pred_proba)
    
    predictions_array = np.array(predictions).T  # (n_samples, n_models)
    avg_cv_score = np.mean(cv_scores)
    
    print(f"   ğŸ“Š Results:")
    print(f"      Average CV F1: {avg_cv_score:.4f}")
    print(f"      CV F1 std: {np.std(cv_scores):.4f}")
    print(f"      CV F1 range: [{np.min(cv_scores):.4f}, {np.max(cv_scores):.4f}]")
    
    return predictions_array, cv_scores, feature_cols

# =====================================================
# ä¸»ç¨‹åº - Enhanced PyTorch Version
# =====================================================
def main():
    print("=== ULTRA Multi-Strategy Ensemble with Optimized Meta-ANN ===")
    
    # æ•°æ®åŠ è½½
    print("\n=== Loading Data ===")
    features_path = '/Users/mannormal/4011/Qi Zihan/feature_extraction/generated_features/all_features_super_optimized.csv'
    all_features_df = pd.read_csv(features_path)

    pwd = '/Users/mannormal/4011/Qi Zihan/original_data/'
    ta = pd.read_csv(pwd + 'train_acc.csv')
    te = pd.read_csv(pwd + 'test_acc_predict.csv')
    ta.loc[ta['flag'] == 0, 'flag'] = -1

    strategy_data = load_strategy_categories()
    training_df = pd.merge(all_features_df, ta[['account', 'flag']], on='account', how='inner')
    training_df['account_type'] = training_df.apply(classify_account_type_original, axis=1)

    print(f"Training data: {training_df.shape}")
    print(f"Account type distribution: {dict(training_df['account_type'].value_counts())}")
    print(f"Flag distribution: {dict(training_df['flag'].value_counts())}")
    
    # å‡†å¤‡åŸå§‹ç‰¹å¾
    feature_cols = [col for col in training_df.columns 
                   if col not in ['account', 'flag', 'account_type']]
    original_features = training_df[feature_cols].values
    y_true = np.where(training_df['flag'].values == -1, 0, 1)
    
    print(f"Original features shape: {original_features.shape}")
    print(f"Class distribution: {dict(zip(*np.unique(y_true, return_counts=True)))}")

    # =====================================================
    # Phase 1: å¢å¼ºéšæœºæ£®æ—é›†æˆ
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 1: Enhanced Random Forest Ensemble")
    print(f"{'='*80}")
    
    rf_predictions, rf_cv_scores, rf_feature_names = train_enhanced_rf_ensemble(
        training_df, n_models=100
    )
    
    # =====================================================
    # Phase 2: ç­–ç•¥é›†æˆ
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 2: Strategy-Based Ensembles")
    print(f"{'='*80}")
    
    all_strategy_predictions = []
    strategy_results = {}
    
    for strategy_name, strategy_categories in strategy_data.items():
        print(f"\n--- {strategy_name.upper()} Strategy ---")
        strategy_preds, strategy_cv = train_strategy_ensemble(
            training_df, strategy_name, strategy_categories, n_models=20
        )
        all_strategy_predictions.append(strategy_preds)
        strategy_results[strategy_name] = {
            'predictions': strategy_preds,
            'cv_scores': strategy_cv,
            'avg_cv': np.mean(strategy_cv)
        }
        print(f"   Average CV F1: {np.mean(strategy_cv):.4f}")
    
    # åˆå¹¶æ‰€æœ‰é¢„æµ‹
    print(f"\nğŸ“Š Combining Predictions:")
    print(f"   RF predictions: {rf_predictions.shape}")
    for i, strategy_name in enumerate(strategy_data.keys()):
        print(f"   {strategy_name} predictions: {all_strategy_predictions[i].shape}")
    
    combined_base_predictions = np.hstack([rf_predictions] + all_strategy_predictions)
    
    print(f"   ğŸ“Š Combined base predictions: {combined_base_predictions.shape}")
    print(f"   ğŸ“Š Total models: {combined_base_predictions.shape[1]} (100 RF + {combined_base_predictions.shape[1]-100} Strategy)")
    
    # =====================================================
    # Phase 3: ä¼˜åŒ–çš„Meta-ANNè®­ç»ƒ (æ›¿æ¢åŸæœ‰çš„PyTorch Meta-ANNéƒ¨åˆ†)
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 3: ä¼˜åŒ–Meta-ANNè®­ç»ƒ (é‡ç‚¹: Badå®¢æˆ·F1)")
    print(f"{'='*80}")
    
    meta_predictions, meta_model, feature_scaler, meta_results = train_refined_meta_ann(
        base_predictions=combined_base_predictions,
        original_features=original_features,
        y_true=y_true,
        n_epochs=500,
        patience=20
    )
    
    # =====================================================
    # Phase 4: å¢å¼ºçš„äº¤å‰éªŒè¯åˆ†æ
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 4: å¢å¼ºäº¤å‰éªŒè¯åˆ†æ (ä¸“æ³¨Badå®¢æˆ·F1)")
    print(f"{'='*80}")
    
    # 10æŠ˜äº¤å‰éªŒè¯è¯„ä¼°
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    cv_results = []
    
    print("\nFold | Bad F1   | Good F1  | Macro F1 | Accuracy | Bad Prec | Bad Recall | Status")
    print("-" * 80)
    
    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(combined_base_predictions, y_true), desc="CV Folds")):
        # åˆ†å‰²æ•°æ®
        X_base_train = combined_base_predictions[train_idx]
        X_base_val = combined_base_predictions[val_idx]
        X_feat_train = original_features[train_idx]
        X_feat_val = original_features[val_idx]
        y_train_fold = y_true[train_idx]
        y_val_fold = y_true[val_idx]
        
        # ç‰¹å¾ç¼©æ”¾
        scaler_fold = StandardScaler()
        X_feat_train_scaled = scaler_fold.fit_transform(X_feat_train)
        X_feat_val_scaled = scaler_fold.transform(X_feat_val)
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_counts = np.bincount(y_train_fold)
        class_weights = torch.FloatTensor(len(y_train_fold) / (len(class_counts) * class_counts))
        
        # è®­ç»ƒMeta-ANN
        device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
        model_fold = RefinedMetaANN(
            n_base_models=X_base_train.shape[1], 
            n_original_features=X_feat_train_scaled.shape[1]
        ).to(device)
        
        optimizer = optim.AdamW(model_fold.parameters(), lr=1e-3, weight_decay=5e-4)
        criterion = LightLabelSmoothingCE(smoothing=0.03, class_weights=class_weights.to(device))
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_base_train_t = torch.tensor(X_base_train, dtype=torch.float32).to(device)
        X_feat_train_t = torch.tensor(X_feat_train_scaled, dtype=torch.float32).to(device)
        y_train_t = torch.tensor(y_train_fold, dtype=torch.long).to(device)
        
        X_base_val_t = torch.tensor(X_base_val, dtype=torch.float32).to(device)
        X_feat_val_t = torch.tensor(X_feat_val_scaled, dtype=torch.float32).to(device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_base_train_t, X_feat_train_t, y_train_t)
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        
        # å¿«é€Ÿè®­ç»ƒï¼ˆç”¨äºCVï¼‰
        for epoch in range(100):
            model_fold.train()
            for batch_xb, batch_xf, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model_fold(batch_xb, batch_xf)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
        
        # è¯„ä¼°
        model_fold.eval()
        with torch.no_grad():
            val_outputs = model_fold(X_base_val_t, X_feat_val_t)
            _, val_preds = torch.max(val_outputs, 1)
            val_label = val_preds.cpu().numpy()
            
            # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
            val_accuracy = accuracy_score(y_val_fold, val_label)
            val_f1_macro = f1_score(y_val_fold, val_label, average='macro', zero_division=0)
            
            val_f1_per_class = f1_score(y_val_fold, val_label, average=None, zero_division=0)
            good_f1 = val_f1_per_class[0]
            bad_f1 = val_f1_per_class[1] if len(val_f1_per_class) > 1 else 0
            
            precision_per_class = precision_score(y_val_fold, val_label, average=None, zero_division=0)
            recall_per_class = recall_score(y_val_fold, val_label, average=None, zero_division=0)
            
            bad_precision = precision_per_class[1] if len(precision_per_class) > 1 else 0
            bad_recall = recall_per_class[1] if len(recall_per_class) > 1 else 0
            
            status = "Strong" if bad_f1 > 0.6 else "Good" if bad_f1 > 0.4 else "Weak"
            
            print(f"{fold+1:4d} | {bad_f1:8.4f} | {good_f1:8.4f} | {val_f1_macro:8.4f} | "
                  f"{val_accuracy:8.4f} | {bad_precision:8.4f} | {bad_recall:9.4f} | {status}")
            
            cv_results.append({
                'fold': fold + 1,
                'bad_f1': bad_f1,
                'good_f1': good_f1,
                'macro_f1': val_f1_macro,
                'accuracy': val_accuracy,
                'bad_precision': bad_precision,
                'bad_recall': bad_recall
            })

    # CVç»Ÿè®¡
    avg_bad_f1 = np.mean([r['bad_f1'] for r in cv_results])
    avg_good_f1 = np.mean([r['good_f1'] for r in cv_results])
    avg_macro_f1 = np.mean([r['macro_f1'] for r in cv_results])
    avg_accuracy = np.mean([r['accuracy'] for r in cv_results])
    avg_bad_precision = np.mean([r['bad_precision'] for r in cv_results])
    avg_bad_recall = np.mean([r['bad_recall'] for r in cv_results])

    print("-" * 80)
    print(f"Avg  | {avg_bad_f1:8.4f} | {avg_good_f1:8.4f} | {avg_macro_f1:8.4f} | "
          f"{avg_accuracy:8.4f} | {avg_bad_precision:8.4f} | {avg_bad_recall:9.4f} | Summary")

    print(f"\nğŸ¤– ä¼˜åŒ–Meta-ANNæ€§èƒ½æ€»ç»“:")
    print(f"   è®­ç»ƒé›†Bad F1: {meta_results['final_bad_f1']:.4f}")
    print(f"   äº¤å‰éªŒè¯Bad F1: {avg_bad_f1:.4f} Â± {np.std([r['bad_f1'] for r in cv_results]):.4f}")
    print(f"   Badå®¢æˆ·æ£€å‡ºç‡: {avg_bad_recall:.4f} Â± {np.std([r['bad_recall'] for r in cv_results]):.4f}")
    print(f"   Badå®¢æˆ·é¢„æµ‹å‡†ç¡®ç‡: {avg_bad_precision:.4f} Â± {np.std([r['bad_precision'] for r in cv_results]):.4f}")
    print(f"   æ•´ä½“å‡†ç¡®ç‡: {avg_accuracy:.4f} Â± {np.std([r['accuracy'] for r in cv_results]):.4f}")

    generalization_gap = meta_results['final_bad_f1'] - avg_bad_f1
    if generalization_gap > 0.1:
        print(f"   âš ï¸ è­¦å‘Š: é«˜è¿‡æ‹Ÿåˆ (å·®è·: {generalization_gap:+.4f})")
    elif generalization_gap > 0.05:
        print(f"   âš ï¸ æ³¨æ„: ä¸­åº¦è¿‡æ‹Ÿåˆ (å·®è·: {generalization_gap:+.4f})")
    else:
        print(f"   âœ… è‰¯å¥½: ä½è¿‡æ‹Ÿåˆ (å·®è·: {generalization_gap:+.4f})")

    # =====================================================
    # Phase 5: æµ‹è¯•é›†é¢„æµ‹ & ç”Ÿæˆæäº¤æ–‡ä»¶ (ä¿®æ”¹éƒ¨åˆ†)
    # =====================================================
    print(f"\n{'='*80}")
    print("ğŸ¯ PHASE 5: Test Set Prediction & Submission Generation")
    print(f"{'='*80}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_df = pd.merge(all_features_df, te[['account']], on='account', how='inner')
    test_df['account_type'] = test_df.apply(classify_account_type_original, axis=1)
    
    print(f"ğŸ“Š Test Data Info:")
    print(f"   Test accounts: {test_df.shape[0]}")
    print(f"   Test account type distribution: {dict(test_df['account_type'].value_counts())}")
    
    # =====================================================
    # ä¸ºæµ‹è¯•é›†ç”ŸæˆåŸºç¡€æ¨¡å‹é¢„æµ‹
    # =====================================================
    print(f"\nğŸ”® Generating Test Predictions...")
    
    # 1. RFé¢„æµ‹ (é‡æ–°è®­ç»ƒæ‰€æœ‰RFæ¨¡å‹)
    print("ğŸŒ³ RF Ensemble Test Predictions...")
    test_rf_predictions = []
    
    data_copy = training_df.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    rf_feature_cols = [col for col in data_copy.columns 
                      if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]
    
    X_train_rf = data_copy[rf_feature_cols].values
    y_train_rf = data_copy['flag'].values
    X_test_rf = test_df[rf_feature_cols].values
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    sample_size = min(good_accounts, bad_accounts)
    
    rf_configs = [
        {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 8, 'min_samples_leaf': 3},
        {'n_estimators': 180, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 2},
        {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4},
        {'n_estimators': 220, 'max_depth': 35, 'min_samples_split': 12, 'min_samples_leaf': 5},
    ]
    
    for i in tqdm(range(100), desc="RF Test Prediction"):
        bootstrap_ratio = 0.8 + 0.4 * np.random.random()
        actual_sample_size = int(sample_size * bootstrap_ratio)
        
        good_sample = data_copy[data_copy['flag'] == 1].sample(
            n=actual_sample_size, replace=True, random_state=i
        )
        bad_sample = data_copy[data_copy['flag'] == 0].sample(
            n=actual_sample_size, replace=True, random_state=i+3000
        )
        train_data = pd.concat([good_sample, bad_sample], axis=0)
        
        X_train_rf_model = train_data[rf_feature_cols].values
        y_train_rf_model = train_data['flag'].values
        
        config = rf_configs[i % len(rf_configs)]
        clf = RandomForestClassifier(
            **config,
            random_state=i,
            class_weight='balanced_subsample',
            max_features='sqrt',
            bootstrap=True,
            n_jobs=1
        )
        clf.fit(X_train_rf_model, y_train_rf_model)
        
        test_pred = clf.predict_proba(X_test_rf)[:, 1]
        test_rf_predictions.append(test_pred)
    
    test_rf_predictions = np.array(test_rf_predictions).T
    print(f"   RF test predictions shape: {test_rf_predictions.shape}")
    
    # 2. ç­–ç•¥é¢„æµ‹
    print("ğŸ¯ Strategy Ensemble Test Predictions...")
    test_strategy_predictions = []
    
    for strategy_name, strategy_categories in strategy_data.items():
        print(f"   Processing {strategy_name} strategy...")
        
        # è®­ç»ƒæ•°æ®å¤„ç†
        train_with_strategy = training_df.merge(strategy_categories, on='account', how='left')
        strategy_col = f"{strategy_name}_category"
        train_with_strategy[strategy_col] = train_with_strategy[strategy_col].fillna('unknown')
        train_copy = train_with_strategy.copy()
        train_copy.loc[train_copy['flag'] == -1, 'flag'] = 0
        
        # æµ‹è¯•æ•°æ®å¤„ç†
        test_with_strategy = test_df.merge(strategy_categories, on='account', how='left')
        test_with_strategy[strategy_col] = test_with_strategy[strategy_col].fillna('unknown')
        
        # ç‰¹å¾å¤„ç†
        feature_cols_strategy = [col for col in train_copy.columns if col not in ['account', 'flag', 'account_type']]
        train_strategy_dummies = pd.get_dummies(train_copy[strategy_col], prefix=strategy_name)
        test_strategy_dummies = pd.get_dummies(test_with_strategy[strategy_col], prefix=strategy_name)
        
        # ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•é›†æœ‰ç›¸åŒçš„åˆ—
        all_strategy_cols = set(train_strategy_dummies.columns) | set(test_strategy_dummies.columns)
        for col in all_strategy_cols:
            if col not in train_strategy_dummies.columns:
                train_strategy_dummies[col] = 0
            if col not in test_strategy_dummies.columns:
                test_strategy_dummies[col] = 0
        
        train_strategy_dummies = train_strategy_dummies[sorted(all_strategy_cols)]
        test_strategy_dummies = test_strategy_dummies[sorted(all_strategy_cols)]
        
        train_feature_data = pd.concat([
            train_copy[[col for col in feature_cols_strategy if not col.endswith('_category')]],
            train_strategy_dummies
        ], axis=1)
        
        test_feature_data = pd.concat([
            test_with_strategy[[col for col in train_copy.columns if col not in ['account', 'flag', 'account_type'] and not col.endswith('_category')]],
            test_strategy_dummies
        ], axis=1)
        
        # è®­ç»ƒæ¨¡å‹å¹¶é¢„æµ‹
        strategy_test_preds = []
        for i in range(20):
            good_sample = train_copy[train_copy['flag'] == 1].sample(n=sample_size, replace=True, random_state=i*100)
            bad_sample = train_copy[train_copy['flag'] == 0].sample(n=sample_size, replace=True, random_state=i*100+50)
            sample_indices = list(good_sample.index) + list(bad_sample.index)
            
            X_train_strategy = train_feature_data.loc[sample_indices].values
            y_train_strategy = pd.concat([good_sample, bad_sample])['flag'].values
            
            clf = RandomForestClassifier(
                n_estimators=120,
                max_depth=18,
                min_samples_split=8,
                min_samples_leaf=4,
                random_state=i*10,
                class_weight='balanced'
            )
            clf.fit(X_train_strategy, y_train_strategy)
            
            test_pred = clf.predict_proba(test_feature_data.values)[:, 1]
            strategy_test_preds.append(test_pred)
        
        strategy_test_preds = np.array(strategy_test_preds).T
        test_strategy_predictions.append(strategy_test_preds)
        print(f"   {strategy_name} test predictions shape: {strategy_test_preds.shape}")
    
    # 3. åˆå¹¶æµ‹è¯•é›†é¢„æµ‹
    test_combined_base_predictions = np.hstack([test_rf_predictions] + test_strategy_predictions)
    print(f"ğŸ“Š Combined test predictions shape: {test_combined_base_predictions.shape}")
    
    # 4. ä¼˜åŒ–Meta-ANNæµ‹è¯•é¢„æµ‹
    print("ğŸ¤– ä¼˜åŒ–Meta-ANNæµ‹è¯•é¢„æµ‹...")
    test_original_features = test_df[feature_cols].values
    test_original_features_scaled = feature_scaler.transform(test_original_features)
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    X_test_base_tensor = torch.tensor(test_combined_base_predictions, dtype=torch.float32).to(device)
    X_test_feat_tensor = torch.tensor(test_original_features_scaled, dtype=torch.float32).to(device)
    
    meta_model.eval()
    with torch.no_grad():
        test_outputs = meta_model(X_test_base_tensor, X_test_feat_tensor)
        test_probabilities = F.softmax(test_outputs, dim=1)
        _, test_final_predictions = torch.max(test_outputs, 1)
        
        test_final_labels = test_final_predictions.cpu().numpy()
        test_bad_probabilities = test_probabilities[:, 1].cpu().numpy()  # Badå®¢æˆ·çš„æ¦‚ç‡
    
    # =====================================================
    # ç”Ÿæˆæäº¤æ–‡ä»¶éƒ¨åˆ†ä¿®æ”¹
    # =====================================================
    print(f"\nğŸ’¾ ä¿å­˜æäº¤æ–‡ä»¶...")
    
    # åˆ›å»ºæäº¤DataFrame
    submission_df = pd.DataFrame({
        'account': test_df['account'].values,
        'flag': test_final_labels
    })
    
    # ç»Ÿè®¡é¢„æµ‹ç»“æœ
    pred_counts = submission_df['flag'].value_counts()
    print(f"ğŸ“Š æµ‹è¯•é¢„æµ‹æ€»ç»“:")
    print(f"   æ€»æµ‹è¯•è´¦æˆ·: {len(submission_df)}")
    print(f"   é¢„æµ‹Good (0): {pred_counts.get(0, 0)} ({pred_counts.get(0, 0)/len(submission_df)*100:.1f}%)")
    print(f"   é¢„æµ‹Bad (1): {pred_counts.get(1, 0)} ({pred_counts.get(1, 0)/len(submission_df)*100:.1f}%)")
    
    # ç”Ÿæˆæ–‡ä»¶åï¼Œä½¿ç”¨äº¤å‰éªŒè¯çš„Bad F1åˆ†æ•°
    mean_cv_bad_f1 = avg_bad_f1
    filename = f"optimized_meta_ann_bad_f1_{mean_cv_bad_f1:.4f}.csv"
    filepath = f"/Users/mannormal/4011/Qi Zihan/result_analysis/prediction_results/{filename}"
    
    # ä¿å­˜æ–‡ä»¶
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    submission_df.to_csv(filepath, index=False)
    
    print(f"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: {filename}")
    print(f"ğŸ“ å®Œæ•´è·¯å¾„: {filepath}")
    
    # =====================================================
    # é¢å¤–ä¿å­˜è¯¦ç»†åˆ†ææ–‡ä»¶
    # =====================================================
    # ä¿å­˜æµ‹è¯•é›†çš„é¢„æµ‹æ¦‚ç‡ç”¨äºè¿›ä¸€æ­¥åˆ†æ
    detailed_test_results = pd.DataFrame({
        'account': test_df['account'].values,
        'predicted_label': test_final_labels,
        'bad_probability': test_bad_probabilities,
        'confidence': np.max([1 - test_bad_probabilities, test_bad_probabilities], axis=0)
    })
    
    detailed_filename = f"detailed_test_results_bad_f1_{mean_cv_bad_f1:.4f}.csv"
    detailed_filepath = f"/Users/mannormal/4011/Qi Zihan/result_analysis/prediction_results/{detailed_filename}"
    detailed_test_results.to_csv(detailed_filepath, index=False)
    
    print(f"ğŸ“„ è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜: {detailed_filename}")
    
    # ç½®ä¿¡åº¦åˆ†æ
    high_confidence_bad = detailed_test_results[
        (detailed_test_results['predicted_label'] == 1) & 
        (detailed_test_results['confidence'] > 0.8)
    ]
    high_confidence_good = detailed_test_results[
        (detailed_test_results['predicted_label'] == 0) & 
        (detailed_test_results['confidence'] > 0.8)
    ]
    
    print(f"\nğŸ“ˆ ç½®ä¿¡åº¦åˆ†æ:")
    print(f"   é«˜ç½®ä¿¡åº¦Badé¢„æµ‹: {len(high_confidence_bad)} ä¸ª")
    print(f"   é«˜ç½®ä¿¡åº¦Goodé¢„æµ‹: {len(high_confidence_good)} ä¸ª")
    print(f"   é«˜ç½®ä¿¡åº¦é¢„æµ‹æ¯”ä¾‹: {(len(high_confidence_bad) + len(high_confidence_good))/len(submission_df)*100:.1f}%")
    
    return {
        'meta_model': meta_model,
        'feature_scaler': feature_scaler,
        'rf_predictions': rf_predictions,
        'strategy_predictions': all_strategy_predictions,
        'cv_results': cv_results,
        'meta_results': meta_results,
        'final_bad_f1': avg_bad_f1,  # ä½¿ç”¨äº¤å‰éªŒè¯çš„Bad F1
        'submission_df': submission_df,
        'submission_filepath': filepath,
        'detailed_results': detailed_test_results,
        'detailed_filepath': detailed_filepath
    }

# åœ¨importséƒ¨åˆ†æ·»åŠ 
from sklearn.metrics import roc_curve, precision_recall_curve

# æ·»åŠ é˜ˆå€¼ä¼˜åŒ–å‡½æ•°
def find_optimal_threshold(y_true, y_prob, metric='f1_bad', plot=False):
    """
    å¯»æ‰¾æœ€ä¼˜åˆ†ç±»é˜ˆå€¼
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_prob: é¢„æµ‹æ¦‚ç‡ (Badå®¢æˆ·çš„æ¦‚ç‡)
        metric: ä¼˜åŒ–æŒ‡æ ‡ ('f1_bad', 'f1_macro', 'precision_recall_balance', 'youden')
        plot: æ˜¯å¦ç»˜åˆ¶é˜ˆå€¼æ›²çº¿
    
    Returns:
        æœ€ä¼˜é˜ˆå€¼å’Œå¯¹åº”çš„æŒ‡æ ‡å€¼
    """
    thresholds = np.arange(0.1, 0.9, 0.01)  # ä»0.1åˆ°0.9ï¼Œæ­¥é•¿0.01
    metrics_scores = []
    
    best_threshold = 0.5
    best_score = 0
    threshold_details = []
    
    for threshold in thresholds:
        y_pred = (y_prob >= threshold).astype(int)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred)
        f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
        
        # åˆ†ç±»åˆ«æŒ‡æ ‡
        f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)
        precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)
        recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)
        
        good_f1 = f1_per_class[0] if len(f1_per_class) > 0 else 0
        bad_f1 = f1_per_class[1] if len(f1_per_class) > 1 else 0
        
        good_precision = precision_per_class[0] if len(precision_per_class) > 0 else 0
        bad_precision = precision_per_class[1] if len(precision_per_class) > 1 else 0
        
        good_recall = recall_per_class[0] if len(recall_per_class) > 0 else 0
        bad_recall = recall_per_class[1] if len(recall_per_class) > 1 else 0
        
        # è®¡ç®—ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡
        if metric == 'f1_bad':
            score = bad_f1
        elif metric == 'f1_macro':
            score = f1_macro
        elif metric == 'precision_recall_balance':
            # Badå®¢æˆ·çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
            if bad_precision + bad_recall > 0:
                score = 2 * (bad_precision * bad_recall) / (bad_precision + bad_recall)
            else:
                score = 0
        elif metric == 'youden':
            # Youden's J statistic = Sensitivity + Specificity - 1
            sensitivity = bad_recall  # å¯¹Badå®¢æˆ·çš„å¬å›ç‡
            specificity = good_recall  # å¯¹Goodå®¢æˆ·çš„å¬å›ç‡ (çœŸè´Ÿç‡)
            score = sensitivity + specificity - 1
        else:
            score = bad_f1  # é»˜è®¤ä½¿ç”¨Bad F1
        
        metrics_scores.append(score)
        
        threshold_details.append({
            'threshold': threshold,
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'bad_f1': bad_f1,
            'good_f1': good_f1,
            'bad_precision': bad_precision,
            'bad_recall': bad_recall,
            'good_precision': good_precision,
            'good_recall': good_recall,
            'score': score
        })
        
        if score > best_score:
            best_score = score
            best_threshold = threshold
    
    # æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼çš„è¯¦ç»†ä¿¡æ¯
    best_details = next(d for d in threshold_details if d['threshold'] == best_threshold)
    
    print(f"\nğŸ¯ æœ€ä¼˜é˜ˆå€¼ä¼˜åŒ–ç»“æœ (ä¼˜åŒ–æŒ‡æ ‡: {metric}):")
    print(f"   æœ€ä¼˜é˜ˆå€¼: {best_threshold:.3f}")
    print(f"   ä¼˜åŒ–æŒ‡æ ‡åˆ†æ•°: {best_score:.4f}")
    print(f"   Badå®¢æˆ·F1: {best_details['bad_f1']:.4f}")
    print(f"   Goodå®¢æˆ·F1: {best_details['good_f1']:.4f}")
    print(f"   å®å¹³å‡F1: {best_details['f1_macro']:.4f}")
    print(f"   Badå®¢æˆ·ç²¾ç¡®ç‡: {best_details['bad_precision']:.4f}")
    print(f"   Badå®¢æˆ·å¬å›ç‡: {best_details['bad_recall']:.4f}")
    print(f"   æ•´ä½“å‡†ç¡®ç‡: {best_details['accuracy']:.4f}")
    
    # æ˜¾ç¤ºé˜ˆå€¼é€‰æ‹©çš„å½±å“
    default_details = next(d for d in threshold_details if abs(d['threshold'] - 0.5) < 0.01)
    print(f"\nğŸ“Š ç›¸æ¯”é»˜è®¤é˜ˆå€¼0.5çš„æ”¹è¿›:")
    print(f"   Bad F1: {best_details['bad_f1']:.4f} vs {default_details['bad_f1']:.4f} "
          f"(æ”¹è¿›: {best_details['bad_f1'] - default_details['bad_f1']:+.4f})")
    print(f"   å®å¹³å‡F1: {best_details['f1_macro']:.4f} vs {default_details['f1_macro']:.4f} "
          f"(æ”¹è¿›: {best_details['f1_macro'] - default_details['f1_macro']:+.4f})")
    
    return best_threshold, best_score, threshold_details

def evaluate_with_optimal_threshold(model, scaler, X_base, X_feat, y_true, 
                                  optimization_metric='f1_bad', device='cpu'):
    """
    ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¯„ä¼°æ¨¡å‹
    """
    model.eval()
    with torch.no_grad():
        X_base_tensor = torch.tensor(X_base, dtype=torch.float32).to(device)
        X_feat_scaled = scaler.transform(X_feat)
        X_feat_tensor = torch.tensor(X_feat_scaled, dtype=torch.float32).to(device)
        
        outputs = model(X_base_tensor, X_feat_tensor)
        probabilities = F.softmax(outputs, dim=1)
        bad_probabilities = probabilities[:, 1].cpu().numpy()
    
    # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
    optimal_threshold, optimal_score, threshold_details = find_optimal_threshold(
        y_true, bad_probabilities, metric=optimization_metric
    )
    
    # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¿›è¡Œé¢„æµ‹
    optimal_predictions = (bad_probabilities >= optimal_threshold).astype(int)
    
    return optimal_predictions, optimal_threshold, bad_probabilities, threshold_details

# ä¿®æ”¹train_refined_meta_annå‡½æ•°ï¼Œåœ¨æœ€ç»ˆè¯„ä¼°éƒ¨åˆ†æ·»åŠ é˜ˆå€¼ä¼˜åŒ–
def train_refined_meta_ann(base_predictions, original_features, y_true, n_epochs=500, patience=20):
    """
    è®­ç»ƒä¼˜åŒ–çš„Meta-ANNï¼Œé‡ç‚¹å…³æ³¨Badå®¢æˆ·F1åˆ†æ•°ï¼ŒåŒ…å«é˜ˆå€¼ä¼˜åŒ–
    
    Args:
        base_predictions: (n_samples, n_models) - åŸºç¡€æ¨¡å‹é¢„æµ‹æ¦‚ç‡
        original_features: (n_samples, n_features) - åŸå§‹ç‰¹å¾
        y_true: (n_samples,) - çœŸå®æ ‡ç­¾ (0: Good, 1: Bad)
    
    Returns:
        è®­ç»ƒå¥½çš„æ¨¡å‹ã€é¢„æµ‹ç»“æœå’Œè¯¦ç»†æŒ‡æ ‡
    """
    print(f"\nğŸ¤– è®­ç»ƒä¼˜åŒ–Meta-ANN (é‡ç‚¹: Badå®¢æˆ·F1)")
    print(f"Base predictions shape: {base_predictions.shape}")
    print(f"Original features shape: {original_features.shape}")
    print(f"Label distribution: {dict(zip(*np.unique(y_true, return_counts=True)))}")
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    original_features_scaled = scaler.fit_transform(original_features)
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_base_tensor = torch.tensor(base_predictions, dtype=torch.float32).to(device)
    X_feat_tensor = torch.tensor(original_features_scaled, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y_true, dtype=torch.long).to(device)  # æ³¨æ„è¿™é‡Œç”¨LongTensor
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = np.bincount(y_true)
    class_weights = torch.FloatTensor(len(y_true) / (len(class_counts) * class_counts)).to(device)
    print(f"Class weights: {class_weights}")
    
    # åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹
    model = RefinedMetaANN(
        n_base_models=base_predictions.shape[1], 
        n_original_features=original_features_scaled.shape[1],
        dropout_rates=[0.3, 0.2, 0.2, 0.1, 0.1]
    ).to(device)
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)
    criterion = LightLabelSmoothingCE(smoothing=0.03, class_weights=class_weights)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=8, factor=0.5, min_lr=1e-6)
    early_stopping = SimpleEarlyStopping(patience=patience, min_delta=0.001)
    
    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    train_idx, val_idx = list(skf.split(base_predictions, y_true))[0]
    
    Xb_train, Xb_val = X_base_tensor[train_idx], X_base_tensor[val_idx]
    Xf_train, Xf_val = X_feat_tensor[train_idx], X_feat_tensor[val_idx]
    y_train, y_val = y_tensor[train_idx], y_tensor[val_idx]
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(Xb_train, Xf_train, y_train)
    val_dataset = TensorDataset(Xb_val, Xf_val, y_val)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    
    # è®­ç»ƒå†å²è®°å½•
    train_history = []
    best_bad_f1 = 0
    
    print("\nEpoch | Train Loss | Bad F1 | Macro F1 | Accuracy | Good F1 | LR       | Status")
    print("-" * 80)
    
    for epoch in range(n_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        for batch_xb, batch_xf, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_xb, batch_xf)
            loss = criterion(outputs, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for batch_xb, batch_xf, batch_y in val_loader:
                outputs = model(batch_xb, batch_xf)
                _, predicted = torch.max(outputs, 1)
                val_predictions.extend(predicted.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        val_f1_macro = f1_score(val_targets, val_predictions, average='macro', zero_division=0)
        val_accuracy = accuracy_score(val_targets, val_predictions)
        
        # è®¡ç®—å„ç±»åˆ«F1
        val_f1_per_class = f1_score(val_targets, val_predictions, average=None, zero_division=0)
        good_f1 = val_f1_per_class[0]
        bad_f1 = val_f1_per_class[1] if len(val_f1_per_class) > 1 else 0  # ä¸»è¦æŒ‡æ ‡
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(bad_f1)
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®°å½•å†å²
        epoch_data = {
            'epoch': epoch + 1,
            'train_loss': avg_train_loss,
            'bad_f1': bad_f1,
            'macro_f1': val_f1_macro,
            'accuracy': val_accuracy,
            'good_f1': good_f1,
            'lr': current_lr
        }
        train_history.append(epoch_data)
        
        # æ—©åœæ£€æŸ¥
        if bad_f1 > best_bad_f1:
            best_bad_f1 = bad_f1
            status = "âœ… Best"
        else:
            status = f"â³ {early_stopping.counter+1}/{early_stopping.patience}"
        
        # æ‰“å°è¿›åº¦
        if epoch % 20 == 0 or epoch >= n_epochs - 5:
            print(f"{epoch+1:5d} | {avg_train_loss:10.4f} | {bad_f1:6.4f} | {val_f1_macro:8.4f} | "
                  f"{val_accuracy:8.4f} | {good_f1:7.4f} | {current_lr:.2e} | {status}")
        
        if early_stopping(bad_f1, model):
            print(f"\nğŸ›‘ Early stopping at epoch {epoch+1}")
            print(f"ğŸ† Best Bad F1: {best_bad_f1:.4f}")
            break
    
    # =====================================================
    # æœ€ç»ˆè¯„ä¼° - æ·»åŠ é˜ˆå€¼ä¼˜åŒ–
    # =====================================================
    print(f"\n{'='*60}")
    print("ğŸ¯ é˜ˆå€¼ä¼˜åŒ–ä¸æœ€ç»ˆè¯„ä¼°")
    print(f"{'='*60}")
    
    model.eval()
    with torch.no_grad():
        # è·å–æ¦‚ç‡é¢„æµ‹
        final_outputs = model(X_base_tensor, X_feat_tensor)
        final_probs = F.softmax(final_outputs, dim=1)
        final_probs_np = final_probs.cpu().numpy()
        bad_probabilities = final_probs_np[:, 1]
    
    # å¤šç§é˜ˆå€¼ä¼˜åŒ–ç­–ç•¥
    optimization_metrics = ['f1_bad', 'f1_macro', 'precision_recall_balance', 'youden']
    optimal_results = {}
    
    for opt_metric in optimization_metrics:
        print(f"\n--- ä¼˜åŒ–æŒ‡æ ‡: {opt_metric} ---")
        optimal_threshold, optimal_score, threshold_details = find_optimal_threshold(
            y_true, bad_probabilities, metric=opt_metric
        )
        
        # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼é¢„æµ‹
        optimal_predictions = (bad_probabilities >= optimal_threshold).astype(int)
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        final_accuracy = accuracy_score(y_true, optimal_predictions)
        final_f1_macro = f1_score(y_true, optimal_predictions, average='macro', zero_division=0)
        final_f1_weighted = f1_score(y_true, optimal_predictions, average='weighted', zero_division=0)
        
        final_f1_per_class = f1_score(y_true, optimal_predictions, average=None, zero_division=0)
        final_good_f1 = final_f1_per_class[0]
        final_bad_f1 = final_f1_per_class[1] if len(final_f1_per_class) > 1 else 0
        
        precision_per_class = precision_score(y_true, optimal_predictions, average=None, zero_division=0)
        recall_per_class = recall_score(y_true, optimal_predictions, average=None, zero_division=0)
        
        final_good_precision = precision_per_class[0]
        final_good_recall = recall_per_class[0]
        final_bad_precision = precision_per_class[1] if len(precision_per_class) > 1 else 0
        final_bad_recall = recall_per_class[1] if len(recall_per_class) > 1 else 0
        
        optimal_results[opt_metric] = {
            'threshold': optimal_threshold,
            'predictions': optimal_predictions,
            'final_bad_f1': final_bad_f1,
            'final_good_f1': final_good_f1,
            'final_macro_f1': final_f1_macro,
            'final_weighted_f1': final_f1_weighted,
            'final_accuracy': final_accuracy,
            'final_bad_precision': final_bad_precision,
            'final_bad_recall': final_bad_recall,
            'final_good_precision': final_good_precision,
            'final_good_recall': final_good_recall,
            'threshold_details': threshold_details
        }
    
    # é€‰æ‹©æœ€ä½³çš„é˜ˆå€¼ä¼˜åŒ–ç­–ç•¥ (åŸºäºBad F1)
    best_strategy = max(optimal_results.keys(), 
                       key=lambda k: optimal_results[k]['final_bad_f1'])
    best_result = optimal_results[best_strategy]
    
    print(f"\nğŸ† æ¨èçš„æœ€ä½³é˜ˆå€¼ç­–ç•¥: {best_strategy}")
    print(f"   æœ€ä¼˜é˜ˆå€¼: {best_result['threshold']:.3f}")
    print(f"   Badå®¢æˆ·F1: {best_result['final_bad_f1']:.4f}")
    print(f"   Goodå®¢æˆ·F1: {best_result['final_good_f1']:.4f}")
    print(f"   å®å¹³å‡F1: {best_result['final_macro_f1']:.4f}")
    print(f"   æ•´ä½“å‡†ç¡®ç‡: {best_result['final_accuracy']:.4f}")
    print(f"   Badå®¢æˆ·æ£€å‡ºç‡: {best_result['final_bad_recall']:.4f}")
    print(f"   Badå®¢æˆ·é¢„æµ‹å‡†ç¡®ç‡: {best_result['final_bad_precision']:.4f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, best_result['predictions'])
    print(f"\n   ğŸ“‹ æ··æ·†çŸ©é˜µ (æœ€ä¼˜é˜ˆå€¼ {best_result['threshold']:.3f}):")
    print(f"              é¢„æµ‹Good  é¢„æµ‹Bad")
    print(f"   å®é™…Good    {cm[0,0]:6d}   {cm[0,1]:6d}")
    if cm.shape[0] > 1:
        print(f"   å®é™…Bad     {cm[1,0]:6d}   {cm[1,1]:6d}")
    
    return bad_probabilities, model, scaler, {
        'optimal_results': optimal_results,
        'best_strategy': best_strategy,
        'best_threshold': best_result['threshold'],
        'final_bad_f1': best_result['final_bad_f1'],
        'final_good_f1': best_result['final_good_f1'],
        'final_macro_f1': best_result['final_macro_f1'],
        'final_weighted_f1': best_result['final_weighted_f1'],
        'final_accuracy': best_result['final_accuracy'],
        'final_bad_precision': best_result['final_bad_precision'],
        'final_bad_recall': best_result['final_bad_recall'],
        'best_val_bad_f1': best_bad_f1,
        'train_history': train_history,
        'confusion_matrix': cm,
        'best_predictions': best_result['predictions']
    }

# ä¿®æ”¹äº¤å‰éªŒè¯éƒ¨åˆ†ï¼Œä½¿ç”¨é˜ˆå€¼ä¼˜åŒ–
# åœ¨mainå‡½æ•°çš„Phase 4éƒ¨åˆ†ï¼Œæ·»åŠ é˜ˆå€¼ä¼˜åŒ–çš„äº¤å‰éªŒè¯
def enhanced_cross_validation_with_threshold_optimization(combined_base_predictions, original_features, y_true, n_folds=5):
    """
    å¸¦é˜ˆå€¼ä¼˜åŒ–çš„å¢å¼ºäº¤å‰éªŒè¯
    """
    print(f"\nğŸ¯ Phase 4: å¸¦é˜ˆå€¼ä¼˜åŒ–çš„å¢å¼ºäº¤å‰éªŒè¯")
    print(f"{'='*80}")
    
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
    cv_results = []
    all_threshold_strategies = ['f1_bad', 'f1_macro', 'precision_recall_balance']
    
    print("\nFold | Strategy           | Threshold | Bad F1   | Good F1  | Macro F1 | Accuracy | Bad Prec | Bad Recall")
    print("-" * 105)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(combined_base_predictions, y_true)):
        # æ•°æ®åˆ†å‰²
        X_base_train = combined_base_predictions[train_idx]
        X_base_val = combined_base_predictions[val_idx]
        X_feat_train = original_features[train_idx]
        X_feat_val = original_features[val_idx]
        y_train_fold = y_true[train_idx]
        y_val_fold = y_true[val_idx]
        
        # ç‰¹å¾ç¼©æ”¾
        scaler_fold = StandardScaler()
        X_feat_train_scaled = scaler_fold.fit_transform(X_feat_train)
        X_feat_val_scaled = scaler_fold.transform(X_feat_val)
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_counts = np.bincount(y_train_fold)
        class_weights = torch.FloatTensor(len(y_train_fold) / (len(class_counts) * class_counts))
        
        # è®­ç»ƒMeta-ANN
        device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
        model_fold = RefinedMetaANN(
            n_base_models=X_base_train.shape[1], 
            n_original_features=X_feat_train_scaled.shape[1]
        ).to(device)
        
        optimizer = optim.AdamW(model_fold.parameters(), lr=1e-3, weight_decay=5e-4)
        criterion = LightLabelSmoothingCE(smoothing=0.03, class_weights=class_weights.to(device))
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_base_train_t = torch.tensor(X_base_train, dtype=torch.float32).to(device)
        X_feat_train_t = torch.tensor(X_feat_train_scaled, dtype=torch.float32).to(device)
        y_train_t = torch.tensor(y_train_fold, dtype=torch.long).to(device)
        
        X_base_val_t = torch.tensor(X_base_val, dtype=torch.float32).to(device)
        X_feat_val_t = torch.tensor(X_feat_val_scaled, dtype=torch.float32).to(device)
        
        # å¿«é€Ÿè®­ç»ƒ
        train_dataset = TensorDataset(X_base_train_t, X_feat_train_t, y_train_t)
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        
        for epoch in range(80):  # å‡å°‘epochç”¨äºCV
            model_fold.train()
            for batch_xb, batch_xf, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model_fold(batch_xb, batch_xf)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
        
        # è·å–éªŒè¯é›†æ¦‚ç‡é¢„æµ‹
        model_fold.eval()
        with torch.no_grad():
            val_outputs = model_fold(X_base_val_t, X_feat_val_t)
            val_probabilities = F.softmax(val_outputs, dim=1)
            val_bad_probs = val_probabilities[:, 1].cpu().numpy()
        
        # å¯¹æ¯ç§é˜ˆå€¼ä¼˜åŒ–ç­–ç•¥è¿›è¡Œè¯„ä¼°
        fold_results = {'fold': fold + 1}
        
        for strategy in all_threshold_strategies:
            # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
            optimal_threshold, _, _ = find_optimal_threshold(
                y_val_fold, val_bad_probs, metric=strategy
            )
            
            # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼é¢„æµ‹
            val_predictions = (val_bad_probs >= optimal_threshold).astype(int)
            
            # è®¡ç®—æŒ‡æ ‡
            val_accuracy = accuracy_score(y_val_fold, val_predictions)
            val_f1_macro = f1_score(y_val_fold, val_predictions, average='macro', zero_division=0)
            
            val_f1_per_class = f1_score(y_val_fold, val_predictions, average=None, zero_division=0)
            good_f1 = val_f1_per_class[0]
            bad_f1 = val_f1_per_class[1] if len(val_f1_per_class) > 1 else 0
            
            precision_per_class = precision_score(y_val_fold, val_predictions, average=None, zero_division=0)
            recall_per_class = recall_score(y_val_fold, val_predictions, average=None, zero_division=0)
            
            bad_precision = precision_per_class[1] if len(precision_per_class) > 1 else 0
            bad_recall = recall_per_class[1] if len(recall_per_class) > 1 else 0
            
            fold_results[f'{strategy}_threshold'] = optimal_threshold
            fold_results[f'{strategy}_bad_f1'] = bad_f1
            fold_results[f'{strategy}_good_f1'] = good_f1
            fold_results[f'{strategy}_macro_f1'] = val_f1_macro
            fold_results[f'{strategy}_accuracy'] = val_accuracy
            fold_results[f'{strategy}_bad_precision'] = bad_precision
            fold_results[f'{strategy}_bad_recall'] = bad_recall
            
            print(f"{fold+1:4d} | {strategy:18s} | {optimal_threshold:9.3f} | {bad_f1:8.4f} | {good_f1:8.4f} | "
                  f"{val_f1_macro:8.4f} | {val_accuracy:8.4f} | {bad_precision:8.4f} | {bad_recall:9.4f}")
        
        cv_results.append(fold_results)
    
    # æ±‡æ€»CVç»“æœ
    print("-" * 105)
    
    summary_results = {}
    for strategy in all_threshold_strategies:
        avg_threshold = np.mean([r[f'{strategy}_threshold'] for r in cv_results])
        avg_bad_f1 = np.mean([r[f'{strategy}_bad_f1'] for r in cv_results])
        avg_good_f1 = np.mean([r[f'{strategy}_good_f1'] for r in cv_results])
        avg_macro_f1 = np.mean([r[f'{strategy}_macro_f1'] for r in cv_results])
        avg_accuracy = np.mean([r[f'{strategy}_accuracy'] for r in cv_results])
        avg_bad_precision = np.mean([r[f'{strategy}_bad_precision'] for r in cv_results])
        avg_bad_recall = np.mean([r[f'{strategy}_bad_recall'] for r in cv_results])
        
        summary_results[strategy] = {
            'avg_threshold': avg_threshold,
            'avg_bad_f1': avg_bad_f1,
            'avg_good_f1': avg_good_f1,
            'avg_macro_f1': avg_macro_f1,
            'avg_accuracy': avg_accuracy,
            'avg_bad_precision': avg_bad_precision,
            'avg_bad_recall': avg_bad_recall,
            'std_bad_f1': np.std([r[f'{strategy}_bad_f1'] for r in cv_results])
        }
        
        print(f"Avg  | {strategy:18s} | {avg_threshold:9.3f} | {avg_bad_f1:8.4f} | {avg_good_f1:8.4f} | "
              f"{avg_macro_f1:8.4f} | {avg_accuracy:8.4f} | {avg_bad_precision:8.4f} | {avg_bad_recall:9.4f}")
    
    # æ‰¾åˆ°æœ€ä½³ç­–ç•¥
    best_cv_strategy = max(summary_results.keys(), 
                          key=lambda k: summary_results[k]['avg_bad_f1'])
    best_cv_result = summary_results[best_cv_strategy]
    
    print(f"\nğŸ† äº¤å‰éªŒè¯æœ€ä½³é˜ˆå€¼ç­–ç•¥: {best_cv_strategy}")
    print(f"   å¹³å‡æœ€ä¼˜é˜ˆå€¼: {best_cv_result['avg_threshold']:.3f}")
    print(f"   å¹³å‡Bad F1: {best_cv_result['avg_bad_f1']:.4f} Â± {best_cv_result['std_bad_f1']:.4f}")
    print(f"   å¹³å‡å®F1: {best_cv_result['avg_macro_f1']:.4f}")
    print(f"   å¹³å‡å‡†ç¡®ç‡: {best_cv_result['avg_accuracy']:.4f}")
    
    return cv_results, summary_results, best_cv_strategy, best_cv_result

# ä¿®æ”¹æµ‹è¯•é›†é¢„æµ‹éƒ¨åˆ†ï¼Œä½¿ç”¨æœ€ä¼˜é˜ˆå€¼
# åœ¨Phase 5çš„æœ€åï¼Œä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¿›è¡Œæµ‹è¯•é›†é¢„æµ‹
def predict_test_with_optimal_threshold(meta_model, feature_scaler, test_combined_base_predictions, 
                                      test_original_features, optimal_threshold, device):
    """
    ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
    """
    print(f"ğŸ¤– ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼ {optimal_threshold:.3f} è¿›è¡Œæµ‹è¯•é¢„æµ‹...")
    
    test_original_features_scaled = feature_scaler.transform(test_original_features)
    
    X_test_base_tensor = torch.tensor(test_combined_base_predictions, dtype=torch.float32).to(device)
    X_test_feat_tensor = torch.tensor(test_original_features_scaled, dtype=torch.float32).to(device)
    
    meta_model.eval()
    with torch.no_grad():
        test_outputs = meta_model(X_test_base_tensor, X_test_feat_tensor)
        test_probabilities = F.softmax(test_outputs, dim=1)
        test_bad_probabilities = test_probabilities[:, 1].cpu().numpy()
        
        # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¿›è¡Œåˆ†ç±»
        test_final_labels = (test_bad_probabilities >= optimal_threshold).astype(int)
    
    return test_final_labels, test_bad_probabilities