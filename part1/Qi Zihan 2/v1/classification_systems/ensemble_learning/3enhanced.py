import pandas as pd
import numpy as np
import os
from tqdm import tqdm
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import StratifiedKFold
import xgboost as xgb
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')

print("=== ULTRA ENHANCED Ensemble System v3.0 ===")
print("Multi-model ensemble with probability averaging and adaptive thresholds")

def extract_ultra_features(df):
    """æ›´æ¿€è¿›çš„ç‰¹å¾å·¥ç¨‹"""
    # åŸºç¡€ç‰¹å¾
    df['has_forward_cnt'] = (df['normal_fprofit'] > 0) | (df['normal_fsize'] > 0) | \
                           (df['abnormal_fprofit'] > 0) | (df['abnormal_fsize'] > 0) | \
                           (df['bad_fprofit'] > 0) | (df['bad_fsize'] > 0)
    
    df['has_backward_cnt'] = (df['normal_bprofit'] > 0) | (df['normal_bsize'] > 0) | \
                            (df['abnormal_bprofit'] > 0) | (df['abnormal_bsize'] > 0) | \
                            (df['bad_bprofit'] > 0) | (df['bad_bsize'] > 0)
    
    df['total_forward_transactions'] = df['normal_fsize'] + df['abnormal_fsize'] + df['bad_fsize']
    df['total_backward_transactions'] = df['normal_bsize'] + df['abnormal_bsize'] + df['bad_bsize']
    df['total_transactions'] = df['total_forward_transactions'] + df['total_backward_transactions']
    
    df['total_forward_profit'] = df['normal_fprofit'] + df['abnormal_fprofit'] + df['bad_fprofit']
    df['total_backward_profit'] = df['normal_bprofit'] + df['abnormal_bprofit'] + df['bad_bprofit']
    df['total_profit'] = df['total_forward_profit'] + df['total_backward_profit']
    
    df['has_A_forward'] = (df['A_fprofit'] > 0) | (df['A_fsize'] > 0)
    df['has_B_forward'] = (df['B_fprofit'] > 0) | (df['B_fsize'] > 0)
    df['has_A_backward'] = (df['A_bprofit'] > 0) | (df['A_bsize'] > 0)
    df['has_B_backward'] = (df['B_bprofit'] > 0) | (df['B_bsize'] > 0)
    
    # æ–°å¢äº¤äº’ç‰¹å¾
    df['B_total_profit'] = df['B_fprofit'] + df['B_bprofit']
    df['B_total_size'] = df['B_fsize'] + df['B_bsize']
    df['A_total_profit'] = df['A_fprofit'] + df['A_bprofit'] 
    df['A_total_size'] = df['A_fsize'] + df['A_bsize']
    
    # æ¯”ä¾‹ç‰¹å¾
    df['forward_backward_ratio'] = np.where(df['total_backward_transactions'] > 0, 
                                           df['total_forward_transactions'] / df['total_backward_transactions'], 
                                           df['total_forward_transactions'])
    
    df['profit_per_transaction'] = np.where(df['total_transactions'] > 0, 
                                          df['total_profit'] / df['total_transactions'], 0)
    
    # é«˜é˜¶äº¤äº’ç‰¹å¾
    df['B_profit_density'] = np.where(df['B_total_size'] > 0, df['B_total_profit'] / df['B_total_size'], 0)
    df['A_profit_density'] = np.where(df['A_total_size'] > 0, df['A_total_profit'] / df['A_total_size'], 0)
    
    # å¯¹æ•°ç‰¹å¾ï¼ˆå¤„ç†æå€¼ï¼‰
    for col in ['total_profit', 'B_total_profit', 'A_total_profit']:
        df[f'log_{col}'] = np.sign(df[col]) * np.log1p(np.abs(df[col]))
    
    # å¹³æ–¹å’Œäº¤å‰ç‰¹å¾ï¼ˆé‡è¦ç‰¹å¾çš„ç»„åˆï¼‰
    df['B_profit_size_interaction'] = df['B_fprofit'] * df['B_fsize']
    df['profit_transaction_ratio'] = np.where(df['total_transactions'] > 0, 
                                            df['total_profit'] / np.sqrt(df['total_transactions']), 0)
    
    return df

def classify_account_type_enhanced(row):
    """å¢å¼ºçš„è´¦æˆ·åˆ†ç±»"""
    has_forward = row['has_forward_cnt']
    has_backward = row['has_backward_cnt']
    
    if has_forward and has_backward:
        return 'type1'
    elif has_forward and not has_backward:
        return 'type2'
    elif not has_forward and has_backward:
        return 'type3'
    else:
        return 'type4'

def create_diverse_models():
    """åˆ›å»ºå¤šæ ·åŒ–çš„æ¨¡å‹é›†åˆ"""
    models = []
    
    # RandomForestå˜ä½“
    for i in range(30):
        rf = RandomForestClassifier(
            n_estimators=200 + i*10,
            max_depth=15 + i%5,
            min_samples_split=3 + i%3,
            min_samples_leaf=1 + i%2,
            max_features='sqrt' if i%2==0 else 'log2',
            random_state=i
        )
        models.append(('RF', rf))
    
    # XGBoostå˜ä½“  
    for i in range(25):
        xgb_model = xgb.XGBClassifier(
            n_estimators=300 + i*20,
            max_depth=6 + i%3,
            learning_rate=0.05 + (i%5)*0.01,
            subsample=0.8 + (i%3)*0.05,
            colsample_bytree=0.8 + (i%3)*0.05,
            random_state=i,
            eval_metric='logloss'
        )
        models.append(('XGB', xgb_model))
    
    # LightGBMå˜ä½“
    for i in range(25):
        lgb_model = lgb.LGBMClassifier(
            n_estimators=400 + i*20,
            max_depth=8 + i%3,
            learning_rate=0.05 + (i%5)*0.01,
            feature_fraction=0.8 + (i%3)*0.05,
            bagging_fraction=0.8 + (i%3)*0.05,
            random_state=i,
            verbosity=-1
        )
        models.append(('LGB', lgb_model))
    
    # é€»è¾‘å›å½’å˜ä½“
    for i in range(20):
        lr = LogisticRegression(
            C=0.1 * (10 ** (i/10.0)),
            penalty='l1' if i%2==0 else 'l2',
            solver='liblinear',
            random_state=i,
            max_iter=1000
        )
        models.append(('LR', lr))
    
    print(f"Created {len(models)} diverse models")
    return models

def train_enhanced_ensemble(data, account_type, n_models=100):
    """è®­ç»ƒå¢å¼ºçš„ensembleï¼Œä½¿ç”¨æ¦‚ç‡å¹³å‡"""
    print(f"\nTraining enhanced ensemble for {account_type}:")
    print(f"Total accounts: {len(data)}")
    
    data_copy = data.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    
    flag_counts = data_copy['flag'].value_counts()
    print(f"Flag distribution: {dict(flag_counts)}")
    
    feature_cols = [col for col in data_copy.columns if col not in ['account', 'flag', 'account_type']]
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    
    if good_accounts == 0:
        return None, None, None
    
    # åˆ›å»ºå¤šæ ·åŒ–æ¨¡å‹
    models = create_diverse_models()
    
    # è®­ç»ƒæ¨¡å‹å¹¶æ”¶é›†æ¦‚ç‡é¢„æµ‹
    probability_predictions = []
    model_weights = []
    
    # 5æŠ˜äº¤å‰éªŒè¯æ¥è¯„ä¼°æ¨¡å‹è´¨é‡
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    X_all = data_copy[feature_cols].values
    y_all = data_copy['flag'].values
    
    for idx, (model_name, model) in enumerate(tqdm(models[:n_models], desc=f"Training {account_type} models")):
        try:
            # å¹³è¡¡é‡‡æ ·
            sample_size = min(good_accounts, bad_accounts)
            good_sample = data_copy[data_copy['flag'] == 1].sample(n=sample_size, replace=True, random_state=idx)
            bad_sample = data_copy[data_copy['flag'] == 0].sample(n=sample_size, replace=True, random_state=idx)
            train_data = pd.concat([good_sample, bad_sample], axis=0)
            
            X_train = train_data[feature_cols].values
            y_train = train_data['flag'].values
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_train, y_train)
            
            # é¢„æµ‹æ¦‚ç‡
            y_proba = model.predict_proba(X_all)[:, 1]  # æ­£ç±»æ¦‚ç‡
            probability_predictions.append(y_proba)
            
            # è®¡ç®—æ¨¡å‹æƒé‡ï¼ˆåŸºäºäº¤å‰éªŒè¯æ€§èƒ½ï¼‰
            cv_scores = []
            for train_idx, val_idx in kfold.split(X_train, y_train):
                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
                y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]
                
                model_copy = type(model)(**model.get_params())
                model_copy.fit(X_fold_train, y_fold_train)
                y_fold_pred = model_copy.predict(X_fold_val)
                cv_scores.append(metrics.f1_score(y_fold_val, y_fold_pred))
            
            weight = np.mean(cv_scores)
            model_weights.append(weight)
            
        except Exception as e:
            print(f"Model {idx} ({model_name}) failed: {e}")
            continue
    
    if not probability_predictions:
        return None, None, None
    
    # åŠ æƒæ¦‚ç‡å¹³å‡
    probability_predictions = np.array(probability_predictions)
    model_weights = np.array(model_weights)
    model_weights = model_weights / np.sum(model_weights)  # å½’ä¸€åŒ–
    
    weighted_probabilities = np.average(probability_predictions, axis=0, weights=model_weights)
    
    # åŠ¨æ€é˜ˆå€¼é€‰æ‹©
    thresholds = np.arange(0.1, 0.9, 0.02)
    best_threshold = 0.5
    best_f1 = 0
    
    for threshold in thresholds:
        y_pred = (weighted_probabilities >= threshold).astype(int)
        f1 = metrics.f1_score(y_all, y_pred)
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    print(f"Best threshold: {best_threshold:.3f}, F1: {best_f1:.4f}")
    
    final_predictions = (weighted_probabilities >= best_threshold).astype(int)
    
    return probability_predictions, final_predictions, best_threshold

# ä¸»æµç¨‹
features_path = '/Users/mannormal/4011/Qi Zihan/feature_extraction/generated_features/all_features.csv'
all_features_df = pd.read_csv(features_path)

print("Extracting ultra features...")
all_features_df = extract_ultra_features(all_features_df)
print(f"Enhanced features shape: {all_features_df.shape}")

# åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
pwd = '/Users/mannormal/4011/Qi Zihan/original_data/'
ta = pd.read_csv(pwd + 'train_acc.csv')
te = pd.read_csv(pwd + 'test_acc_predict.csv')
ta.loc[ta['flag'] == 0, 'flag'] = -1

# åˆå¹¶è®­ç»ƒæ•°æ®
training_df = pd.merge(all_features_df, ta[['account', 'flag']], on='account', how='inner')
training_df['account_type'] = training_df.apply(classify_account_type_enhanced, axis=1)

print(f"Training data ready: {training_df.shape}")
print("Account type distribution:")
print(training_df['account_type'].value_counts())

# è®­ç»ƒå¢å¼ºçš„ensembleæ¨¡å‹
type_data = {}
enhanced_models = {}
type_predictions = {}
type_thresholds = {}

for account_type in ['type1', 'type2', 'type3', 'type4']:
    type_data[account_type] = training_df[training_df['account_type'] == account_type].copy()
    
    if len(type_data[account_type]) > 0:
        predictions_array, final_predictions, threshold = train_enhanced_ensemble(
            type_data[account_type], account_type, n_models=100
        )
        
        if predictions_array is not None:
            enhanced_models[account_type] = predictions_array
            type_predictions[account_type] = final_predictions
            type_thresholds[account_type] = threshold

# å¤„ç†æµ‹è¯•æ•°æ®
print("\nProcessing test accounts...")
test_df = pd.merge(all_features_df, te[['account']], on='account', how='inner')
test_df['account_type'] = test_df.apply(classify_account_type_enhanced, axis=1)

print(f"Test data ready: {test_df.shape}")
print("Test account type distribution:")
print(test_df['account_type'].value_counts())

# ç”Ÿæˆæµ‹è¯•é¢„æµ‹ï¼ˆè¿™éƒ¨åˆ†éœ€è¦ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
print("\nGenerating enhanced test predictions...")

# ä¸ºäº†è¿›è¡Œäº¤å‰éªŒè¯åˆ†æï¼Œæˆ‘ä»¬éœ€è¦å°†è®­ç»ƒæ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
from sklearn.model_selection import train_test_split

print("\nPreparing cross-validation analysis...")
type_data_train = {}
type_data_val = {}
type_predictions_train = {}
type_predictions_val = {}

for account_type in ['type1', 'type2', 'type3', 'type4']:
    if account_type in type_data and len(type_data[account_type]) > 10:
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_data, val_data = train_test_split(
            type_data[account_type], 
            test_size=0.2, 
            random_state=42, 
            stratify=type_data[account_type]['flag']
        )
        
        type_data_train[account_type] = train_data
        type_data_val[account_type] = val_data
        
        # ä¸ºç®€åŒ–ï¼Œä½¿ç”¨å·²æœ‰çš„é¢„æµ‹ç»“æœï¼ˆå®é™…åº”è¯¥é‡æ–°è®­ç»ƒï¼‰
        if account_type in type_predictions:
            # è·å–è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç´¢å¼•
            train_indices = train_data.index
            val_indices = val_data.index
            original_indices = type_data[account_type].index
            
            # åˆ›å»ºç´¢å¼•æ˜ å°„
            index_to_pos = {idx: pos for pos, idx in enumerate(original_indices)}
            train_positions = [index_to_pos[idx] for idx in train_indices if idx in index_to_pos]
            val_positions = [index_to_pos[idx] for idx in val_indices if idx in index_to_pos]
            
            # è·å–å¯¹åº”çš„é¢„æµ‹ç»“æœ
            if len(train_positions) > 0 and len(val_positions) > 0:
                all_predictions = type_predictions[account_type]
                type_predictions_train[account_type] = all_predictions[train_positions]
                type_predictions_val[account_type] = all_predictions[val_positions]

# è®¡ç®—è®­ç»ƒé›†å’ŒéªŒè¯é›†F1åˆ†æ•°å¯¹æ¯”
print("\n" + "="*60)
print("CROSS-VALIDATION F1-SCORE ANALYSIS")
print("="*60)

overall_train_f1 = 0
overall_val_f1 = 0
total_train_accounts = 0
total_val_accounts = 0

print(f"{'Type':<8} {'Train Acc':<10} {'Train F1':<10} {'Val Acc':<10} {'Val F1':<10} {'Overfitting':<12}")
print("-" * 70)

for account_type in ['type1', 'type2', 'type3', 'type4']:
    if account_type not in type_predictions_train or account_type not in type_predictions_val:
        continue
        
    # è®­ç»ƒé›†æ€§èƒ½
    train_data = type_data_train[account_type]
    y_train_true = np.where(train_data['flag'].values == -1, 0, 1)
    y_train_pred = type_predictions_train[account_type]
    
    # éªŒè¯é›†æ€§èƒ½
    val_data = type_data_val[account_type]
    y_val_true = np.where(val_data['flag'].values == -1, 0, 1)
    y_val_pred = type_predictions_val[account_type]
    
    # ç¡®ä¿é•¿åº¦ä¸€è‡´
    if len(y_train_true) == len(y_train_pred) and len(y_val_true) == len(y_val_pred):
        # è®­ç»ƒé›†æŒ‡æ ‡
        train_f1 = metrics.f1_score(y_train_true, y_train_pred, average='binary', zero_division=0)
        train_acc = metrics.accuracy_score(y_train_true, y_train_pred)
        
        # éªŒè¯é›†æŒ‡æ ‡
        val_f1 = metrics.f1_score(y_val_true, y_val_pred, average='binary', zero_division=0)
        val_acc = metrics.accuracy_score(y_val_true, y_val_pred)
        
        # è¿‡æ‹Ÿåˆæ£€æµ‹
        overfitting = train_f1 - val_f1
        overfitting_status = "HIGH" if overfitting > 0.1 else "MEDIUM" if overfitting > 0.05 else "LOW"
        
        print(f"{account_type:<8} {train_acc:<10.3f} {train_f1:<10.3f} {val_acc:<10.3f} {val_f1:<10.3f} {overfitting_status:<12}")
        
        # æƒé‡è®¡ç®—ï¼ˆåŸºäºæ ·æœ¬é‡ï¼‰
        train_weight = len(train_data)
        val_weight = len(val_data)
        
        overall_train_f1 += train_f1 * train_weight
        overall_val_f1 += val_f1 * val_weight
        total_train_accounts += train_weight
        total_val_accounts += val_weight
        
        # è¯¦ç»†æŠ¥å‘Š
        print(f"\n{account_type.upper()} è¯¦ç»†åˆ†æ:")
        print(f"  è®­ç»ƒé›†: {len(train_data)}è´¦æˆ·, F1={train_f1:.4f}, Acc={train_acc:.4f}")
        print(f"  éªŒè¯é›†: {len(val_data)}è´¦æˆ·, F1={val_f1:.4f}, Acc={val_acc:.4f}")
        print(f"  è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting:.4f} ({overfitting_status})")
        
        # æ··æ·†çŸ©é˜µ
        print(f"  è®­ç»ƒæ··æ·†çŸ©é˜µ:\n{metrics.confusion_matrix(y_train_true, y_train_pred)}")
        print(f"  éªŒè¯æ··æ·†çŸ©é˜µ:\n{metrics.confusion_matrix(y_val_true, y_val_pred)}")

# è®¡ç®—æ€»ä½“æ€§èƒ½
if total_train_accounts > 0 and total_val_accounts > 0:
    overall_train_f1 /= total_train_accounts
    overall_val_f1 /= total_val_accounts
else:
    print("è­¦å‘Š: æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œæ€§èƒ½è¯„ä¼°")
    overall_train_f1 = 0
    overall_val_f1 = 0

print(f"\n" + "="*60)
print("ğŸ¯ CROSS-VALIDATION SUMMARY")
print("="*60)
print(f"è®­ç»ƒé›†æ€»ä½“F1:   {overall_train_f1:.4f}")
print(f"éªŒè¯é›†æ€»ä½“F1:   {overall_val_f1:.4f}")
print(f"æ³›åŒ–å·®è·:       {overall_train_f1 - overall_val_f1:.4f}")

if overall_train_f1 - overall_val_f1 > 0.1:
    print("âš ï¸  ä¸¥é‡è¿‡æ‹Ÿåˆæ£€æµ‹åˆ°ï¼")
    print("   å»ºè®®:")
    print("   - å‡å°‘æ¨¡å‹å¤æ‚åº¦")
    print("   - å¢åŠ æ­£åˆ™åŒ–")
    print("   - å¢åŠ è®­ç»ƒæ•°æ®")
elif overall_train_f1 - overall_val_f1 > 0.05:
    print("âš ï¸  ä¸­åº¦è¿‡æ‹Ÿåˆæ£€æµ‹åˆ°")
    print("   æ¨¡å‹å¯èƒ½åœ¨æµ‹è¯•é›†ä¸Šæ€§èƒ½ä¸‹é™")
else:
    print("âœ… è¿‡æ‹Ÿåˆç¨‹åº¦å¯æ¥å—")
    print("   æ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½")

print(f"\nçœŸå®æ€§èƒ½é¢„æœŸ:")
print(f"  AutoGluonåŸºçº¿:     0.6201")
print(f"  ä½ çš„è®­ç»ƒF1:       {overall_train_f1:.4f}")
print(f"  ä½ çš„éªŒè¯F1:       {overall_val_f1:.4f}")
print(f"  é¢„æœŸæµ‹è¯•F1:       {overall_val_f1:.4f} Â± 0.02")

if overall_val_f1 > 0.6201:
    improvement = overall_val_f1 - 0.6201
    print(f"âœ… ç›¸å¯¹AutoGluonæå‡: +{improvement:.4f}")
else:
    decline = 0.6201 - overall_val_f1
    print(f"âŒ ç›¸å¯¹AutoGluonä¸‹é™: -{decline:.4f}")

print(f"\nå¢å¼ºç‰¹å¾æ•°: {all_features_df.shape[1]-1}")
print(f"åˆ†æçš„æ€»è´¦æˆ·æ•°: {total_train_accounts + total_val_accounts}")