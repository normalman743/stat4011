# filepath: /Users/mannormal/4011/Qi Zihan/classification_systems/ensemble_learning/meta_learning_natxis_classification.py
import pandas as pd
import numpy as np
import os
from tqdm import tqdm
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn import metrics
import warnings
warnings.filterwarnings('ignore')

print("=== Meta-Learning NATXIS Classification System ===")
print("ä½¿ç”¨é€»è¾‘å›å½’ä½œä¸ºå…ƒåˆ†ç±»å™¨ï¼Œä¼˜äºç¡¬æŠ•ç¥¨é›†æˆ")

def classify_account_type(row):
    """å°†è´¦æˆ·åˆ†ä¸º4ç§ç±»å‹åŸºäºäº¤æ˜“æ¨¡å¼"""
    has_forward = (row['normal_fprofit'] > 0 or row['abnormal_fprofit'] > 0 or 
                   row['normal_fsize'] > 0 or row['abnormal_fsize'] > 0)
    
    has_backward = (row['normal_bprofit'] > 0 or row['abnormal_bprofit'] > 0 or
                    row['normal_bsize'] > 0 or row['abnormal_bsize'] > 0)
    
    if has_forward and has_backward:
        return 'type1'
    elif has_forward and not has_backward:
        return 'type2'
    elif not has_forward and has_backward:
        return 'type3'
    else:
        return 'type4'

def train_meta_learning_ensemble(data, account_type, n_base_models=50, n_meta_models=10):
    """è®­ç»ƒåŸºäºå…ƒå­¦ä¹ çš„é›†æˆæ¨¡å‹"""
    print(f"\nè®­ç»ƒå…ƒå­¦ä¹ é›†æˆæ¨¡å‹ {account_type}:")
    print(f"æ€»è´¦æˆ·æ•°: {len(data)}")
    
    # å‡†å¤‡æ•°æ®
    flag_counts = data['flag'].value_counts()
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(flag_counts)}")
    
    data_copy = data.copy()
    data_copy.loc[data_copy['flag'] == -1, 'flag'] = 0
    
    feature_cols = [col for col in data_copy.columns if col not in ['account', 'flag', 'account_type']]
    
    good_accounts = len(data_copy[data_copy['flag'] == 1])
    bad_accounts = len(data_copy[data_copy['flag'] == 0])
    
    if good_accounts == 0:
        print("æœªæ‰¾åˆ°å¥½è´¦æˆ·ï¼Œè·³è¿‡æ­¤ç±»å‹")
        return None, None, None
        
    sample_size = min(good_accounts, bad_accounts)
    print(f"ä½¿ç”¨å¹³è¡¡é‡‡æ ·: æ¯ç±» {sample_size} ä¸ªè´¦æˆ·")
    
    # ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒåŸºç¡€åˆ†ç±»å™¨
    print("ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒåŸºç¡€åˆ†ç±»å™¨...")
    base_predictions = []
    base_models = []
    
    for i in tqdm(range(n_base_models), desc=f"è®­ç»ƒåŸºç¡€æ¨¡å‹"):
        # å¹³è¡¡é‡‡æ ·
        good_sample = data_copy[data_copy['flag'] == 1].sample(n=sample_size, replace=True, random_state=i)
        bad_sample = data_copy[data_copy['flag'] == 0].sample(n=sample_size, replace=True, random_state=i)
        
        train_data = pd.concat([good_sample, bad_sample], axis=0)
        X_train = train_data[feature_cols].values
        y_train = train_data['flag'].values
        
        # è®­ç»ƒéšæœºæ£®æ—
        clf = RandomForestClassifier(n_estimators=100, random_state=i, max_depth=10)
        clf.fit(X_train, y_train)
        base_models.append(clf)
        
        # åœ¨å…¨éƒ¨æ•°æ®ä¸Šé¢„æµ‹æ¦‚ç‡
        X_all = data_copy[feature_cols].values
        y_pred_proba = clf.predict_proba(X_all)[:, 1]  # å¥½è´¦æˆ·çš„æ¦‚ç‡
        base_predictions.append(y_pred_proba)
    
    # ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒå…ƒåˆ†ç±»å™¨
    print("ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒå…ƒåˆ†ç±»å™¨...")
    base_predictions_array = np.array(base_predictions).T  # è½¬ç½®ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬çš„æ‰€æœ‰åŸºç¡€é¢„æµ‹
    y_true = data_copy['flag'].values
    
    # ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒå…ƒåˆ†ç±»å™¨
    meta_models = []
    meta_predictions = []
    
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(base_predictions_array, y_true)):
        X_meta_train = base_predictions_array[train_idx]
        y_meta_train = y_true[train_idx]
        X_meta_val = base_predictions_array[val_idx]
        
        # è®­ç»ƒé€»è¾‘å›å½’å…ƒåˆ†ç±»å™¨
        meta_clf = LogisticRegression(random_state=fold, max_iter=1000)
        meta_clf.fit(X_meta_train, y_meta_train)
        meta_models.append(meta_clf)
        
        # åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹
        val_pred = meta_clf.predict(X_meta_val)
        meta_predictions.append((val_idx, val_pred))
    
    # ç»„åˆæ‰€æœ‰éªŒè¯é¢„æµ‹
    final_meta_predictions = np.zeros(len(data_copy))
    for val_idx, val_pred in meta_predictions:
        final_meta_predictions[val_idx] = val_pred
    
    # è®¡ç®—å…ƒå­¦ä¹ å‡†ç¡®ç‡
    meta_accuracy = metrics.accuracy_score(y_true, final_meta_predictions)
    
    # æ¯”è¾ƒç¡¬æŠ•ç¥¨ç»“æœ
    hard_voting_predictions = np.where(np.mean(base_predictions_array, axis=1) > 0.5, 1, 0)
    hard_voting_accuracy = metrics.accuracy_score(y_true, hard_voting_predictions)
    
    print(f"å…ƒå­¦ä¹ å‡†ç¡®ç‡: {meta_accuracy:.4f}")
    print(f"ç¡¬æŠ•ç¥¨å‡†ç¡®ç‡: {hard_voting_accuracy:.4f}")
    print(f"å…ƒå­¦ä¹ æå‡: {meta_accuracy - hard_voting_accuracy:.4f}")
    
    return base_models, meta_models, final_meta_predictions

def predict_with_meta_learning(base_models, meta_models, test_data, feature_cols):
    """ä½¿ç”¨å…ƒå­¦ä¹ æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    X_test = test_data[feature_cols].values
    
    # åŸºç¡€æ¨¡å‹é¢„æµ‹
    base_predictions = []
    for model in base_models:
        pred_proba = model.predict_proba(X_test)[:, 1]
        base_predictions.append(pred_proba)
    
    base_predictions_array = np.array(base_predictions).T
    
    # å…ƒåˆ†ç±»å™¨é¢„æµ‹
    meta_predictions = []
    for meta_model in meta_models:
        meta_pred = meta_model.predict(base_predictions_array)
        meta_predictions.append(meta_pred)
    
    # å¯¹å…ƒåˆ†ç±»å™¨ç»“æœè¿›è¡ŒæŠ•ç¥¨
    meta_predictions_array = np.array(meta_predictions)
    final_predictions = np.where(np.mean(meta_predictions_array, axis=0) > 0.5, 1, 0)
    
    return final_predictions

# åŠ è½½æ•°æ®
features_path = '/Users/mannormal/4011/Qi Zihan/feature_extraction/generated_features/all_features.csv'
if os.path.exists(features_path):
    print("åŠ è½½é¢„æå–ç‰¹å¾...")
    all_features_df = pd.read_csv(features_path)
    print(f"åŠ è½½ç‰¹å¾å½¢çŠ¶: {all_features_df.shape}")
else:
    print(f"é”™è¯¯: {features_path} æœªæ‰¾åˆ°!")
    exit()

# åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
pwd = '/Users/mannormal/4011/Qi Zihan/original_data/'
ta = pd.read_csv(pwd + 'train_acc.csv')
te = pd.read_csv(pwd + 'test_acc_predict.csv')

ta.loc[ta['flag'] == 0, 'flag'] = -1

print(f"è®­ç»ƒè´¦æˆ·: {ta.shape[0]}")
print(f"æµ‹è¯•è´¦æˆ·: {te.shape[0]}")

# åˆå¹¶è®­ç»ƒæ•°æ®
training_df = pd.merge(all_features_df, ta[['account', 'flag']], on='account', how='inner')
training_df['account_type'] = training_df.apply(classify_account_type, axis=1)

print(f"\nè®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ: {training_df.shape}")
print("è´¦æˆ·ç±»å‹åˆ†å¸ƒ:")
print(training_df['account_type'].value_counts())

# æŒ‰è´¦æˆ·ç±»å‹æ‹†åˆ†æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹
type_data = {}
meta_learning_models = {}
type_predictions = {}

for account_type in ['type1', 'type2', 'type3', 'type4']:
    type_data[account_type] = training_df[training_df['account_type'] == account_type].copy()
    
    if len(type_data[account_type]) > 0:
        base_models, meta_models, final_predictions = train_meta_learning_ensemble(
            type_data[account_type], 
            account_type,
            n_base_models=50,
            n_meta_models=10
        )
        
        if base_models is not None:
            meta_learning_models[account_type] = {
                'base_models': base_models,
                'meta_models': meta_models
            }
            type_predictions[account_type] = final_predictions

print(f"\nä¸ºä»¥ä¸‹ç±»å‹è®­ç»ƒäº†æ¨¡å‹: {list(meta_learning_models.keys())}")

# å¤„ç†æµ‹è¯•è´¦æˆ·
print("\nå¤„ç†æµ‹è¯•è´¦æˆ·...")
test_df = pd.merge(all_features_df, te[['account']], on='account', how='inner')
test_df['account_type'] = test_df.apply(classify_account_type, axis=1)

print(f"æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆ: {test_df.shape}")
print("æµ‹è¯•è´¦æˆ·ç±»å‹åˆ†å¸ƒ:")
print(test_df['account_type'].value_counts())

# å¯¹æµ‹è¯•è´¦æˆ·è¿›è¡Œå…ƒå­¦ä¹ é¢„æµ‹
print("\nå¯¹æµ‹è¯•è´¦æˆ·è¿›è¡Œå…ƒå­¦ä¹ é¢„æµ‹...")
test_predictions = {}

for account_type in ['type1', 'type2', 'type3', 'type4']:
    type_test_data = test_df[test_df['account_type'] == account_type].copy()
    
    if len(type_test_data) > 0 and account_type in meta_learning_models:
        print(f"é¢„æµ‹ {account_type}: {len(type_test_data)} ä¸ªè´¦æˆ·")
        
        feature_cols = [col for col in type_test_data.columns if col not in ['account', 'account_type']]
        
        base_models = meta_learning_models[account_type]['base_models']
        meta_models = meta_learning_models[account_type]['meta_models']
        
        final_predictions = predict_with_meta_learning(
            base_models, meta_models, type_test_data, feature_cols
        )
        
        print(f"{account_type} é¢„æµ‹åˆ†å¸ƒ: {np.bincount(final_predictions)}")
        test_predictions[account_type] = {
            'accounts': type_test_data['account'].values,
            'predictions': final_predictions
        }

# åˆå¹¶æµ‹è¯•é¢„æµ‹ç»“æœ
print("\nåˆå¹¶æµ‹è¯•é¢„æµ‹ç»“æœ...")
final_test_results = []

for account_type in ['type1', 'type2', 'type3', 'type4']:
    if account_type in test_predictions:
        accounts = test_predictions[account_type]['accounts']
        predictions = test_predictions[account_type]['predictions']
        
        for acc, pred in zip(accounts, predictions):
            final_test_results.append({'account': acc, 'flag': pred})

results_df = pd.DataFrame(final_test_results)
print(f"æœ€ç»ˆæµ‹è¯•ç»“æœ: {len(results_df)} ä¸ªè´¦æˆ·")
print("é¢„æµ‹åˆ†å¸ƒ:")
print(results_df['flag'].value_counts())

# ä¿å­˜ç»“æœ
output_path = '../../result_analysis/prediction_results/meta_learning_predictions.csv'
results_df.to_csv(output_path, index=False)
print(f"å…ƒå­¦ä¹ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {output_path}")

# è®¡ç®—è®­ç»ƒæ•°æ®çš„F1åˆ†æ•°
print("\n" + "="*60)
print("å…ƒå­¦ä¹ ç³»ç»Ÿ F1-SCORE åˆ†æ")
print("="*60)

overall_f1_binary = 0
overall_f1_weighted = 0
overall_f1_macro = 0
total_accounts = 0

for account_type in ['type1', 'type2', 'type3', 'type4']:
    type_training_data = training_df[training_df['account_type'] == account_type].copy()
    
    if len(type_training_data) == 0 or account_type not in type_predictions:
        print(f"{account_type.upper()}: æ— æ•°æ®æˆ–é¢„æµ‹ç»“æœ")
        continue
    
    y_true = np.where(type_training_data['flag'].values == -1, 0, 1)
    y_pred = type_predictions[account_type]
    
    if len(y_true) != len(y_pred):
        print(f"{account_type.upper()}: é•¿åº¦ä¸åŒ¹é… - çœŸå®: {len(y_true)}, é¢„æµ‹: {len(y_pred)}")
        continue
    
    try:
        f1_binary = metrics.f1_score(y_true, y_pred, average='binary', zero_division=0)
        f1_weighted = metrics.f1_score(y_true, y_pred, average='weighted', zero_division=0)
        f1_macro = metrics.f1_score(y_true, y_pred, average='macro', zero_division=0)
        accuracy = metrics.accuracy_score(y_true, y_pred)
        
        true_counts = np.bincount(y_true)
        pred_counts = np.bincount(y_pred)
        
        print(f"\n{account_type.upper()} è¯¦ç»†ç»“æœ:")
        print(f"  è´¦æˆ·æ•°é‡: {len(type_training_data)}")
        print(f"  çœŸå®æ ‡ç­¾åˆ†å¸ƒ: Good={true_counts[1] if len(true_counts)>1 else 0}, Bad={true_counts[0]}")
        print(f"  é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ: Good={pred_counts[1] if len(pred_counts)>1 else 0}, Bad={pred_counts[0]}")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  F1-Score (binary): {f1_binary:.4f}")
        print(f"  F1-Score (weighted): {f1_weighted:.4f}")
        print(f"  F1-Score (macro): {f1_macro:.4f}")
        
        weight = len(type_training_data)
        overall_f1_binary += f1_binary * weight
        overall_f1_weighted += f1_weighted * weight
        overall_f1_macro += f1_macro * weight
        total_accounts += weight
        
    except Exception as e:
        print(f"{account_type.upper()}: è®¡ç®—F1åˆ†æ•°æ—¶å‡ºé”™: {e}")

if total_accounts > 0:
    overall_f1_binary /= total_accounts
    overall_f1_weighted /= total_accounts
    overall_f1_macro /= total_accounts

print(f"\n" + "="*60)
print("ğŸ† å…ƒå­¦ä¹ NATXISåˆ†ç±»ç³»ç»Ÿæ€»ç»“")
print("="*60)
print(f"æ€»ä½“F1-Score (binary):   {overall_f1_binary:.4f}")
print(f"æ€»ä½“F1-Score (weighted): {overall_f1_weighted:.4f}")
print(f"æ€»ä½“F1-Score (macro):    {overall_f1_macro:.4f}")
print(f"åˆ†æçš„æ€»è´¦æˆ·æ•°: {total_accounts}")
print(f"ä½¿ç”¨ç‰¹å¾æ•°: 31ä¸ªåŸºç¡€ç‰¹å¾")

print(f"\næµ‹è¯•é¢„æµ‹ç»Ÿè®¡:")
print(f"æµ‹è¯•è´¦æˆ·æ€»æ•°: {len(results_df)}")
print(f"é¢„æµ‹ä¸ºGoodçš„è´¦æˆ·: {len(results_df[results_df['flag']==1])}")
print(f"é¢„æµ‹ä¸ºBadçš„è´¦æˆ·: {len(results_df[results_df['flag']==0])}")
print(f"Goodè´¦æˆ·æ¯”ä¾‹: {len(results_df[results_df['flag']==1])/len(results_df)*100:.1f}%")

print("\n=== å…ƒå­¦ä¹ NATXISåˆ†ç±»å®Œæˆ ===")
print("âœ… ä½¿ç”¨é€»è¾‘å›å½’å…ƒåˆ†ç±»å™¨æå‡ç¡¬æŠ•ç¥¨æ•ˆæœ!")
print("ğŸ¯ é¢„æœŸä¼˜äºç¡¬æŠ•ç¥¨é›†æˆæ–¹æ³•")
print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {output_path}")