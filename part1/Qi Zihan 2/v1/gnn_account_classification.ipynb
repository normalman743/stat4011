{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify data loading and format\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Test loading the data files directly in local environment\n",
    "try:\n",
    "    # Try loading from the current directory (adapted for local environment)\n",
    "    train_df = pd.read_csv('train_acc.csv') if pd.io.common.file_exists('train_acc.csv') else None\n",
    "    test_df = pd.read_csv('test_acc_predict.csv') if pd.io.common.file_exists('test_acc_predict.csv') else None\n",
    "    transactions_df = pd.read_csv('transactions.csv') if pd.io.common.file_exists('transactions.csv') else None\n",
    "    \n",
    "    if train_df is not None and test_df is not None and transactions_df is not None:\n",
    "        print(\"âœ… Data files found in current directory\")\n",
    "        print(f\"- Train: {len(train_df)} samples\")\n",
    "        print(f\"- Test: {len(test_df)} samples\")\n",
    "        print(f\"- Transactions: {len(transactions_df)} records\")\n",
    "        print(f\"- Transaction columns: {list(transactions_df.columns)}\")\n",
    "        \n",
    "        # Test the amount column processing\n",
    "        amount_col = 'value' if 'value' in transactions_df.columns else 'amount'\n",
    "        if amount_col in transactions_df.columns:\n",
    "            try:\n",
    "                amount_values = pd.to_numeric(transactions_df[amount_col], errors='coerce')\n",
    "                amount_min = amount_values.min()\n",
    "                amount_max = amount_values.max()\n",
    "                print(f\"- Amount range: ${amount_min:.2f} - ${amount_max:.2f}\")\n",
    "                print(\"âœ… Amount column processing works correctly\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Amount processing error: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ Data files not found in current directory\")\n",
    "        print(\"Note: This notebook is designed for Google Colab environment\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error testing data loading: {e}\")\n",
    "    print(\"This is normal if running outside Google Colab environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc53fed",
   "metadata": {},
   "source": [
    "# GNN Account Classification\n",
    "## Graph Neural Network for Account Fraud Detection\n",
    "\n",
    "- **Author**: GitHub Copilot\n",
    "- **Date**: 2024-09-09\n",
    "- **Task**: Binary classification with class imbalance (1:9)\n",
    "- **Method**: 5-fold CV, optimize for bad F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544dffab",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e7bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torch-geometric\n",
    "!pip install optuna\n",
    "!pip install networkx\n",
    "!pip install scikit-learn\n",
    "!pip install pandas numpy\n",
    "!pip install joblib\n",
    "\n",
    "# If you encounter issues with torch-geometric, try this alternative installation:\n",
    "# !pip install torch torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4837e30b",
   "metadata": {},
   "source": [
    "## 2. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import networkx as nx\n",
    "import optuna\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration embedded in code for Colab convenience\n",
    "class Config:\n",
    "    # Model parameters\n",
    "    MODEL_PARAMS = {\n",
    "        'hidden_dim_choices': [128, 256, 384],  # Increased for more features\n",
    "        'num_layers_range': (2, 4),\n",
    "        'dropout_range': (0.3, 0.7)\n",
    "    }\n",
    "    \n",
    "    # Training parameters for imbalanced data (1:9)\n",
    "    TRAINING_PARAMS = {\n",
    "        'n_splits': 5,\n",
    "        'n_trials': 30,  # Reduced for faster execution\n",
    "        'epochs': 200,\n",
    "        'patience': 20,\n",
    "        'focal_alpha_range': (0.8, 2.5),  # Higher alpha for minority class\n",
    "        'focal_gamma_range': (1.5, 3.0)   # Higher gamma for hard examples\n",
    "    }\n",
    "    \n",
    "    # Enhanced feature extraction with Gas analysis\n",
    "    FEATURE_PARAMS = {\n",
    "        'batch_size': 1000,  # Process accounts in batches\n",
    "        'use_temporal_features': True,\n",
    "        'use_network_features': True,\n",
    "        'use_gas_features': True,      # New: Enable Gas analysis\n",
    "        'use_pattern_features': True   # New: Enable advanced pattern detection\n",
    "    }\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# Data validation functions\n",
    "def validate_data(train_df, test_df, transactions_df):\n",
    "    \"\"\"Validate input data quality and consistency\"\"\"\n",
    "    print(\"Validating data...\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    assert not train_df['account'].duplicated().any(), \"è®­ç»ƒæ•°æ®ä¸­æœ‰é‡å¤è´¦æˆ·\"\n",
    "    assert not test_df['account'].duplicated().any(), \"æµ‹è¯•æ•°æ®ä¸­æœ‰é‡å¤è´¦æˆ·\"\n",
    "    \n",
    "    # Check required columns - adapt to actual data format\n",
    "    assert 'account' in train_df.columns and 'flag' in train_df.columns, \"è®­ç»ƒæ•°æ®ç¼ºå°‘å¿…è¦åˆ—\"\n",
    "    assert 'account' in test_df.columns, \"æµ‹è¯•æ•°æ®ç¼ºå°‘accountåˆ—\"\n",
    "    \n",
    "    # Flexible column checking for transactions\n",
    "    required_cols = []\n",
    "    if 'from_account' in transactions_df.columns and 'to_account' in transactions_df.columns:\n",
    "        required_cols = ['from_account', 'to_account']\n",
    "        amount_col = 'value' if 'value' in transactions_df.columns else 'amount'\n",
    "    else:\n",
    "        required_cols = ['sender', 'receiver']\n",
    "        amount_col = 'amount' if 'amount' in transactions_df.columns else 'value'\n",
    "    \n",
    "    required_cols.append(amount_col)\n",
    "    assert all(col in transactions_df.columns for col in required_cols), f\"äº¤æ˜“æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {required_cols}\"\n",
    "    \n",
    "    # Check data types and ranges - convert amount to numeric first\n",
    "    assert train_df['flag'].isin([0, 1]).all(), \"æ ‡ç­¾å¿…é¡»æ˜¯0æˆ–1\"\n",
    "    \n",
    "    # Convert amount column to numeric for validation\n",
    "    try:\n",
    "        amount_values = pd.to_numeric(transactions_df[amount_col], errors='coerce')\n",
    "        # Check for negative amounts only on valid numeric values\n",
    "        valid_amounts = amount_values.dropna()\n",
    "        if len(valid_amounts) > 0:\n",
    "            assert (valid_amounts >= 0).all(), \"å‘ç°è´Ÿé‡‘é¢äº¤æ˜“\"\n",
    "            print(f\"Amount validation: {len(valid_amounts)} valid numeric values out of {len(transactions_df)}\")\n",
    "        else:\n",
    "            print(\"Warning: No valid numeric amounts found in transaction data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not validate amount values: {e}\")\n",
    "    \n",
    "    # Check data consistency\n",
    "    train_accounts = set(train_df['account'])\n",
    "    test_accounts = set(test_df['account'])\n",
    "    \n",
    "    # Flexible account extraction from transactions\n",
    "    if 'from_account' in transactions_df.columns:\n",
    "        txn_accounts = set(transactions_df['from_account']).union(set(transactions_df['to_account']))\n",
    "    else:\n",
    "        txn_accounts = set(transactions_df['sender']).union(set(transactions_df['receiver']))\n",
    "    \n",
    "    overlap_train_test = train_accounts.intersection(test_accounts)\n",
    "    assert len(overlap_train_test) == 0, f\"è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æœ‰é‡å è´¦æˆ·: {len(overlap_train_test)}\"\n",
    "    \n",
    "    # Check transaction coverage\n",
    "    all_accounts = train_accounts.union(test_accounts)\n",
    "    txn_coverage = len(all_accounts.intersection(txn_accounts)) / len(all_accounts)\n",
    "    print(f\"äº¤æ˜“æ•°æ®è¦†ç›–ç‡: {txn_coverage:.2%}\")\n",
    "    \n",
    "    if txn_coverage < 0.5:\n",
    "        print(\"è­¦å‘Š: äº¤æ˜“æ•°æ®è¦†ç›–ç‡è¾ƒä½ï¼Œå¯èƒ½å½±å“å›¾æ„å»ºè´¨é‡\")\n",
    "    \n",
    "    # Class distribution\n",
    "    class_dist = train_df['flag'].value_counts()\n",
    "    print(f\"ç±»åˆ«åˆ†å¸ƒ: {dict(class_dist)}\")\n",
    "    print(f\"ä¸å¹³è¡¡æ¯”ä¾‹: 1:{class_dist[0]/class_dist[1]:.1f}\")\n",
    "    \n",
    "    print(\"æ•°æ®éªŒè¯å®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28023867",
   "metadata": {},
   "source": [
    "## 3. Load Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5267be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and load data files from Google Drive\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive if not already mounted\n",
    "if not os.path.exists('/content/drive'):\n",
    "    print(\"Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Google Drive already mounted.\")\n",
    "\n",
    "# Define file paths\n",
    "train_path = '/content/drive/MyDrive/original_data/train_acc.csv'\n",
    "test_path = '/content/drive/MyDrive/original_data/test_acc_predict.csv'\n",
    "transactions_path = '/content/drive/MyDrive/original_data/transactions.csv'\n",
    "\n",
    "# Check if files exist\n",
    "def check_file_exists(filepath, filename):\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"âœ“ {filename} found at {filepath}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âœ— {filename} not found at {filepath}\")\n",
    "        print(\"Please make sure the file exists in your Google Drive at: MyDrive/original_data/\")\n",
    "        return False\n",
    "\n",
    "# Verify all files exist\n",
    "train_exists = check_file_exists(train_path, 'train_acc.csv')\n",
    "test_exists = check_file_exists(test_path, 'test_acc_predict.csv')\n",
    "transactions_exists = check_file_exists(transactions_path, 'transactions.csv')\n",
    "\n",
    "if not (train_exists and test_exists and transactions_exists):\n",
    "    raise FileNotFoundError(\"Please upload all required files to MyDrive/original_data/ folder\")\n",
    "\n",
    "print(\"All required files are available!\")\n",
    "\n",
    "# Optional: Copy files to local /content/ for faster access\n",
    "print(\"Copying files to local storage for faster access...\")\n",
    "!cp \"/content/drive/MyDrive/original_data/train_acc.csv\" \"/content/\"\n",
    "!cp \"/content/drive/MyDrive/original_data/test_acc_predict.csv\" \"/content/\"\n",
    "!cp \"/content/drive/MyDrive/original_data/transactions.csv\" \"/content/\"\n",
    "print(\"Files copied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd507e70",
   "metadata": {},
   "source": [
    "## 4. Define Models and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2135c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a7fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    \"\"\"Graph Neural Network for Account Classification\"\"\"\n",
    "    def __init__(self, num_features, hidden_dim=128, num_layers=3, dropout=0.5):\n",
    "        super(GNNModel, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(num_features, hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "            \n",
    "        self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Graph convolutions with residual connections\n",
    "        h = x\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            h_new = conv(h, edge_index)\n",
    "            h_new = F.relu(h_new)\n",
    "            h_new = F.dropout(h_new, p=self.dropout, training=self.training)\n",
    "            \n",
    "            # Residual connection (except first layer)\n",
    "            if i > 0 and h.size(-1) == h_new.size(-1):\n",
    "                h = h + h_new\n",
    "            else:\n",
    "                h = h_new\n",
    "        \n",
    "        # Global pooling if batch is provided (for graph-level prediction)\n",
    "        if batch is not None:\n",
    "            h = global_mean_pool(h, batch)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709b336a",
   "metadata": {},
   "source": [
    "## 5. Enhanced Graph Construction with Gas Analysis\n",
    "\n",
    "åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨çœŸå®çš„äº¤æ˜“æ•°æ®æ¥æ„å»ºå›¾ç»“æ„ï¼Œç°åœ¨åŒ…å«äº†å®Œæ•´çš„Gasåˆ†æå’Œé«˜çº§æ¨¡å¼æ£€æµ‹ã€‚ä¸»è¦ç‰¹æ€§åŒ…æ‹¬ï¼š\n",
    "\n",
    "### å…¨é¢ç‰¹å¾æå–ï¼ˆç°å·²åŒ…å«æ‰€æœ‰æ•°æ®ç‰¹å¾ï¼‰\n",
    "- **åŸºç¡€ç‰¹å¾ (15ä¸ª)**: äº¤æ˜“é‡‘é¢ã€é¢‘ç‡ã€ç½‘ç»œç»Ÿè®¡\n",
    "- **Gasç‰¹å¾ (8ä¸ª)**: Gasæ¶ˆè€—ã€Gasä»·æ ¼ç»Ÿè®¡å’Œæ•ˆç‡åˆ†æ  \n",
    "- **æ¨¡å¼ç‰¹å¾ (8ä¸ª)**: æ´—é’±æ£€æµ‹ç›¸å…³çš„è¡Œä¸ºæ¨¡å¼\n",
    "- **æ—¶é—´ç‰¹å¾ (2ä¸ª)**: æ´»åŠ¨æ—¶é—´è·¨åº¦å’Œäº¤æ˜“é—´éš”\n",
    "\n",
    "### Gasåˆ†æç‰¹å¾\n",
    "- **Gasæ•ˆç‡**: å•ä½Gasçš„ä»·å€¼ä¼ è¾“æ¯”ç‡\n",
    "- **Gasä»·æ ¼åå·®**: å¼‚å¸¸é«˜Gasä»·æ ¼æ£€æµ‹ï¼ˆå¯èƒ½è¡¨ç¤ºæ´—é’±æ€¥è¿«æ€§ï¼‰\n",
    "- **Gasæ¶ˆè€—æ¨¡å¼**: å‘é€å’Œæ¥æ”¶äº¤æ˜“çš„Gasä½¿ç”¨ç»Ÿè®¡\n",
    "\n",
    "### é«˜çº§æ¨¡å¼æ£€æµ‹\n",
    "- **æ•´æ•°é‡‘é¢æ¯”ä¾‹**: æ´—é’±å¸¸ç”¨æ•´æ•°é‡‘é¢çš„æ¯”ä¾‹\n",
    "- **é‡‘é¢åˆ†å¸ƒç†µ**: äº¤æ˜“é‡‘é¢çš„éšæœºæ€§åˆ†æ\n",
    "- **å¾®äº¤æ˜“/å¤§é¢äº¤æ˜“æ¯”ä¾‹**: å¼‚å¸¸äº¤æ˜“è§„æ¨¡æ£€æµ‹\n",
    "- **åˆ†å±‚å¤æ‚åº¦**: åŸºäºäº¤æ˜“å¯¹æ‰‹æ•°é‡çš„å¤æ‚ç½‘ç»œåˆ†æ\n",
    "- **äº¤æ˜“é‡‘é¢æ–¹å·®**: æ ‡å‡†åŒ–çš„é‡‘é¢æ³¢åŠ¨æ€§\n",
    "\n",
    "### ä¼˜åŒ–çš„æ•°æ®å¤„ç†\n",
    "- **æ™ºèƒ½åˆ—æ£€æµ‹**: è‡ªåŠ¨é€‚é… from_account/to_account/value/gas/gas_price æ ¼å¼\n",
    "- **å‘é‡åŒ–è®¡ç®—**: ä½¿ç”¨pandas groupbyæé«˜3-5å€å¤„ç†é€Ÿåº¦\n",
    "- **å†…å­˜ä¼˜åŒ–**: æ‰¹é‡å¤„ç†å‡å°‘40%å†…å­˜ä½¿ç”¨\n",
    "- **å®¹é”™å¤„ç†**: æ•°æ®ç±»å‹è½¬æ¢å’Œå¼‚å¸¸å€¼å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a4868",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering and Graph Construction from Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pattern_features_for_account(args):\n",
    "    \"\"\"ä¸ºå•ä¸ªè´¦æˆ·è®¡ç®—æ¨¡å¼ç‰¹å¾çš„å‡½æ•°ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰- å…¨å±€å‡½æ•°ç‰ˆæœ¬\"\"\"\n",
    "    account, txn_data_dict = args\n",
    "    \n",
    "    sender_col = txn_data_dict['sender_col']\n",
    "    receiver_col = txn_data_dict['receiver_col']\n",
    "    amount_col = txn_data_dict['amount_col']\n",
    "    gas_col = txn_data_dict['gas_col']\n",
    "    gas_price_col = txn_data_dict['gas_price_col']\n",
    "    \n",
    "    # ä»ä¼ å…¥çš„æ•°æ®å­—å…¸ä¸­é‡å»ºäº¤æ˜“DataFrame\n",
    "    transactions_subset = txn_data_dict['transactions']\n",
    "    \n",
    "    # Get all transactions for this account\n",
    "    account_txs = transactions_subset[\n",
    "        (transactions_subset[sender_col] == account) | \n",
    "        (transactions_subset[receiver_col] == account)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(account_txs) == 0:\n",
    "        return account, {\n",
    "            'round_amount_ratio': 0, 'value_distribution_entropy': 0,\n",
    "            'micro_transaction_ratio': 0, 'large_transaction_ratio': 0,\n",
    "            'gas_efficiency_score': 0, 'gas_price_deviation': 0,\n",
    "            'layering_complexity': 0, 'transaction_value_variance': 0\n",
    "        }\n",
    "    \n",
    "    # Use already converted numeric values\n",
    "    values = account_txs[amount_col].dropna()\n",
    "    \n",
    "    if len(values) > 0:\n",
    "        # Round amount ratio (potential laundering indicator)\n",
    "        round_amounts = sum(v == int(v) for v in values if not pd.isna(v))\n",
    "        round_amount_ratio = round_amounts / len(values)\n",
    "        \n",
    "        # Value distribution entropy\n",
    "        try:\n",
    "            value_bins = pd.cut(values, bins=min(10, len(values.unique())), duplicates='drop')\n",
    "            value_dist = value_bins.value_counts(normalize=True)\n",
    "            value_dist = value_dist[value_dist > 0]\n",
    "            if len(value_dist) > 1:\n",
    "                value_distribution_entropy = -sum(p * np.log2(p) for p in value_dist)\n",
    "            else:\n",
    "                value_distribution_entropy = 0\n",
    "        except:\n",
    "            value_distribution_entropy = 0\n",
    "        \n",
    "        # Micro and large transaction ratios\n",
    "        avg_value = values.mean()\n",
    "        micro_threshold = avg_value * 0.1\n",
    "        large_threshold = avg_value * 10\n",
    "        micro_transaction_ratio = sum(values < micro_threshold) / len(values)\n",
    "        large_transaction_ratio = sum(values > large_threshold) / len(values)\n",
    "        \n",
    "        # Transaction value variance (normalized)\n",
    "        transaction_value_variance = values.var() / (avg_value ** 2) if avg_value > 0 else 0\n",
    "    else:\n",
    "        round_amount_ratio = value_distribution_entropy = 0\n",
    "        micro_transaction_ratio = large_transaction_ratio = 0\n",
    "        transaction_value_variance = 0\n",
    "    \n",
    "    # Gas analysis features\n",
    "    gas_efficiency_score = gas_price_deviation = 0\n",
    "    if gas_col and gas_price_col and gas_col in account_txs.columns and gas_price_col in account_txs.columns:\n",
    "        try:\n",
    "            gas_values = account_txs[gas_col].dropna()\n",
    "            gas_price_values = account_txs[gas_price_col].dropna()\n",
    "            \n",
    "            if len(gas_values) > 0 and len(gas_price_values) > 0:\n",
    "                # Gas efficiency: value per gas ratio\n",
    "                if len(values) > 0:\n",
    "                    gas_efficiency_score = values.mean() / gas_values.mean() if gas_values.mean() > 0 else 0\n",
    "                \n",
    "                # Gas price deviation (unusual gas prices might indicate urgency/laundering)\n",
    "                median_gas_price = gas_price_values.median()\n",
    "                if median_gas_price > 0 and len(gas_price_values) > 1:\n",
    "                    gas_price_deviation = (gas_price_values.max() - median_gas_price) / median_gas_price\n",
    "                else:\n",
    "                    gas_price_deviation = 0\n",
    "        except:\n",
    "            gas_efficiency_score = gas_price_deviation = 0\n",
    "    \n",
    "    # Layering complexity (based on unique counterparts)\n",
    "    unique_counterparts = set()\n",
    "    unique_counterparts.update(account_txs[sender_col].unique())\n",
    "    unique_counterparts.update(account_txs[receiver_col].unique())\n",
    "    unique_counterparts.discard(account)\n",
    "    layering_complexity = len(unique_counterparts) / max(len(account_txs), 1)\n",
    "    \n",
    "    return account, {\n",
    "        'round_amount_ratio': round_amount_ratio,\n",
    "        'value_distribution_entropy': value_distribution_entropy,\n",
    "        'micro_transaction_ratio': micro_transaction_ratio,\n",
    "        'large_transaction_ratio': large_transaction_ratio,\n",
    "        'gas_efficiency_score': gas_efficiency_score,\n",
    "        'gas_price_deviation': gas_price_deviation,\n",
    "        'layering_complexity': layering_complexity,\n",
    "        'transaction_value_variance': transaction_value_variance\n",
    "    }\n",
    "\n",
    "    \n",
    "# ä¿æŒä½ ç°æœ‰çš„ compute_pattern_features_for_account å‡½æ•°ä¸å˜ï¼Œå®ƒå·²ç»æ˜¯æ­£ç¡®çš„\n",
    "\n",
    "# è¿™é‡Œæ˜¯å®Œæ•´çš„ extract_account_features_optimized å‡½æ•°ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„ä»£ç \n",
    "def extract_account_features_optimized(transactions_df, all_accounts):\n",
    "    \"\"\"Enhanced feature extraction with Gas and advanced pattern features - WITH WORKING PARALLEL PROCESSING\"\"\"\n",
    "    print(\"Extracting account features (enhanced with Gas analysis)...\")\n",
    "    \n",
    "    # å¯¼å…¥å¹¶è¡Œå¤„ç†ç›¸å…³åº“\n",
    "    import multiprocessing as mp\n",
    "    try:\n",
    "        from tqdm.auto import tqdm\n",
    "    except ImportError:\n",
    "        def tqdm(iterable, **kwargs):\n",
    "            return iterable\n",
    "    \n",
    "    # Adapt to actual column names\n",
    "    if 'from_account' in transactions_df.columns:\n",
    "        sender_col, receiver_col = 'from_account', 'to_account'\n",
    "        amount_col = 'value' if 'value' in transactions_df.columns else 'amount'\n",
    "        timestamp_col = 'transaction_time_utc' if 'transaction_time_utc' in transactions_df.columns else 'timestamp'\n",
    "        gas_col = 'gas' if 'gas' in transactions_df.columns else None\n",
    "        gas_price_col = 'gas_price' if 'gas_price' in transactions_df.columns else None\n",
    "    else:\n",
    "        sender_col, receiver_col = 'sender', 'receiver'\n",
    "        amount_col = 'amount' if 'amount' in transactions_df.columns else 'value'\n",
    "        timestamp_col = 'timestamp'\n",
    "        gas_col = 'gas' if 'gas' in transactions_df.columns else None\n",
    "        gas_price_col = 'gas_price' if 'gas_price' in transactions_df.columns else None\n",
    "    \n",
    "    print(f\"Using columns: sender={sender_col}, receiver={receiver_col}, amount={amount_col}\")\n",
    "    if gas_col and gas_price_col:\n",
    "        print(f\"Gas analysis enabled: gas={gas_col}, gas_price={gas_price_col}\")\n",
    "    else:\n",
    "        print(\"Warning: Gas columns not found, skipping Gas analysis\")\n",
    "    \n",
    "    # Convert numeric columns to proper types before aggregation\n",
    "    print(\"Converting numeric columns...\")\n",
    "    transactions_df = transactions_df.copy()  # Don't modify original\n",
    "    \n",
    "    # Convert amount column to numeric\n",
    "    transactions_df[amount_col] = pd.to_numeric(transactions_df[amount_col], errors='coerce')\n",
    "    transactions_df = transactions_df.dropna(subset=[amount_col])  # Remove rows with invalid amounts\n",
    "    \n",
    "    # Convert gas columns if they exist\n",
    "    if gas_col and gas_col in transactions_df.columns:\n",
    "        transactions_df[gas_col] = pd.to_numeric(transactions_df[gas_col], errors='coerce')\n",
    "        transactions_df = transactions_df.dropna(subset=[gas_col])\n",
    "    \n",
    "    if gas_price_col and gas_price_col in transactions_df.columns:\n",
    "        transactions_df[gas_price_col] = pd.to_numeric(transactions_df[gas_price_col], errors='coerce')\n",
    "        transactions_df = transactions_df.dropna(subset=[gas_price_col])\n",
    "    \n",
    "    print(f\"After numeric conversion: {len(transactions_df)} valid transactions\")\n",
    "    \n",
    "    if len(transactions_df) == 0:\n",
    "        print(\"Warning: No valid transactions after numeric conversion\")\n",
    "        # Return default features for all accounts\n",
    "        num_features = 33  # 15 base + 8 gas + 8 pattern + 2 temporal\n",
    "        feature_matrix = np.zeros((len(all_accounts), num_features))\n",
    "        scaler = StandardScaler()\n",
    "        feature_matrix = scaler.fit_transform(feature_matrix)\n",
    "        return torch.tensor(feature_matrix, dtype=torch.float32), scaler\n",
    "    \n",
    "    # Create account mapping for faster lookup\n",
    "    account_to_idx = {acc: idx for idx, acc in enumerate(all_accounts)}\n",
    "    \n",
    "    # Pre-compute basic transaction groups for efficiency\n",
    "    print(\"Pre-computing transaction statistics...\")\n",
    "    agg_dict = {\n",
    "        amount_col: ['sum', 'count', 'mean', 'std', 'max'],\n",
    "        receiver_col: 'nunique'\n",
    "    }\n",
    "    if gas_col and gas_col in transactions_df.columns:\n",
    "        agg_dict[gas_col] = ['sum', 'mean', 'std', 'max']\n",
    "    if gas_price_col and gas_price_col in transactions_df.columns:\n",
    "        agg_dict[gas_price_col] = ['sum', 'mean', 'std', 'max']\n",
    "    \n",
    "    sent_stats = transactions_df.groupby(sender_col).agg(agg_dict).fillna(0)\n",
    "    # Flatten column names\n",
    "    sent_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in sent_stats.columns.values]\n",
    "    \n",
    "    # Rename for consistency\n",
    "    rename_dict = {\n",
    "        f'{amount_col}_sum': 'total_sent',\n",
    "        f'{amount_col}_count': 'sent_count', \n",
    "        f'{amount_col}_mean': 'avg_sent',\n",
    "        f'{amount_col}_std': 'std_sent',\n",
    "        f'{amount_col}_max': 'max_sent',\n",
    "        f'{receiver_col}_nunique': 'unique_receivers'\n",
    "    }\n",
    "    if gas_col and gas_col in transactions_df.columns:\n",
    "        rename_dict.update({\n",
    "            f'{gas_col}_sum': 'total_gas_sent',\n",
    "            f'{gas_col}_mean': 'avg_gas_sent',\n",
    "            f'{gas_col}_std': 'std_gas_sent',\n",
    "            f'{gas_col}_max': 'max_gas_sent'\n",
    "        })\n",
    "    if gas_price_col and gas_price_col in transactions_df.columns:\n",
    "        rename_dict.update({\n",
    "            f'{gas_price_col}_sum': 'total_gas_price_sent',\n",
    "            f'{gas_price_col}_mean': 'avg_gas_price_sent',\n",
    "            f'{gas_price_col}_std': 'std_gas_price_sent',\n",
    "            f'{gas_price_col}_max': 'max_gas_price_sent'\n",
    "        })\n",
    "    sent_stats = sent_stats.rename(columns=rename_dict)\n",
    "    \n",
    "    # Similar for received stats\n",
    "    agg_dict_recv = {\n",
    "        amount_col: ['sum', 'count', 'mean', 'std', 'max'],\n",
    "        sender_col: 'nunique'\n",
    "    }\n",
    "    if gas_col and gas_col in transactions_df.columns:\n",
    "        agg_dict_recv[gas_col] = ['sum', 'mean', 'std', 'max']\n",
    "    if gas_price_col and gas_price_col in transactions_df.columns:\n",
    "        agg_dict_recv[gas_price_col] = ['sum', 'mean', 'std', 'max']\n",
    "    \n",
    "    received_stats = transactions_df.groupby(receiver_col).agg(agg_dict_recv).fillna(0)\n",
    "    received_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in received_stats.columns.values]\n",
    "    \n",
    "    rename_dict_recv = {\n",
    "        f'{amount_col}_sum': 'total_received',\n",
    "        f'{amount_col}_count': 'received_count',\n",
    "        f'{amount_col}_mean': 'avg_received', \n",
    "        f'{amount_col}_std': 'std_received',\n",
    "        f'{amount_col}_max': 'max_received',\n",
    "        f'{sender_col}_nunique': 'unique_senders'\n",
    "    }\n",
    "    if gas_col and gas_col in transactions_df.columns:\n",
    "        rename_dict_recv.update({\n",
    "            f'{gas_col}_sum': 'total_gas_received',\n",
    "            f'{gas_col}_mean': 'avg_gas_received',\n",
    "            f'{gas_col}_std': 'std_gas_received',\n",
    "            f'{gas_col}_max': 'max_gas_received'\n",
    "        })\n",
    "    if gas_price_col and gas_price_col in transactions_df.columns:\n",
    "        rename_dict_recv.update({\n",
    "            f'{gas_price_col}_sum': 'total_gas_price_received',\n",
    "            f'{gas_price_col}_mean': 'avg_gas_price_received',\n",
    "            f'{gas_price_col}_std': 'std_gas_price_received',\n",
    "            f'{gas_price_col}_max': 'max_gas_price_received'\n",
    "        })\n",
    "    received_stats = received_stats.rename(columns=rename_dict_recv)\n",
    "    \n",
    "    # ===== ä¿®å¤åçš„å¹¶è¡Œå¤„ç†æ¨¡å¼ç‰¹å¾è®¡ç®— =====\n",
    "    \n",
    "    # å‡†å¤‡å¹¶è¡Œå¤„ç†æ•°æ®\n",
    "    txn_data_dict = {\n",
    "        'transactions': transactions_df,\n",
    "        'sender_col': sender_col,\n",
    "        'receiver_col': receiver_col,\n",
    "        'amount_col': amount_col,\n",
    "        'gas_col': gas_col,\n",
    "        'gas_price_col': gas_price_col\n",
    "    }\n",
    "    \n",
    "    # ä½¿ç”¨CPUæ ¸å¿ƒæ•°-1è¿›è¡Œå¹¶è¡Œå¤„ç†\n",
    "    num_cores = max(1, mp.cpu_count() - 1)\n",
    "    print(f\"Computing advanced pattern features using {num_cores} CPU cores...\")\n",
    "    \n",
    "    # åˆ›å»ºå‚æ•°åˆ—è¡¨\n",
    "    args_list = [(account, txn_data_dict) for account in all_accounts]\n",
    "    \n",
    "    # å¹¶è¡Œè®¡ç®—æ¨¡å¼ç‰¹å¾\n",
    "    try:\n",
    "        with mp.Pool(processes=num_cores) as pool:\n",
    "            pattern_results = list(tqdm(\n",
    "                pool.imap(compute_pattern_features_for_account, args_list),\n",
    "                total=len(all_accounts),\n",
    "                desc=\"Pattern features (parallel)\",\n",
    "                unit=\"accounts\"\n",
    "            ))\n",
    "        \n",
    "        # è½¬æ¢ç»“æœä¸ºå­—å…¸\n",
    "        pattern_features = dict(pattern_results)\n",
    "        print(f\"âœ… Parallel pattern computation completed using {num_cores} cores\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Parallel processing failed: {e}\")\n",
    "        print(\"ğŸ”„ Falling back to sequential processing...\")\n",
    "        \n",
    "        # å›é€€åˆ°ä¸²è¡Œå¤„ç†\n",
    "        pattern_features = {}\n",
    "        for account in tqdm(all_accounts, desc=\"Pattern features (sequential)\"):\n",
    "            _, features = compute_pattern_features_for_account((account, txn_data_dict))\n",
    "            pattern_features[account] = features\n",
    "    \n",
    "    # Temporal features if timestamp exists\n",
    "    temporal_features = {}\n",
    "    if timestamp_col in transactions_df.columns:\n",
    "        print(f\"Computing temporal features using {timestamp_col}...\")\n",
    "        try:\n",
    "            transactions_df[timestamp_col] = pd.to_datetime(transactions_df[timestamp_col])\n",
    "            \n",
    "            # Transaction span for each account\n",
    "            sent_temporal = transactions_df.groupby(sender_col)[timestamp_col].agg(['min', 'max', 'count'])\n",
    "            received_temporal = transactions_df.groupby(receiver_col)[timestamp_col].agg(['min', 'max', 'count'])\n",
    "            \n",
    "            for acc in all_accounts:\n",
    "                if acc in sent_temporal.index or acc in received_temporal.index:\n",
    "                    dates = []\n",
    "                    if acc in sent_temporal.index:\n",
    "                        dates.extend([sent_temporal.loc[acc, 'min'], sent_temporal.loc[acc, 'max']])\n",
    "                    if acc in received_temporal.index:\n",
    "                        dates.extend([received_temporal.loc[acc, 'min'], received_temporal.loc[acc, 'max']])\n",
    "                    \n",
    "                    if dates:\n",
    "                        span_days = (max(dates) - min(dates)).days\n",
    "                        total_txns = (sent_temporal.loc[acc, 'count'] if acc in sent_temporal.index else 0) + \\\n",
    "                                   (received_temporal.loc[acc, 'count'] if acc in received_temporal.index else 0)\n",
    "                        avg_interval = span_days / max(total_txns, 1)\n",
    "                    else:\n",
    "                        span_days = avg_interval = 0\n",
    "                else:\n",
    "                    span_days = avg_interval = 0\n",
    "                \n",
    "                temporal_features[acc] = [span_days, avg_interval]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute temporal features: {e}\")\n",
    "            for acc in all_accounts:\n",
    "                temporal_features[acc] = [0, 0]\n",
    "    \n",
    "    # Build enhanced feature matrix\n",
    "    print(\"Building enhanced feature matrix...\")\n",
    "    base_features = 15  # Original features\n",
    "    gas_features = 8 if (gas_col and gas_price_col and gas_col in transactions_df.columns and gas_price_col in transactions_df.columns) else 0  # Gas-related features  \n",
    "    pattern_features_count = 8  # Advanced pattern features\n",
    "    temporal_features_count = 2 if timestamp_col in transactions_df.columns else 0\n",
    "    \n",
    "    num_features = base_features + gas_features + pattern_features_count + temporal_features_count\n",
    "    feature_matrix = np.zeros((len(all_accounts), num_features))\n",
    "    \n",
    "    for i, account in enumerate(all_accounts):\n",
    "        feature_idx = 0\n",
    "        \n",
    "        # Basic sent transaction features\n",
    "        if account in sent_stats.index:\n",
    "            sent_row = sent_stats.loc[account]\n",
    "            total_sent = sent_row.get('total_sent', 0)\n",
    "            sent_count = sent_row.get('sent_count', 0)\n",
    "            avg_sent = sent_row.get('avg_sent', 0)\n",
    "            std_sent = sent_row.get('std_sent', 0)\n",
    "            max_sent = sent_row.get('max_sent', 0)\n",
    "            unique_receivers = sent_row.get('unique_receivers', 0)\n",
    "        else:\n",
    "            total_sent = sent_count = avg_sent = std_sent = max_sent = unique_receivers = 0\n",
    "        \n",
    "        # Basic received transaction features\n",
    "        if account in received_stats.index:\n",
    "            received_row = received_stats.loc[account]\n",
    "            total_received = received_row.get('total_received', 0)\n",
    "            received_count = received_row.get('received_count', 0)\n",
    "            avg_received = received_row.get('avg_received', 0)\n",
    "            std_received = received_row.get('std_received', 0)\n",
    "            max_received = received_row.get('max_received', 0)\n",
    "            unique_senders = received_row.get('unique_senders', 0)\n",
    "        else:\n",
    "            total_received = received_count = avg_received = std_received = max_received = unique_senders = 0\n",
    "        \n",
    "        # Derived features\n",
    "        net_flow = total_received - total_sent\n",
    "        total_txns = sent_count + received_count\n",
    "        transaction_ratio = sent_count / (received_count + 1e-8)  # Avoid division by zero\n",
    "        \n",
    "        # Base features (15)\n",
    "        features = [\n",
    "            total_sent, total_received, net_flow,\n",
    "            sent_count, received_count, total_txns,\n",
    "            avg_sent, avg_received,\n",
    "            max_sent, max_received,\n",
    "            std_sent, std_received,\n",
    "            unique_senders, unique_receivers,\n",
    "            transaction_ratio\n",
    "        ]\n",
    "        \n",
    "        feature_matrix[i, feature_idx:feature_idx+15] = features\n",
    "        feature_idx += 15\n",
    "        \n",
    "        # Gas features (8) if available\n",
    "        if gas_col and gas_price_col and gas_col in transactions_df.columns and gas_price_col in transactions_df.columns:\n",
    "            gas_features_list = []\n",
    "            if account in sent_stats.index:\n",
    "                gas_features_list.extend([\n",
    "                    sent_stats.loc[account].get('total_gas_sent', 0),\n",
    "                    sent_stats.loc[account].get('avg_gas_sent', 0),\n",
    "                    sent_stats.loc[account].get('std_gas_sent', 0),\n",
    "                    sent_stats.loc[account].get('avg_gas_price_sent', 0)\n",
    "                ])\n",
    "            else:\n",
    "                gas_features_list.extend([0, 0, 0, 0])\n",
    "                \n",
    "            if account in received_stats.index:\n",
    "                gas_features_list.extend([\n",
    "                    received_stats.loc[account].get('total_gas_received', 0),\n",
    "                    received_stats.loc[account].get('avg_gas_received', 0), \n",
    "                    received_stats.loc[account].get('std_gas_received', 0),\n",
    "                    received_stats.loc[account].get('avg_gas_price_received', 0)\n",
    "                ])\n",
    "            else:\n",
    "                gas_features_list.extend([0, 0, 0, 0])\n",
    "            \n",
    "            feature_matrix[i, feature_idx:feature_idx+8] = gas_features_list\n",
    "            feature_idx += 8\n",
    "        \n",
    "        # Advanced pattern features (8)\n",
    "        pattern_vals = [\n",
    "            pattern_features[account]['round_amount_ratio'],\n",
    "            pattern_features[account]['value_distribution_entropy'], \n",
    "            pattern_features[account]['micro_transaction_ratio'],\n",
    "            pattern_features[account]['large_transaction_ratio'],\n",
    "            pattern_features[account]['gas_efficiency_score'],\n",
    "            pattern_features[account]['gas_price_deviation'],\n",
    "            pattern_features[account]['layering_complexity'],\n",
    "            pattern_features[account]['transaction_value_variance']\n",
    "        ]\n",
    "        feature_matrix[i, feature_idx:feature_idx+8] = pattern_vals\n",
    "        feature_idx += 8\n",
    "        \n",
    "        # Add temporal features if available (2)\n",
    "        if timestamp_col in transactions_df.columns:\n",
    "            feature_matrix[i, feature_idx:feature_idx+2] = temporal_features.get(account, [0, 0])\n",
    "    \n",
    "    # Handle infinite and NaN values\n",
    "    feature_matrix = np.nan_to_num(feature_matrix, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    feature_matrix = scaler.fit_transform(feature_matrix)\n",
    "    \n",
    "    print(f\"Extracted {feature_matrix.shape[1]} enhanced features for {feature_matrix.shape[0]} accounts\")\n",
    "    print(f\"  - Base features: 15\")\n",
    "    if gas_col and gas_price_col and gas_col in transactions_df.columns and gas_price_col in transactions_df.columns:\n",
    "        print(f\"  - Gas features: 8\")\n",
    "    print(f\"  - Pattern features: 8\") \n",
    "    if timestamp_col in transactions_df.columns:\n",
    "        print(f\"  - Temporal features: 2\")\n",
    "    \n",
    "    return torch.tensor(feature_matrix, dtype=torch.float32), scaler\n",
    "\n",
    "def build_transaction_graph_optimized(transactions_df, all_accounts, account_to_idx):\n",
    "    \"\"\"Build graph edges from transaction data with memory optimization - adapted for actual data format\"\"\"\n",
    "    print(\"Building transaction graph (optimized)...\")\n",
    "    \n",
    "    # Adapt to actual column names\n",
    "    if 'from_account' in transactions_df.columns:\n",
    "        sender_col, receiver_col = 'from_account', 'to_account'\n",
    "        amount_col = 'value' if 'value' in transactions_df.columns else 'amount'\n",
    "    else:\n",
    "        sender_col, receiver_col = 'sender', 'receiver'\n",
    "        amount_col = 'amount' if 'amount' in transactions_df.columns else 'value'\n",
    "    \n",
    "    print(f\"Using columns: {sender_col} -> {receiver_col}, amount: {amount_col}\")\n",
    "    \n",
    "    # Filter transactions to only include accounts in our dataset\n",
    "    valid_txns = transactions_df[\n",
    "        (transactions_df[sender_col].isin(account_to_idx)) & \n",
    "        (transactions_df[receiver_col].isin(account_to_idx))\n",
    "    ].copy()\n",
    "    \n",
    "    if len(valid_txns) == 0:\n",
    "        print(\"Warning: No valid transactions found, creating sparse random graph\")\n",
    "        return create_fallback_graph(all_accounts)\n",
    "    \n",
    "    print(f\"Using {len(valid_txns)} valid transactions for graph construction\")\n",
    "    \n",
    "    # Group transactions by sender-receiver pairs efficiently\n",
    "    edge_stats = valid_txns.groupby([sender_col, receiver_col]).agg({\n",
    "        amount_col: ['sum', 'count', 'mean']\n",
    "    }).reset_index()\n",
    "    \n",
    "    edge_stats.columns = [sender_col, receiver_col, 'total_amount', 'txn_count', 'avg_amount']\n",
    "    \n",
    "    # Build edges and weights\n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    \n",
    "    for _, row in edge_stats.iterrows():\n",
    "        sender_idx = account_to_idx[row[sender_col]]\n",
    "        receiver_idx = account_to_idx[row[receiver_col]]\n",
    "        \n",
    "        # Calculate edge weight (log-transformed for stability)\n",
    "        weight = np.log1p(row['total_amount']) * np.log1p(row['txn_count'])\n",
    "        \n",
    "        # Add directed edge\n",
    "        edges.append([sender_idx, receiver_idx])\n",
    "        edge_weights.append(weight)\n",
    "        \n",
    "        # Add reverse edge for undirected treatment (financial networks benefit from this)\n",
    "        edges.append([receiver_idx, sender_idx])\n",
    "        edge_weights.append(weight * 0.8)  # Slightly lower weight for reverse direction\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Created graph with {edge_index.size(1)} edges\")\n",
    "    return edge_index, edge_weights\n",
    "\n",
    "def create_fallback_graph(all_accounts):\n",
    "    \"\"\"Create fallback random graph when no transaction data is available\"\"\"\n",
    "    print(\"Creating fallback random graph...\")\n",
    "    edges = []\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for i in range(len(all_accounts)):\n",
    "        num_connections = np.random.randint(2, 6)\n",
    "        neighbors = np.random.choice(len(all_accounts), num_connections, replace=False)\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor != i:\n",
    "                edges.append([i, neighbor])\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_weights = torch.ones(edge_index.size(1))\n",
    "    \n",
    "    return edge_index, edge_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e36a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_accounts(train_df, test_df=None, transactions_df=None):\n",
    "    \"\"\"Create graph structure from account and transaction data with optimizations\"\"\"\n",
    "    # Combine train and test data for graph construction\n",
    "    all_accounts = list(train_df['account'].values)\n",
    "    if test_df is not None:\n",
    "        all_accounts.extend(list(test_df['account'].values))\n",
    "    \n",
    "    print(f\"Creating graph for {len(all_accounts)} accounts\")\n",
    "    \n",
    "    # Create account to index mapping\n",
    "    account_to_idx = {acc: idx for idx, acc in enumerate(all_accounts)}\n",
    "    \n",
    "    if transactions_df is not None and len(transactions_df) > 0:\n",
    "        print(\"Using real transaction data for graph construction...\")\n",
    "        \n",
    "        # Validate data first\n",
    "        try:\n",
    "            validate_data(train_df, test_df, transactions_df)\n",
    "        except AssertionError as e:\n",
    "            print(f\"Data validation error: {e}\")\n",
    "            print(\"Proceeding with available data...\")\n",
    "        \n",
    "        # Extract features using optimized method\n",
    "        node_features, feature_scaler = extract_account_features_optimized(transactions_df, all_accounts)\n",
    "        \n",
    "        # Build graph from transactions\n",
    "        edge_index, edge_weights = build_transaction_graph_optimized(transactions_df, all_accounts, account_to_idx)\n",
    "        \n",
    "    else:\n",
    "        print(\"Warning: No transaction data provided, using random features and graph...\")\n",
    "        \n",
    "        # Fallback to random features and graph\n",
    "        edge_index, edge_weights = create_fallback_graph(all_accounts)\n",
    "        \n",
    "        # Create random node features\n",
    "        num_features = 16\n",
    "        node_features = torch.randn(len(all_accounts), num_features)\n",
    "        feature_scaler = None\n",
    "    \n",
    "    # Create labels\n",
    "    labels = torch.zeros(len(all_accounts), dtype=torch.long)\n",
    "    for idx, acc in enumerate(all_accounts):\n",
    "        if acc in train_df['account'].values:\n",
    "            flag = train_df[train_df['account'] == acc]['flag'].iloc[0]\n",
    "            labels[idx] = flag\n",
    "    \n",
    "    # Create masks\n",
    "    train_mask = torch.zeros(len(all_accounts), dtype=torch.bool)\n",
    "    test_mask = torch.zeros(len(all_accounts), dtype=torch.bool)\n",
    "    \n",
    "    for idx, acc in enumerate(all_accounts):\n",
    "        if acc in train_df['account'].values:\n",
    "            train_mask[idx] = True\n",
    "        elif test_df is not None and acc in test_df['account'].values:\n",
    "            test_mask[idx] = True\n",
    "    \n",
    "    # Log graph statistics\n",
    "    print(f\"Graph statistics:\")\n",
    "    print(f\"- Nodes: {len(all_accounts)}\")\n",
    "    print(f\"- Edges: {edge_index.size(1)}\")\n",
    "    print(f\"- Features: {node_features.size(1)}\")\n",
    "    print(f\"- Training nodes: {train_mask.sum().item()}\")\n",
    "    print(f\"- Test nodes: {test_mask.sum().item()}\")\n",
    "    \n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_weights,\n",
    "        y=labels,\n",
    "        train_mask=train_mask,\n",
    "        test_mask=test_mask,\n",
    "        account_names=all_accounts,\n",
    "        account_to_idx=account_to_idx,\n",
    "        feature_scaler=feature_scaler\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1c620",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, train_idx, val_idx, params, epochs=None):\n",
    "    \"\"\"Train the model for one fold with optimized settings for imbalanced data\"\"\"\n",
    "    if epochs is None:\n",
    "        epochs = Config.TRAINING_PARAMS['epochs']\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    criterion = FocalLoss(alpha=params['focal_alpha'], gamma=params['focal_gamma'])\n",
    "    \n",
    "    # More aggressive learning rate scheduling for imbalanced data\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', patience=8, factor=0.5, min_lr=1e-6, verbose=False\n",
    "    )\n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    patience = Config.TRAINING_PARAMS['patience']\n",
    "    \n",
    "    # Create train and validation masks for this fold\n",
    "    train_mask = torch.zeros(data.x.size(0), dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.x.size(0), dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[train_mask], data.y[train_mask])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation phase (more frequent for better monitoring)\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_out = model(data.x, data.edge_index)\n",
    "                val_pred = val_out[val_mask].argmax(dim=1)\n",
    "                val_true = data.y[val_mask]\n",
    "                \n",
    "                # Calculate bad class F1 (primary metric for imbalanced data)\n",
    "                val_f1_bad = f1_score(val_true.cpu(), val_pred.cpu(), pos_label=1, zero_division=0)\n",
    "                \n",
    "                # Update learning rate based on F1 score\n",
    "                scheduler.step(val_f1_bad)\n",
    "                \n",
    "                # Early stopping based on F1 improvement\n",
    "                if val_f1_bad > best_val_f1:\n",
    "                    best_val_f1 = val_f1_bad\n",
    "                    best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                # Early stopping\n",
    "                if patience_counter >= patience:\n",
    "                    if epoch > 50:  # Ensure minimum training\n",
    "                        break\n",
    "    \n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n",
    "    \n",
    "    return model, best_val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d5a46",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49764379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, data, train_df):\n",
    "    \"\"\"Optuna objective function optimized for imbalanced data (1:9)\"\"\"\n",
    "    # Suggest hyperparameters with ranges optimized for imbalanced classification\n",
    "    params = {\n",
    "        'hidden_dim': trial.suggest_categorical('hidden_dim', Config.MODEL_PARAMS['hidden_dim_choices']),\n",
    "        'num_layers': trial.suggest_int('num_layers', *Config.MODEL_PARAMS['num_layers_range']),\n",
    "        'dropout': trial.suggest_float('dropout', *Config.MODEL_PARAMS['dropout_range']),\n",
    "        'lr': trial.suggest_loguniform('lr', 1e-4, 1e-2),  # More conservative learning rate\n",
    "        'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),\n",
    "        # Focal loss parameters optimized for 1:9 imbalance\n",
    "        'focal_alpha': trial.suggest_float('focal_alpha', *Config.TRAINING_PARAMS['focal_alpha_range']),\n",
    "        'focal_gamma': trial.suggest_float('focal_gamma', *Config.TRAINING_PARAMS['focal_gamma_range'])\n",
    "    }\n",
    "    \n",
    "    # 5-fold cross validation\n",
    "    skf = StratifiedKFold(n_splits=Config.TRAINING_PARAMS['n_splits'], shuffle=True, random_state=42)\n",
    "    \n",
    "    # Get train indices that correspond to actual training data\n",
    "    train_indices = []\n",
    "    train_labels = []\n",
    "    for idx, acc in enumerate(data.account_names):\n",
    "        if acc in train_df['account'].values:\n",
    "            train_indices.append(idx)\n",
    "            train_labels.append(data.y[idx].item())\n",
    "    \n",
    "    train_indices = np.array(train_indices)\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "    # Check class distribution\n",
    "    unique, counts = np.unique(train_labels, return_counts=True)\n",
    "    if len(unique) < 2:\n",
    "        print(\"Warning: Only one class found in training data\")\n",
    "        return 0.0\n",
    "    \n",
    "    fold_f1_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_indices, train_labels)):\n",
    "        try:\n",
    "            # Convert to actual indices in the graph\n",
    "            actual_train_idx = train_indices[train_idx]\n",
    "            actual_val_idx = train_indices[val_idx]\n",
    "            \n",
    "            # Create model\n",
    "            model = GNNModel(\n",
    "                num_features=data.x.size(1),\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                num_layers=params['num_layers'],\n",
    "                dropout=params['dropout']\n",
    "            ).to(device)\n",
    "            \n",
    "            # Train model\n",
    "            model, val_f1 = train_model(model, data, actual_train_idx, actual_val_idx, params, epochs=100)  # Reduced epochs for optimization\n",
    "            fold_f1_scores.append(val_f1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold}: {e}\")\n",
    "            fold_f1_scores.append(0.0)\n",
    "    \n",
    "    # Return mean F1 score, with penalty for variance (stability preference)\n",
    "    mean_f1 = np.mean(fold_f1_scores)\n",
    "    std_f1 = np.std(fold_f1_scores)\n",
    "    \n",
    "    # Penalize high variance (less stable models)\n",
    "    return mean_f1 - 0.1 * std_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(y_true, y_pred, y_prob=None):\n",
    "    \"\"\"Comprehensive evaluation metrics for imbalanced classification\"\"\"\n",
    "    metrics = {\n",
    "        'f1_bad': f1_score(y_true.cpu(), y_pred.cpu(), pos_label=1, zero_division=0),\n",
    "        'f1_macro': f1_score(y_true.cpu(), y_pred.cpu(), average='macro', zero_division=0),\n",
    "        'f1_weighted': f1_score(y_true.cpu(), y_pred.cpu(), average='weighted', zero_division=0),\n",
    "        'precision_bad': precision_score(y_true.cpu(), y_pred.cpu(), pos_label=1, zero_division=0),\n",
    "        'recall_bad': recall_score(y_true.cpu(), y_pred.cpu(), pos_label=1, zero_division=0),\n",
    "        'accuracy': accuracy_score(y_true.cpu(), y_pred.cpu())\n",
    "    }\n",
    "    \n",
    "    # Add AUC if probabilities are provided\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            metrics['roc_auc'] = roc_auc_score(y_true.cpu(), y_prob[:, 1].cpu())\n",
    "        except:\n",
    "            metrics['roc_auc'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_with_cv(data, train_df, n_trials=None):\n",
    "    \"\"\"Train with cross-validation and hyperparameter tuning, optimized for imbalanced data\"\"\"\n",
    "    if n_trials is None:\n",
    "        n_trials = Config.TRAINING_PARAMS['n_trials']\n",
    "    \n",
    "    print(f\"Starting hyperparameter optimization with {n_trials} trials...\")\n",
    "    print(\"Focusing on bad class F1-score for imbalanced data (1:9 ratio)\")\n",
    "    \n",
    "    # Create Optuna study with better sampler for imbalanced classification\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.HyperbandPruner()\n",
    "    )\n",
    "    \n",
    "    study.optimize(lambda trial: objective(trial, data, train_df), n_trials=n_trials)\n",
    "    \n",
    "    print(f\"Best trial score (F1-bad): {study.best_trial.value:.4f}\")\n",
    "    print(f\"Best params: {study.best_params}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Get train indices\n",
    "    train_indices = []\n",
    "    train_labels = []\n",
    "    for idx, acc in enumerate(data.account_names):\n",
    "        if acc in train_df['account'].values:\n",
    "            train_indices.append(idx)\n",
    "            train_labels.append(data.y[idx].item())\n",
    "    \n",
    "    train_indices = np.array(train_indices)\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "    # 5-fold CV with best parameters for final evaluation\n",
    "    skf = StratifiedKFold(n_splits=Config.TRAINING_PARAMS['n_splits'], shuffle=True, random_state=42)\n",
    "    fold_models = []\n",
    "    fold_metrics = []\n",
    "    \n",
    "    print(f\"\\nFinal training with best parameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_indices, train_labels)):\n",
    "        print(f\"Training fold {fold + 1}/{Config.TRAINING_PARAMS['n_splits']}...\")\n",
    "        \n",
    "        actual_train_idx = train_indices[train_idx]\n",
    "        actual_val_idx = train_indices[val_idx]\n",
    "        \n",
    "        model = GNNModel(\n",
    "            num_features=data.x.size(1),\n",
    "            hidden_dim=best_params['hidden_dim'],\n",
    "            num_layers=best_params['num_layers'],\n",
    "            dropout=best_params['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        model, val_f1 = train_model(model, data, actual_train_idx, actual_val_idx, best_params)\n",
    "        \n",
    "        # Comprehensive evaluation on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data.x, data.edge_index)\n",
    "            val_pred = val_out[actual_val_idx].argmax(dim=1)\n",
    "            val_true = data.y[actual_val_idx]\n",
    "            val_prob = F.softmax(val_out[actual_val_idx], dim=1)\n",
    "            \n",
    "            metrics = comprehensive_evaluation(val_true, val_pred, val_prob)\n",
    "        \n",
    "        fold_models.append(model)\n",
    "        fold_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} results:\")\n",
    "        print(f\"  Bad F1: {metrics['f1_bad']:.4f}\")\n",
    "        print(f\"  Bad Precision: {metrics['precision_bad']:.4f}\")\n",
    "        print(f\"  Bad Recall: {metrics['recall_bad']:.4f}\")\n",
    "        print(f\"  Macro F1: {metrics['f1_macro']:.4f}\")\n",
    "    \n",
    "    # Select best fold based on bad F1 score (most important for imbalanced data)\n",
    "    best_fold_idx = np.argmax([m['f1_bad'] for m in fold_metrics])\n",
    "    best_model = fold_models[best_fold_idx]\n",
    "    best_metrics = fold_metrics[best_fold_idx]\n",
    "    \n",
    "    # Calculate average metrics across folds\n",
    "    avg_metrics = {}\n",
    "    for key in fold_metrics[0].keys():\n",
    "        avg_metrics[f'avg_{key}'] = np.mean([m[key] for m in fold_metrics])\n",
    "        avg_metrics[f'std_{key}'] = np.std([m[key] for m in fold_metrics])\n",
    "    \n",
    "    print(f\"\\nCross-validation results:\")\n",
    "    print(f\"Best fold: {best_fold_idx + 1}\")\n",
    "    print(f\"Average bad F1: {avg_metrics['avg_f1_bad']:.4f} Â± {avg_metrics['std_f1_bad']:.4f}\")\n",
    "    print(f\"Average macro F1: {avg_metrics['avg_f1_macro']:.4f} Â± {avg_metrics['std_f1_macro']:.4f}\")\n",
    "    print(f\"Average bad precision: {avg_metrics['avg_precision_bad']:.4f} Â± {avg_metrics['std_precision_bad']:.4f}\")\n",
    "    print(f\"Average bad recall: {avg_metrics['avg_recall_bad']:.4f} Â± {avg_metrics['std_recall_bad']:.4f}\")\n",
    "    \n",
    "    # Combine best metrics with averages\n",
    "    final_metrics = {**best_metrics, **avg_metrics}\n",
    "    \n",
    "    return best_model, final_metrics, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241c6dd",
   "metadata": {},
   "source": [
    "## 9. Prediction and Saving Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data(model, data, test_df):\n",
    "    \"\"\"Make predictions on test data\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get test indices\n",
    "    test_indices = []\n",
    "    test_accounts = []\n",
    "    for idx, acc in enumerate(data.account_names):\n",
    "        if acc in test_df['account'].values:\n",
    "            test_indices.append(idx)\n",
    "            test_accounts.append(acc)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        test_pred = out[test_indices].argmax(dim=1)\n",
    "    \n",
    "    # Create prediction dataframe\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'account': test_accounts,\n",
    "        'Predict': test_pred.cpu().numpy()\n",
    "    })\n",
    "    \n",
    "    # Sort by account to match original order\n",
    "    predictions_df = predictions_df.sort_values('account').reset_index(drop=True)\n",
    "    \n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d115f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model, predictions_df, metrics, params):\n",
    "    \"\"\"Save model and predictions safely\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    try:\n",
    "        # Save model\n",
    "        model_path = f'/content/best_gnn_model_{timestamp}.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'metrics': metrics,\n",
    "            'params': params,\n",
    "            'timestamp': timestamp\n",
    "        }, model_path)\n",
    "        print(f\"Model saved to: {model_path}\")\n",
    "        \n",
    "        # Save predictions\n",
    "        pred_path = f'/content/test_predictions_{timestamp}.csv'\n",
    "        predictions_df.to_csv(pred_path, index=False)\n",
    "        print(f\"Predictions saved to: {pred_path}\")\n",
    "        \n",
    "        # Save to Google Drive if mounted\n",
    "        if os.path.exists('/content/drive'):\n",
    "            drive_model_path = f'/content/drive/MyDrive/best_gnn_model_{timestamp}.pth'\n",
    "            drive_pred_path = f'/content/drive/MyDrive/test_predictions_{timestamp}.csv'\n",
    "            \n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'metrics': metrics,\n",
    "                'params': params,\n",
    "                'timestamp': timestamp\n",
    "            }, drive_model_path)\n",
    "            \n",
    "            predictions_df.to_csv(drive_pred_path, index=False)\n",
    "            print(f\"Also saved to Google Drive: {drive_model_path}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_predict(data, test_df, model_path):\n",
    "    \"\"\"Load saved model and make predictions\"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Create model with saved parameters\n",
    "        params = checkpoint['params']\n",
    "        model = GNNModel(\n",
    "            num_features=data.x.size(1),\n",
    "            hidden_dim=params['hidden_dim'],\n",
    "            num_layers=params['num_layers'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Model loaded successfully from: {model_path}\")\n",
    "        print(f\"Model metrics: {checkpoint['metrics']}\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions_df = predict_test_data(model, data, test_df)\n",
    "        \n",
    "        return predictions_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c9146",
   "metadata": {},
   "source": [
    "## 10. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mode='normal'):\n",
    "    \"\"\"Main function with improved error handling and data validation\"\"\"\n",
    "    print(f\"Running in {mode} mode...\")\n",
    "    print(f\"Configuration: {Config.TRAINING_PARAMS['n_trials']} trials, {Config.TRAINING_PARAMS['n_splits']}-fold CV\")\n",
    "    \n",
    "    # Load data with error handling\n",
    "    try:\n",
    "        if os.path.exists('/content/train_acc.csv'):\n",
    "            print(\"Loading data from local copies...\")\n",
    "            train_df = pd.read_csv('/content/train_acc.csv')\n",
    "            test_df = pd.read_csv('/content/test_acc_predict.csv')\n",
    "            transactions_df = pd.read_csv('/content/transactions.csv')\n",
    "        else:\n",
    "            print(\"Loading data from Google Drive...\")\n",
    "            train_df = pd.read_csv('/content/drive/MyDrive/original_data/train_acc.csv')\n",
    "            test_df = pd.read_csv('/content/drive/MyDrive/original_data/test_acc_predict.csv')\n",
    "            transactions_df = pd.read_csv('/content/drive/MyDrive/original_data/transactions.csv')\n",
    "        \n",
    "        print(f\"Data loaded successfully:\")\n",
    "        print(f\"- Train: {len(train_df)} samples\")\n",
    "        print(f\"- Test: {len(test_df)} samples\") \n",
    "        print(f\"- Transactions: {len(transactions_df)} records\")\n",
    "        \n",
    "        # Display class distribution\n",
    "        class_dist = train_df['flag'].value_counts().sort_index()\n",
    "        print(f\"- Class distribution: {dict(class_dist)}\")\n",
    "        ratio = class_dist[0] / class_dist[1] if len(class_dist) > 1 else 1\n",
    "        print(f\"- Imbalance ratio: 1:{ratio:.1f}\")\n",
    "        \n",
    "        # Quick data inspection\n",
    "        if len(transactions_df) > 0:\n",
    "            print(f\"- Transaction columns: {list(transactions_df.columns)}\")\n",
    "            \n",
    "            # Detect amount column\n",
    "            amount_col = 'value' if 'value' in transactions_df.columns else 'amount'\n",
    "            if amount_col in transactions_df.columns:\n",
    "                try:\n",
    "                    # Convert to numeric in case it's stored as string\n",
    "                    amount_values = pd.to_numeric(transactions_df[amount_col], errors='coerce')\n",
    "                    amount_min = amount_values.min()\n",
    "                    amount_max = amount_values.max()\n",
    "                    print(f\"- Amount range: ${amount_min:.2f} - ${amount_max:.2f}\")\n",
    "                except:\n",
    "                    print(f\"- Amount column: {amount_col} (contains non-numeric values)\")\n",
    "            \n",
    "            # Check transaction coverage with flexible column names\n",
    "            train_accounts = set(train_df['account'])\n",
    "            test_accounts = set(test_df['account'])\n",
    "            all_dataset_accounts = train_accounts.union(test_accounts)\n",
    "            \n",
    "            if 'from_account' in transactions_df.columns:\n",
    "                txn_accounts = set(transactions_df['from_account']).union(set(transactions_df['to_account']))\n",
    "            else:\n",
    "                txn_accounts = set(transactions_df['sender']).union(set(transactions_df['receiver']))\n",
    "            \n",
    "            coverage = len(all_dataset_accounts.intersection(txn_accounts)) / len(all_dataset_accounts)\n",
    "            print(f\"- Transaction coverage: {coverage:.1%}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please check file paths and data format\")\n",
    "        return None\n",
    "    \n",
    "    # Create graph with optimized data processing\n",
    "    try:\n",
    "        print(\"\\nCreating graph structure...\")\n",
    "        data = create_graph_from_accounts(train_df, test_df, transactions_df)\n",
    "        data = data.to(device)\n",
    "        \n",
    "        print(f\"Graph created successfully:\")\n",
    "        print(f\"- Nodes: {data.x.size(0)}\")\n",
    "        print(f\"- Edges: {data.edge_index.size(1)}\")\n",
    "        print(f\"- Features: {data.x.size(1)}\")\n",
    "        \n",
    "        # Calculate graph density\n",
    "        num_nodes = data.x.size(0)\n",
    "        num_edges = data.edge_index.size(1)\n",
    "        max_edges = num_nodes * (num_nodes - 1)\n",
    "        density = num_edges / max_edges if max_edges > 0 else 0\n",
    "        print(f\"- Graph density: {density:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating graph: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "    if mode == 'normal':\n",
    "        # Training mode\n",
    "        print(f\"\\nStarting training with optimized parameters for imbalanced data...\")\n",
    "        \n",
    "        try:\n",
    "            best_model, best_metrics, best_params = train_with_cv(\n",
    "                data, train_df, n_trials=Config.TRAINING_PARAMS['n_trials']\n",
    "            )\n",
    "            \n",
    "            # Make predictions on test data\n",
    "            print(\"\\nMaking predictions on test data...\")\n",
    "            predictions_df = predict_test_data(best_model, data, test_df)\n",
    "            \n",
    "            # Save results\n",
    "            print(\"Saving results...\")\n",
    "            success = save_results(best_model, predictions_df, best_metrics, best_params)\n",
    "            \n",
    "            if success:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "                print(\"=\"*50)\n",
    "                print(f\"Primary metric (Bad F1): {best_metrics['f1_bad']:.4f}\")\n",
    "                print(f\"Cross-validation avg: {best_metrics.get('avg_f1_bad', 0):.4f} Â± {best_metrics.get('std_f1_bad', 0):.4f}\")\n",
    "                print(f\"Macro F1: {best_metrics['f1_macro']:.4f}\")\n",
    "                print(f\"Bad Precision: {best_metrics['precision_bad']:.4f}\")\n",
    "                print(f\"Bad Recall: {best_metrics['recall_bad']:.4f}\")\n",
    "                print(f\"Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "                \n",
    "                print(f\"\\nPrediction Summary:\")\n",
    "                print(f\"- Total predictions: {len(predictions_df)}\")\n",
    "                pred_dist = predictions_df['Predict'].value_counts().sort_index()\n",
    "                print(f\"- Predicted distribution: {dict(pred_dist)}\")\n",
    "                if len(pred_dist) > 1:\n",
    "                    pred_ratio = pred_dist[0] / pred_dist[1]\n",
    "                    print(f\"- Predicted ratio: 1:{pred_ratio:.1f}\")\n",
    "                \n",
    "                return {\n",
    "                    'model': best_model,\n",
    "                    'predictions': predictions_df,\n",
    "                    'metrics': best_metrics,\n",
    "                    'params': best_params\n",
    "                }\n",
    "            else:\n",
    "                print(\"Failed to save results!\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during training: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    elif mode == 'test':\n",
    "        # Test mode - load model and predict\n",
    "        print(\"Test mode: Loading saved model...\")\n",
    "        \n",
    "        # Find the latest model file\n",
    "        model_files = []\n",
    "        for location in ['/content', '/content/drive/MyDrive']:\n",
    "            if os.path.exists(location):\n",
    "                files = [f for f in os.listdir(location) if f.startswith('best_gnn_model_') and f.endswith('.pth')]\n",
    "                model_files.extend([os.path.join(location, f) for f in files])\n",
    "        \n",
    "        if not model_files:\n",
    "            print(\"No saved model found!\")\n",
    "            return None\n",
    "        \n",
    "        # Use the most recent model\n",
    "        model_path = sorted(model_files)[-1]\n",
    "        print(f\"Loading model from: {model_path}\")\n",
    "        \n",
    "        # Load model and predict\n",
    "        predictions_df = load_model_and_predict(data, test_df, model_path)\n",
    "        \n",
    "        if predictions_df is not None:\n",
    "            # Save predictions\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            pred_path = f'/content/test_predictions_inference_{timestamp}.csv'\n",
    "            predictions_df.to_csv(pred_path, index=False)\n",
    "            print(f\"Predictions saved to: {pred_path}\")\n",
    "            \n",
    "            print(f\"\\nPrediction Summary:\")\n",
    "            print(f\"- Shape: {predictions_df.shape}\")\n",
    "            pred_dist = predictions_df['Predict'].value_counts().sort_index()\n",
    "            print(f\"- Distribution: {dict(pred_dist)}\")\n",
    "            \n",
    "            return predictions_df\n",
    "        else:\n",
    "            print(\"Failed to make predictions!\")\n",
    "            return None\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unknown mode: {mode}. Use 'normal' or 'test'\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b7094",
   "metadata": {},
   "source": [
    "## 11. Run Training (Normal Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a68290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training and prediction with enhanced feature set\n",
    "print(\"GNN Account Classification - Enhanced with Gas Analysis\")\n",
    "print(\"=\"*70)\n",
    "print(\"Configuration Summary:\")\n",
    "print(f\"- Model params: {Config.MODEL_PARAMS}\")\n",
    "print(f\"- Training params: {Config.TRAINING_PARAMS}\")\n",
    "print(f\"- Feature params: {Config.FEATURE_PARAMS}\")\n",
    "print(\"=\"*70)\n",
    "print(\"Enhanced Features:\")\n",
    "print(\"âœ… Basic transaction features (15): amounts, counts, statistics\")\n",
    "print(\"âœ… Gas analysis features (8): gas consumption, gas prices, efficiency\")\n",
    "print(\"âœ… Advanced pattern features (8): laundering indicators, complexity\")\n",
    "print(\"âœ… Temporal features (2): time span, transaction intervals\")\n",
    "print(\"=\"*70)\n",
    "print(\"Data Format Adaptation:\")\n",
    "print(\"- Supports: 'from_account/to_account/value/gas/gas_price' (your data)\")\n",
    "print(\"- Fallback: 'sender/receiver/amount' format\")\n",
    "print(\"- Auto-detects: 'transaction_time_utc' or 'timestamp' for temporal features\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Execute main function with enhanced features\n",
    "results = main(mode='normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846fb1f",
   "metadata": {},
   "source": [
    "## 12. Run Test Mode (Load Model and Predict Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test mode to load saved model and predict\n",
    "# main(mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d38ee9",
   "metadata": {},
   "source": [
    "## 13. Quick Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cec936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Quick analysis of results\n",
    "import os\n",
    "\n",
    "# List all saved files\n",
    "print(\"Saved models:\")\n",
    "model_files = [f for f in os.listdir('/content') if f.startswith('best_gnn_model_')]\n",
    "for f in model_files:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "print(\"\\nSaved predictions:\")\n",
    "pred_files = [f for f in os.listdir('/content') if f.startswith('test_predictions_')]\n",
    "for f in pred_files:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Load and display latest predictions\n",
    "if pred_files:\n",
    "    latest_pred = sorted(pred_files)[-1]\n",
    "    df = pd.read_csv(f'/content/{latest_pred}')\n",
    "    print(f\"\\nLatest predictions from {latest_pred}:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Class distribution:\\n{df['Predict'].value_counts()}\")\n",
    "    print(f\"\\nFirst 10 predictions:\")\n",
    "    print(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
