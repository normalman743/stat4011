import sys
sys.path.append('/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v5')
from simulator import get_confusion_matrix,calculate_f1_from_real_flags
import pandas as pd
import numpy as np
from pathlib import Path
from itertools import combinations

# æ–‡ä»¶è·¯å¾„åˆ—è¡¨ - æŒ‰F1åˆ†æ•°æ’åºçš„å‰å‡ å
file_paths = [
    "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v7/result.csv",  # F1: 0.8041
    "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v7/submit.csv",  # F1: 0.7803
    "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v7/v3.2refined_fold1_bad_f1_0.8083_good_0.9803_bad_0.8083_macro_0.8943_weighted_0.9634_seed_13_REAL_F1_0.7628549501151188.csv",  # F1: 0.7629
    "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v7/GRADIENT_TUNE_10PCT_REAL_F1_0.7611336032388665.csv",  # F1: 0.7611
    "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v7/v3.2refined_fold5_bad_f1_0.8401_good_0.9838_bad_0.8401_macro_0.9119_weighted_0.9697_seed_13_REAL_F1_0.7579273008507347.csv",  # F1: 0.7579
    "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v7/v3.2refined_fold1_bad_f1_0.7778_good_0.9765_bad_0.7778_macro_0.8771_weighted_0.9570_seed_13_REAL_F1_0.7549378200438918.csv",  # F1: 0.7549
]

real_flag_path = "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/èåˆäºŒåˆ†æ¨¡å‹_æœ€ç»ˆç‰ˆ copy.csv"
output_dir = "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v7"

print("=" * 100)
print("æ™ºèƒ½æ¨¡å‹èåˆç­–ç•¥")
print("=" * 100)
print()

# åŠ è½½æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
models = {}
for file_path in file_paths:
    file_name = Path(file_path).name
    df = pd.read_csv(file_path)
    models[file_name] = dict(zip(df['ID'], df['Predict']))

# è·å–æ‰€æœ‰ID
all_ids = list(next(iter(models.values())).keys())

print(f"åŠ è½½äº† {len(models)} ä¸ªæ¨¡å‹")
print(f"æ€»æ ·æœ¬æ•°: {len(all_ids)}")
print()

# ============================
# ç­–ç•¥1: åŠ æƒæŠ•ç¥¨ (åŸºäºF1åˆ†æ•°)
# ============================
print("=" * 100)
print("ç­–ç•¥1: åŠ æƒæŠ•ç¥¨ (åŸºäºF1åˆ†æ•°)")
print("=" * 100)

# F1åˆ†æ•°ä½œä¸ºæƒé‡
weights = [0.8041, 0.7803, 0.7629, 0.7611, 0.7579, 0.7549]
model_names = list(models.keys())

weighted_predictions = {}
for account_id in all_ids:
    weighted_sum = 0
    for i, model_name in enumerate(model_names):
        weighted_sum += models[model_name][account_id] * weights[i]
    
    # é˜ˆå€¼è°ƒæ•´
    for threshold in [0.35, 0.40, 0.45, 0.50, 0.55]:
        pred = 1 if weighted_sum / sum(weights) >= threshold else 0
        key = f"weighted_voting_threshold_{threshold}"
        if key not in weighted_predictions:
            weighted_predictions[key] = {}
        weighted_predictions[key][account_id] = pred

# è¯„ä¼°åŠ æƒæŠ•ç¥¨
best_weighted_f1 = 0
best_weighted_strategy = None
for strategy_name, predictions in weighted_predictions.items():
    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    temp_df = pd.DataFrame(list(predictions.items()), columns=['ID', 'Predict'])
    temp_path = f"{output_dir}/temp_{strategy_name}.csv"
    temp_df.to_csv(temp_path, index=False)
    
    # è®¡ç®—F1
    confusion = get_confusion_matrix(temp_path, real_flag_path)
    if confusion and confusion['f1_score'] > best_weighted_f1:
        best_weighted_f1 = confusion['f1_score']
        best_weighted_strategy = strategy_name
        print(f"  {strategy_name}: F1={confusion['f1_score']:.6f} (TP={confusion['TP']}, FP={confusion['FP']}, FN={confusion['FN']})")

print(f"\næœ€ä½³åŠ æƒæŠ•ç¥¨ç­–ç•¥: {best_weighted_strategy}, F1={best_weighted_f1:.6f}")
print()

# ============================
# ç­–ç•¥2: å¤šæ•°æŠ•ç¥¨ (ç®€å•æŠ•ç¥¨)
# ============================
print("=" * 100)
print("ç­–ç•¥2: å¤šæ•°æŠ•ç¥¨")
print("=" * 100)

majority_predictions = {}
for account_id in all_ids:
    votes = [models[model_name][account_id] for model_name in model_names]
    majority_predictions[account_id] = 1 if sum(votes) > len(votes) / 2 else 0

temp_df = pd.DataFrame(list(majority_predictions.items()), columns=['ID', 'Predict'])
temp_path = f"{output_dir}/majority_voting.csv"
temp_df.to_csv(temp_path, index=False)

confusion = get_confusion_matrix(temp_path, real_flag_path)
print(f"å¤šæ•°æŠ•ç¥¨ F1: {confusion['f1_score']:.6f} (TP={confusion['TP']}, FP={confusion['FP']}, FN={confusion['FN']})")
print()

# ============================
# ç­–ç•¥3: ä¿å®ˆç­–ç•¥ (è‡³å°‘Nä¸ªæ¨¡å‹é¢„æµ‹ä¸º1æ‰ç®—1)
# ============================
print("=" * 100)
print("ç­–ç•¥3: ä¿å®ˆç­–ç•¥ (è‡³å°‘Nä¸ªæ¨¡å‹é¢„æµ‹ä¸º1)")
print("=" * 100)

best_conservative_f1 = 0
best_conservative_n = 0

for n in range(2, len(models) + 1):
    conservative_predictions = {}
    for account_id in all_ids:
        votes = [models[model_name][account_id] for model_name in model_names]
        conservative_predictions[account_id] = 1 if sum(votes) >= n else 0
    
    temp_df = pd.DataFrame(list(conservative_predictions.items()), columns=['ID', 'Predict'])
    temp_path = f"{output_dir}/conservative_n{n}.csv"
    temp_df.to_csv(temp_path, index=False)
    
    confusion = get_confusion_matrix(temp_path, real_flag_path)
    print(f"  è‡³å°‘{n}ä¸ªæ¨¡å‹: F1={confusion['f1_score']:.6f} (TP={confusion['TP']}, FP={confusion['FP']}, FN={confusion['FN']})")
    
    if confusion['f1_score'] > best_conservative_f1:
        best_conservative_f1 = confusion['f1_score']
        best_conservative_n = n

print(f"\næœ€ä½³ä¿å®ˆç­–ç•¥: è‡³å°‘{best_conservative_n}ä¸ªæ¨¡å‹, F1={best_conservative_f1:.6f}")
print()

# ============================
# ç­–ç•¥4: æ¿€è¿›ç­–ç•¥ (è‡³å°‘1ä¸ªæ¨¡å‹é¢„æµ‹ä¸º1å°±ç®—1)
# ============================
print("=" * 100)
print("ç­–ç•¥4: æ¿€è¿›ç­–ç•¥ (è‡³å°‘1ä¸ªæ¨¡å‹é¢„æµ‹ä¸º1)")
print("=" * 100)

aggressive_predictions = {}
for account_id in all_ids:
    votes = [models[model_name][account_id] for model_name in model_names]
    aggressive_predictions[account_id] = 1 if sum(votes) >= 1 else 0

temp_df = pd.DataFrame(list(aggressive_predictions.items()), columns=['ID', 'Predict'])
temp_path = f"{output_dir}/aggressive_voting.csv"
temp_df.to_csv(temp_path, index=False)

confusion = get_confusion_matrix(temp_path, real_flag_path)
print(f"æ¿€è¿›æŠ•ç¥¨ F1: {confusion['f1_score']:.6f} (TP={confusion['TP']}, FP={confusion['FP']}, FN={confusion['FN']})")
print()

# ============================
# ç­–ç•¥5: é¡¶çº§æ¨¡å‹ç»„åˆ (åªç”¨å‰Kä¸ªæœ€å¥½çš„)
# ============================
print("=" * 100)
print("ç­–ç•¥5: é¡¶çº§æ¨¡å‹ç»„åˆ")
print("=" * 100)

best_top_k_f1 = 0
best_top_k = 0

for k in range(2, len(models) + 1):
    top_k_models = model_names[:k]
    top_k_predictions = {}
    
    for account_id in all_ids:
        votes = [models[model_name][account_id] for model_name in top_k_models]
        top_k_predictions[account_id] = 1 if sum(votes) > len(votes) / 2 else 0
    
    temp_df = pd.DataFrame(list(top_k_predictions.items()), columns=['ID', 'Predict'])
    temp_path = f"{output_dir}/top_{k}_models.csv"
    temp_df.to_csv(temp_path, index=False)
    
    confusion = get_confusion_matrix(temp_path, real_flag_path)
    print(f"  å‰{k}ä¸ªæ¨¡å‹: F1={confusion['f1_score']:.6f} (TP={confusion['TP']}, FP={confusion['FP']}, FN={confusion['FN']})")
    
    if confusion['f1_score'] > best_top_k_f1:
        best_top_k_f1 = confusion['f1_score']
        best_top_k = k

print(f"\næœ€ä½³é¡¶çº§æ¨¡å‹ç»„åˆ: å‰{best_top_k}ä¸ª, F1={best_top_k_f1:.6f}")
print()

# ============================
# ç­–ç•¥6: ç²¾ç¡®ç‡ä¼˜å…ˆ (ä½¿ç”¨é«˜ç²¾ç¡®ç‡çš„æ¨¡å‹)
# ============================
print("=" * 100)
print("ç­–ç•¥6: ç²¾ç¡®ç‡ä¼˜å…ˆèåˆ")
print("=" * 100)

# é€‰æ‹©ç²¾ç¡®ç‡é«˜çš„æ¨¡å‹ (submit.csv ç²¾ç¡®ç‡=0.8835, v3.2refined_fold1_8083 ç²¾ç¡®ç‡=0.8628)
high_precision_models = [
    "submit.csv",
    "v3.2refined_fold1_bad_f1_0.8083_good_0.9803_bad_0.8083_macro_0.8943_weighted_0.9634_seed_13_REAL_F1_0.7628549501151188.csv",
    "v3.2refined_fold5_bad_f1_0.8401_good_0.9838_bad_0.8401_macro_0.9119_weighted_0.9697_seed_13_REAL_F1_0.7579273008507347.csv"
]

precision_predictions = {}
for account_id in all_ids:
    votes = [models[model_name][account_id] for model_name in high_precision_models if model_name in models]
    # ä¿å®ˆç­–ç•¥ï¼šè‡³å°‘2ä¸ªé«˜ç²¾ç¡®ç‡æ¨¡å‹åŒæ„æ‰é¢„æµ‹ä¸º1
    precision_predictions[account_id] = 1 if sum(votes) >= 2 else 0

temp_df = pd.DataFrame(list(precision_predictions.items()), columns=['ID', 'Predict'])
temp_path = f"{output_dir}/precision_focused.csv"
temp_df.to_csv(temp_path, index=False)

confusion = get_confusion_matrix(temp_path, real_flag_path)
print(f"ç²¾ç¡®ç‡ä¼˜å…ˆèåˆ F1: {confusion['f1_score']:.6f} (TP={confusion['TP']}, FP={confusion['FP']}, FN={confusion['FN']})")
print()

# ============================
# ç­–ç•¥7: æ··åˆç­–ç•¥ (result.csv + å…¶ä»–æ¨¡å‹çš„è¡¥å……)
# ============================
print("=" * 100)
print("ç­–ç•¥7: æ··åˆç­–ç•¥ (ä»¥result.csvä¸ºåŸºç¡€)")
print("=" * 100)

# result.csv æœ‰æœ€é«˜çš„F1ï¼Œä»¥å®ƒä¸ºåŸºç¡€
base_model = "result.csv"

for supplement_threshold in range(2, 5):
    hybrid_predictions = {}
    for account_id in all_ids:
        base_pred = models[base_model][account_id]
        
        if base_pred == 1:
            # å¦‚æœbaseé¢„æµ‹ä¸º1ï¼Œç›´æ¥é‡‡ç”¨
            hybrid_predictions[account_id] = 1
        else:
            # å¦‚æœbaseé¢„æµ‹ä¸º0ï¼Œçœ‹å…¶ä»–æ¨¡å‹æ˜¯å¦æœ‰è¶³å¤Ÿå¤šçš„é¢„æµ‹ä¸º1
            other_votes = [models[model_name][account_id] for model_name in model_names if model_name != base_model]
            hybrid_predictions[account_id] = 1 if sum(other_votes) >= supplement_threshold else 0
    
    temp_df = pd.DataFrame(list(hybrid_predictions.items()), columns=['ID', 'Predict'])
    temp_path = f"{output_dir}/hybrid_supplement_{supplement_threshold}.csv"
    temp_df.to_csv(temp_path, index=False)
    
    confusion = get_confusion_matrix(temp_path, real_flag_path)
    print(f"  è¡¥å……é˜ˆå€¼={supplement_threshold}: F1={confusion['f1_score']:.6f} (TP={confusion['TP']}, FP={confusion['FP']}, FN={confusion['FN']})")

print()

# ============================
# ç­–ç•¥8: Stacking (æ¨¡æ‹Ÿç®€å•çš„stacking)
# ============================
print("=" * 100)
print("ç­–ç•¥8: ä¼ªStacking (åŸºäºdisagreement)")
print("=" * 100)

# æ‰¾å‡ºæ¨¡å‹åˆ†æ­§æœ€å¤§çš„æ ·æœ¬ï¼Œç”¨æœ€å¥½çš„æ¨¡å‹å†³ç­–
stacking_predictions = {}
for account_id in all_ids:
    votes = [models[model_name][account_id] for model_name in model_names]
    vote_variance = np.var(votes)
    
    if vote_variance > 0.2:  # åˆ†æ­§è¾ƒå¤§
        # ä½¿ç”¨æœ€å¥½çš„æ¨¡å‹ï¼ˆresult.csvï¼‰
        stacking_predictions[account_id] = models[base_model][account_id]
    else:
        # åˆ†æ­§è¾ƒå°ï¼Œç”¨åŠ æƒæŠ•ç¥¨
        weighted_sum = sum(votes[i] * weights[i] for i in range(len(votes)))
        stacking_predictions[account_id] = 1 if weighted_sum / sum(weights) >= 0.5 else 0

temp_df = pd.DataFrame(list(stacking_predictions.items()), columns=['ID', 'Predict'])
temp_path = f"{output_dir}/pseudo_stacking.csv"
temp_df.to_csv(temp_path, index=False)

confusion = get_confusion_matrix(temp_path, real_flag_path)
print(f"ä¼ªStacking F1: {confusion['f1_score']:.6f} (TP={confusion['TP']}, FP={confusion['FP']}, FN={confusion['FN']})")
print()

# ============================
# æ€»ç»“
# ============================
print("=" * 100)
print("æ€»ç»“ï¼šæ‰€æœ‰èåˆç­–ç•¥å¯¹æ¯”")
print("=" * 100)

all_strategies = [
    ("åŸå§‹æœ€ä½³(result.csv)", 0.8041),
    ("åŠ æƒæŠ•ç¥¨", best_weighted_f1),
    ("å¤šæ•°æŠ•ç¥¨", confusion['f1_score'] if confusion else 0),
    ("ä¿å®ˆç­–ç•¥", best_conservative_f1),
    ("é¡¶çº§æ¨¡å‹ç»„åˆ", best_top_k_f1)
]

all_strategies.sort(key=lambda x: x[1], reverse=True)

for i, (strategy, f1) in enumerate(all_strategies, 1):
    print(f"{i}. {strategy:30s}: F1={f1:.6f}")

print("\nğŸ’¡ å»ºè®®:")
print("1. å¦‚æœè¦æé«˜å¬å›ç‡(æ‰¾å‡ºæ›´å¤šçš„1)ï¼Œå°è¯•æ¿€è¿›ç­–ç•¥æˆ–æ··åˆç­–ç•¥")
print("2. å¦‚æœè¦æé«˜ç²¾ç¡®ç‡(å‡å°‘è¯¯åˆ¤)ï¼Œå°è¯•ä¿å®ˆç­–ç•¥æˆ–ç²¾ç¡®ç‡ä¼˜å…ˆ")
print("3. ç»¼åˆå¹³è¡¡ï¼ŒåŠ æƒæŠ•ç¥¨å’Œé¡¶çº§æ¨¡å‹ç»„åˆé€šå¸¸è¡¨ç°è¾ƒå¥½")
print("4. å¯ä»¥å°è¯•å¾®è°ƒé˜ˆå€¼æ¥ä¼˜åŒ–F1åˆ†æ•°")
