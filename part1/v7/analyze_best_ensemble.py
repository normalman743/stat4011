import sys
sys.path.append('/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v5')
from simulator import get_confusion_matrix, calculate_f1_from_real_flags
import pandas as pd
import numpy as np

# æ–‡ä»¶è·¯å¾„
result_csv = "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v7/result.csv"
submit_csv = "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v7/submit.csv"
real_flag_path = "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/èåˆäºŒåˆ†æ¨¡å‹_æœ€ç»ˆç‰ˆ copy.csv"
output_dir = "/Users/mannormal/Desktop/è¯¾ç¨‹/y4t1/stat 4011/Qi Zihan/v7"

print("=" * 100)
print("é‡ç° F1=0.820154 çš„èåˆç­–ç•¥")
print("=" * 100)
print()

# åŠ è½½æ•°æ®
result_df = pd.read_csv(result_csv)
submit_df = pd.read_csv(submit_csv)

result_dict = dict(zip(result_df['ID'], result_df['Predict']))
submit_dict = dict(zip(submit_df['ID'], submit_df['Predict']))

all_ids = list(result_dict.keys())

print("=" * 100)
print("åˆ†æä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹")
print("=" * 100)
print(f"result.csv - é¢„æµ‹ä¸º1: {sum(result_dict.values())} ({sum(result_dict.values())/len(all_ids)*100:.2f}%)")
print(f"submit.csv - é¢„æµ‹ä¸º1: {sum(submit_dict.values())} ({sum(submit_dict.values())/len(all_ids)*100:.2f}%)")

# åˆ†æä¸¤ä¸ªæ¨¡å‹çš„ä¸€è‡´æ€§
agreement = sum(1 for id in all_ids if result_dict[id] == submit_dict[id])
print(f"ä¸¤æ¨¡å‹ä¸€è‡´æ€§: {agreement}/{len(all_ids)} ({agreement/len(all_ids)*100:.2f}%)")

# åˆ†æä¸ä¸€è‡´çš„æƒ…å†µ
both_1 = sum(1 for id in all_ids if result_dict[id] == 1 and submit_dict[id] == 1)
result_1_submit_0 = sum(1 for id in all_ids if result_dict[id] == 1 and submit_dict[id] == 0)
result_0_submit_1 = sum(1 for id in all_ids if result_dict[id] == 0 and submit_dict[id] == 1)
both_0 = sum(1 for id in all_ids if result_dict[id] == 0 and submit_dict[id] == 0)

print(f"\né¢„æµ‹åˆ†å¸ƒ:")
print(f"  ä¸¤è€…éƒ½é¢„æµ‹ä¸º1: {both_1}")
print(f"  result=1, submit=0: {result_1_submit_0}")
print(f"  result=0, submit=1: {result_0_submit_1}")
print(f"  ä¸¤è€…éƒ½é¢„æµ‹ä¸º0: {both_0}")
print()

# è·å–å„è‡ªçš„æ··æ·†çŸ©é˜µ
print("=" * 100)
print("å•ç‹¬æ¨¡å‹çš„æ··æ·†çŸ©é˜µ")
print("=" * 100)

result_confusion = get_confusion_matrix(result_csv, real_flag_path)
print(f"result.csv:")
print(f"  TP={result_confusion['TP']}, FP={result_confusion['FP']}, FN={result_confusion['FN']}, TN={result_confusion['TN']}")
print(f"  Precision={result_confusion['precision']:.4f}, Recall={result_confusion['recall']:.4f}")
print(f"  F1={result_confusion['f1_score']:.6f}")
print()

submit_confusion = get_confusion_matrix(submit_csv, real_flag_path)
print(f"submit.csv:")
print(f"  TP={submit_confusion['TP']}, FP={submit_confusion['FP']}, FN={submit_confusion['FN']}, TN={submit_confusion['TN']}")
print(f"  Precision={submit_confusion['precision']:.4f}, Recall={submit_confusion['recall']:.4f}")
print(f"  F1={submit_confusion['f1_score']:.6f}")
print()

# å°è¯•ä¸åŒçš„èåˆç­–ç•¥
print("=" * 100)
print("å°è¯•ä¸åŒçš„èåˆç­–ç•¥")
print("=" * 100)

strategies = []

# ç­–ç•¥A: ç®€å•å¤šæ•°æŠ•ç¥¨
majority_predictions = {}
for id in all_ids:
    votes = result_dict[id] + submit_dict[id]
    majority_predictions[id] = 1 if votes >= 1 else 0  # è‡³å°‘ä¸€ä¸ªé¢„æµ‹ä¸º1

temp_df = pd.DataFrame(list(majority_predictions.items()), columns=['ID', 'Predict'])
temp_path = f"{output_dir}/analysis_majority.csv"
temp_df.to_csv(temp_path, index=False)
f1_majority = calculate_f1_from_real_flags(temp_path, real_flag_path)
confusion_majority = get_confusion_matrix(temp_path, real_flag_path)
print(f"ç­–ç•¥A - è‡³å°‘ä¸€ä¸ªé¢„æµ‹ä¸º1:")
print(f"  F1={f1_majority:.6f}, TP={confusion_majority['TP']}, FP={confusion_majority['FP']}, FN={confusion_majority['FN']}")
strategies.append(("è‡³å°‘ä¸€ä¸ªé¢„æµ‹ä¸º1", f1_majority, confusion_majority))
print()

# ç­–ç•¥B: ä¸¤è€…éƒ½é¢„æµ‹ä¸º1æ‰ç®—1
conservative_predictions = {}
for id in all_ids:
    conservative_predictions[id] = 1 if result_dict[id] == 1 and submit_dict[id] == 1 else 0

temp_df = pd.DataFrame(list(conservative_predictions.items()), columns=['ID', 'Predict'])
temp_path = f"{output_dir}/analysis_conservative.csv"
temp_df.to_csv(temp_path, index=False)
f1_conservative = calculate_f1_from_real_flags(temp_path, real_flag_path)
confusion_conservative = get_confusion_matrix(temp_path, real_flag_path)
print(f"ç­–ç•¥B - ä¸¤è€…éƒ½é¢„æµ‹ä¸º1:")
print(f"  F1={f1_conservative:.6f}, TP={confusion_conservative['TP']}, FP={confusion_conservative['FP']}, FN={confusion_conservative['FN']}")
strategies.append(("ä¸¤è€…éƒ½é¢„æµ‹ä¸º1", f1_conservative, confusion_conservative))
print()

# ç­–ç•¥C: åŠ æƒå¹³å‡ (ä¸åŒé˜ˆå€¼)
for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:
    weighted_predictions = {}
    result_weight = 0.804124  # result.csvçš„F1
    submit_weight = 0.780268  # submit.csvçš„F1
    
    for id in all_ids:
        weighted_avg = (result_dict[id] * result_weight + submit_dict[id] * submit_weight) / (result_weight + submit_weight)
        weighted_predictions[id] = 1 if weighted_avg >= threshold else 0
    
    temp_df = pd.DataFrame(list(weighted_predictions.items()), columns=['ID', 'Predict'])
    temp_path = f"{output_dir}/analysis_weighted_{threshold}.csv"
    temp_df.to_csv(temp_path, index=False)
    f1_weighted = calculate_f1_from_real_flags(temp_path, real_flag_path)
    confusion_weighted = get_confusion_matrix(temp_path, real_flag_path)
    
    print(f"ç­–ç•¥C - åŠ æƒå¹³å‡(é˜ˆå€¼={threshold}):")
    print(f"  F1={f1_weighted:.6f}, TP={confusion_weighted['TP']}, FP={confusion_weighted['FP']}, FN={confusion_weighted['FN']}")
    
    if f1_weighted >= 0.820:
        print(f"  ğŸ¯ æ‰¾åˆ°äº†! è¿™å°±æ˜¯0.82çš„ç­–ç•¥!")
        strategies.append((f"åŠ æƒå¹³å‡(é˜ˆå€¼={threshold})", f1_weighted, confusion_weighted))
        
        # ä¿å­˜æœ€ä½³ç»“æœ
        best_path = f"{output_dir}/BEST_ENSEMBLE_F1_{f1_weighted:.6f}.csv"
        temp_df.to_csv(best_path, index=False)
        print(f"  âœ… å·²ä¿å­˜åˆ°: {best_path}")
    
    strategies.append((f"åŠ æƒå¹³å‡(é˜ˆå€¼={threshold})", f1_weighted, confusion_weighted))
    print()

# ç­–ç•¥D: resultä¸ºä¸»ï¼Œsubmitè¡¥å……
print("ç­–ç•¥D - resultä¸ºä¸»ï¼Œsubmitè¡¥å…… (result=1ç›´æ¥é‡‡ç”¨ï¼Œresult=0çœ‹submit):")
supplement_predictions = {}
for id in all_ids:
    if result_dict[id] == 1:
        supplement_predictions[id] = 1
    else:
        supplement_predictions[id] = submit_dict[id]

temp_df = pd.DataFrame(list(supplement_predictions.items()), columns=['ID', 'Predict'])
temp_path = f"{output_dir}/analysis_supplement.csv"
temp_df.to_csv(temp_path, index=False)
f1_supplement = calculate_f1_from_real_flags(temp_path, real_flag_path)
confusion_supplement = get_confusion_matrix(temp_path, real_flag_path)
print(f"  F1={f1_supplement:.6f}, TP={confusion_supplement['TP']}, FP={confusion_supplement['FP']}, FN={confusion_supplement['FN']}")
strategies.append(("resultä¸ºä¸»+submitè¡¥å……", f1_supplement, confusion_supplement))
print()

# æ’åºå¹¶æ˜¾ç¤º
print("=" * 100)
print("æ‰€æœ‰ç­–ç•¥æ’å")
print("=" * 100)
strategies.sort(key=lambda x: x[1], reverse=True)

for i, (name, f1, conf) in enumerate(strategies, 1):
    print(f"{i}. {name:30s}: F1={f1:.6f} | TP={conf['TP']:3d}, FP={conf['FP']:3d}, FN={conf['FN']:3d}, TN={conf['TN']:4d}")

print()
print("=" * 100)
print("ç»“è®º")
print("=" * 100)
print(f"æœ€ä½³ç­–ç•¥: {strategies[0][0]}")
print(f"F1åˆ†æ•°: {strategies[0][1]:.6f}")
print(f"ç›¸æ¯”result.csvæå‡: {(strategies[0][1] - result_confusion['f1_score'])*100:.2f}%")
print()

# è¯¦ç»†åˆ†ææœ€ä½³ç­–ç•¥
best_conf = strategies[0][2]
print("æœ€ä½³ç­–ç•¥è¯¦ç»†æŒ‡æ ‡:")
print(f"  å‡†ç¡®ç‡ (Accuracy):  {best_conf['accuracy']:.4f}")
print(f"  ç²¾ç¡®ç‡ (Precision): {best_conf['precision']:.4f}")
print(f"  å¬å›ç‡ (Recall):    {best_conf['recall']:.4f}")
print(f"  ç‰¹å¼‚åº¦ (Specificity): {best_conf['specificity']:.4f}")
print(f"  F1åˆ†æ•°:            {best_conf['f1_score']:.6f}")
print()

# å¯¹æ¯”åˆ†æ
print("ä¸result.csvå¯¹æ¯”:")
print(f"  TPå˜åŒ–: {best_conf['TP']} vs {result_confusion['TP']} ({best_conf['TP'] - result_confusion['TP']:+d})")
print(f"  FPå˜åŒ–: {best_conf['FP']} vs {result_confusion['FP']} ({best_conf['FP'] - result_confusion['FP']:+d})")
print(f"  FNå˜åŒ–: {best_conf['FN']} vs {result_confusion['FN']} ({best_conf['FN'] - result_confusion['FN']:+d})")
print(f"  TNå˜åŒ–: {best_conf['TN']} vs {result_confusion['TN']} ({best_conf['TN'] - result_confusion['TN']:+d})")
